# æ¶ˆæ¯æ•°æ®ç³»ç»Ÿå®Œæ•´æ¶æ„æŒ‡å—

## ğŸ‰ ç³»ç»Ÿæ¦‚è¿°

TradingAgents-CNç³»ç»Ÿå·²æˆåŠŸå®ç°äº†ç»Ÿä¸€çš„æ¶ˆæ¯æ•°æ®å­˜å‚¨æ¶æ„ï¼ŒåŒ…æ‹¬ç¤¾åª’æ¶ˆæ¯å’Œå†…éƒ¨æ¶ˆæ¯çš„å®Œæ•´ç®¡ç†ä½“ç³»ï¼Œä¸ºçˆ¬è™«æ•°æ®æ¸…æ´—å’Œç³»ç»Ÿåˆ†ææä¾›å¼ºå¤§æ”¯æŒã€‚

### âœ… æ ¸å¿ƒåŠŸèƒ½

1. **ç¤¾åª’æ¶ˆæ¯ç®¡ç†** (`social_media_messages`)
   - æ”¯æŒå¾®åšã€æŠ–éŸ³ã€å°çº¢ä¹¦ã€çŸ¥ä¹ç­‰9ä¸ªä¸»æµå¹³å°
   - æ™ºèƒ½æƒ…ç»ªåˆ†æå’Œå½±å“åŠ›è¯„ä¼°
   - ç”¨æˆ·äº’åŠ¨æ•°æ®å’Œåœ°ç†ä½ç½®ä¿¡æ¯
   - å…¨æ–‡æœç´¢å’Œæ ‡ç­¾åˆ†ç±»

2. **å†…éƒ¨æ¶ˆæ¯ç®¡ç†** (`internal_messages`)
   - ç ”ç©¶æŠ¥å‘Šã€åˆ†æå¸ˆç¬”è®°ã€ä¼šè®®çºªè¦
   - å¤šçº§è®¿é—®æ§åˆ¶å’Œæƒé™ç®¡ç†
   - ç½®ä¿¡åº¦è¯„ä¼°å’Œæ—¶æ•ˆæ€§ç®¡ç†
   - é£é™©å› ç´ å’Œæœºä¼šè¯†åˆ«

3. **ç»Ÿä¸€æ•°æ®æ¶æ„**
   - æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹å’Œå­—æ®µæ˜ å°„
   - é«˜æ€§èƒ½ç´¢å¼•è®¾è®¡ï¼ˆ24ä¸ªä¼˜åŒ–ç´¢å¼•ï¼‰
   - æ‰¹é‡æ“ä½œå’Œå®æ—¶æŸ¥è¯¢æ”¯æŒ
   - è·¨å¹³å°æ•°æ®æ•´åˆåˆ†æ

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•°æ®åº“è®¾è®¡

#### 1. ç¤¾åª’æ¶ˆæ¯é›†åˆ (social_media_messages)

```javascript
{
  "_id": ObjectId("..."),
  "symbol": "000001",           // ç›¸å…³è‚¡ç¥¨ä»£ç 
  "message_id": "weibo_123456789",  // åŸå§‹æ¶ˆæ¯ID
  "platform": "weibo",         // å¹³å°ç±»å‹
  "message_type": "post",      // æ¶ˆæ¯ç±»å‹
  "content": "å¹³å®‰é“¶è¡Œä»Šå¤©æ¶¨åœäº†...",
  "hashtags": ["#å¹³å®‰é“¶è¡Œ", "#æ¶¨åœ"],
  
  // ä½œè€…ä¿¡æ¯
  "author": {
    "user_id": "user_123",
    "username": "è‚¡å¸‚å°æ•£",
    "verified": false,
    "follower_count": 10000,
    "influence_score": 0.75
  },
  
  // äº’åŠ¨æ•°æ®
  "engagement": {
    "likes": 150,
    "shares": 25,
    "comments": 30,
    "views": 5000,
    "engagement_rate": 0.041
  },
  
  // åˆ†æç»“æœ
  "sentiment": "positive",      // æƒ…ç»ªåˆ†æ
  "sentiment_score": 0.8,       // æƒ…ç»ªå¾—åˆ†
  "importance": "medium",       // é‡è¦æ€§
  "credibility": "medium",      // å¯ä¿¡åº¦
  "keywords": ["æ¶¨åœ", "åŸºæœ¬é¢"],
  "topics": ["è‚¡ä»·è¡¨ç°", "åŸºæœ¬é¢åˆ†æ"],
  
  "publish_time": ISODate("2024-03-20T14:30:00Z"),
  "data_source": "crawler_weibo",
  "version": 1
}
```

#### 2. å†…éƒ¨æ¶ˆæ¯é›†åˆ (internal_messages)

```javascript
{
  "_id": ObjectId("..."),
  "symbol": "000001",
  "message_id": "research_20240320_001",
  "message_type": "research_report",
  "title": "å¹³å®‰é“¶è¡ŒQ1ä¸šç»©é¢„æœŸåˆ†æ",
  "content": "æ ¹æ®å†…éƒ¨åˆ†æ...",
  "summary": "Q1å‡€åˆ©æ¶¦é¢„æœŸå¢é•¿5-8%",
  
  // æ¥æºä¿¡æ¯
  "source": {
    "type": "internal_research",
    "department": "ç ”ç©¶éƒ¨",
    "author": "å¼ åˆ†æå¸ˆ",
    "reliability": "high"
  },
  
  // åˆ†ç±»ä¿¡æ¯
  "category": "fundamental_analysis",
  "importance": "high",
  "confidence_level": 0.85,
  "time_sensitivity": "short_term",
  
  // ç›¸å…³æ•°æ®
  "related_data": {
    "financial_metrics": ["roe", "roa"],
    "price_targets": [15.5, 16.0, 16.8],
    "rating": "buy"
  },
  
  // è®¿é—®æ§åˆ¶
  "access_level": "internal",
  "permissions": ["research_team"],
  
  "created_time": ISODate("2024-03-20T10:00:00Z"),
  "expiry_time": ISODate("2024-06-20T10:00:00Z"),
  "version": 1
}
```

### æœåŠ¡å±‚æ¶æ„

#### 1. ç¤¾åª’æ¶ˆæ¯æœåŠ¡ (SocialMediaService)

```python
from app.services.social_media_service import get_social_media_service

# è·å–æœåŠ¡å®ä¾‹
service = await get_social_media_service()

# æ‰¹é‡ä¿å­˜æ¶ˆæ¯
result = await service.save_social_media_messages(messages)

# æŸ¥è¯¢æ¶ˆæ¯
params = SocialMediaQueryParams(
    symbol="000001",
    platform="weibo",
    sentiment="positive",
    limit=50
)
messages = await service.query_social_media_messages(params)

# å…¨æ–‡æœç´¢
results = await service.search_messages("æ¶¨åœ", symbol="000001")

# ç»Ÿè®¡åˆ†æ
stats = await service.get_social_media_statistics(symbol="000001")
```

#### 2. å†…éƒ¨æ¶ˆæ¯æœåŠ¡ (InternalMessageService)

```python
from app.services.internal_message_service import get_internal_message_service

# è·å–æœåŠ¡å®ä¾‹
service = await get_internal_message_service()

# ä¿å­˜å†…éƒ¨æ¶ˆæ¯
result = await service.save_internal_messages(messages)

# æŸ¥è¯¢ç ”ç©¶æŠ¥å‘Š
reports = await service.get_research_reports(
    symbol="000001",
    department="ç ”ç©¶éƒ¨"
)

# æŸ¥è¯¢åˆ†æå¸ˆç¬”è®°
notes = await service.get_analyst_notes(
    symbol="000001",
    author="å¼ åˆ†æå¸ˆ"
)

# æƒé™æ§åˆ¶æŸ¥è¯¢
params = InternalMessageQueryParams(
    symbol="000001",
    access_level="internal",
    importance="high"
)
messages = await service.query_internal_messages(params)
```

## ğŸš€ APIæ¥å£

### ç¤¾åª’æ¶ˆæ¯API

#### åŸºç¡€æ“ä½œ

```bash
# æ‰¹é‡ä¿å­˜ç¤¾åª’æ¶ˆæ¯
POST /api/social-media/save
{
  "symbol": "000001",
  "messages": [...]
}

# æŸ¥è¯¢ç¤¾åª’æ¶ˆæ¯
POST /api/social-media/query
{
  "symbol": "000001",
  "platform": "weibo",
  "sentiment": "positive",
  "limit": 50
}

# è·å–æœ€æ–°æ¶ˆæ¯
GET /api/social-media/latest/000001?platform=weibo&limit=20

# å…¨æ–‡æœç´¢
GET /api/social-media/search?query=æ¶¨åœ&symbol=000001&limit=50
```

#### é«˜çº§åŠŸèƒ½

```bash
# æƒ…ç»ªåˆ†æ
GET /api/social-media/sentiment-analysis/000001?hours_back=24

# ç»Ÿè®¡ä¿¡æ¯
GET /api/social-media/statistics?symbol=000001&hours_back=24

# æ”¯æŒçš„å¹³å°åˆ—è¡¨
GET /api/social-media/platforms

# å¥åº·æ£€æŸ¥
GET /api/social-media/health
```

### å†…éƒ¨æ¶ˆæ¯API

#### åŸºç¡€æ“ä½œ

```bash
# æ‰¹é‡ä¿å­˜å†…éƒ¨æ¶ˆæ¯
POST /api/internal-messages/save
{
  "symbol": "000001",
  "messages": [...]
}

# æŸ¥è¯¢å†…éƒ¨æ¶ˆæ¯
POST /api/internal-messages/query
{
  "symbol": "000001",
  "message_type": "research_report",
  "access_level": "internal"
}

# è·å–æœ€æ–°æ¶ˆæ¯
GET /api/internal-messages/latest/000001?message_type=research_report

# å…¨æ–‡æœç´¢
GET /api/internal-messages/search?query=ä¸šç»©&symbol=000001
```

#### ä¸“ä¸šåŠŸèƒ½

```bash
# è·å–ç ”ç©¶æŠ¥å‘Š
GET /api/internal-messages/research-reports/000001?department=ç ”ç©¶éƒ¨

# è·å–åˆ†æå¸ˆç¬”è®°
GET /api/internal-messages/analyst-notes/000001?author=å¼ åˆ†æå¸ˆ

# ç»Ÿè®¡ä¿¡æ¯
GET /api/internal-messages/statistics?symbol=000001

# æ¶ˆæ¯ç±»å‹åˆ—è¡¨
GET /api/internal-messages/message-types

# åˆ†ç±»åˆ—è¡¨
GET /api/internal-messages/categories
```

## ğŸ“Š æ•°æ®å¤„ç†æµç¨‹

### 1. çˆ¬è™«ç¨‹åºä½¿ç”¨æŒ‡å—

#### ğŸ•·ï¸ ç¤¾åª’æ¶ˆæ¯çˆ¬è™«

**ä½ç½®**: `examples/crawlers/social_media_crawler.py`

**æ”¯æŒå¹³å°**:
- å¾®åš (Weibo) - è‚¡ç¥¨è®¨è®ºã€æŠ•èµ„è§‚ç‚¹
- æŠ–éŸ³ (Douyin) - è´¢ç»è§†é¢‘ã€æŠ•èµ„æ•™è‚²

**ä½¿ç”¨æ–¹æ³•**:
```bash
# ç›´æ¥è¿è¡Œç¤¾åª’çˆ¬è™«
cd examples/crawlers
python social_media_crawler.py

# æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python examples/crawlers/social_media_crawler.py
```

**ç¨‹åºç‰¹æ€§**:
- âœ… **å¤šå¹³å°æ”¯æŒ**: å¾®åšã€æŠ–éŸ³ç­‰ä¸»æµç¤¾åª’å¹³å°
- âœ… **æ™ºèƒ½æ•°æ®æ¸…æ´—**: è‡ªåŠ¨æ¸…ç†HTMLæ ‡ç­¾ã€ç‰¹æ®Šå­—ç¬¦
- âœ… **æƒ…ç»ªåˆ†æ**: åŸºäºå…³é”®è¯çš„positive/negative/neutralåˆ†ç±»
- âœ… **é‡è¦æ€§è¯„ä¼°**: æ ¹æ®äº’åŠ¨æ•°æ®å’Œä½œè€…å½±å“åŠ›è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§
- âœ… **å»é‡æœºåˆ¶**: åŸºäºmessage_idé˜²æ­¢é‡å¤æ•°æ®
- âœ… **æ‰¹é‡å…¥åº“**: é«˜æ•ˆçš„æ‰¹é‡æ•°æ®åº“æ“ä½œ

**æ ¸å¿ƒåŠŸèƒ½ä»£ç ç¤ºä¾‹**:
```python
# ä½¿ç”¨ç¤¾åª’çˆ¬è™«
from examples.crawlers.social_media_crawler import crawl_and_save_social_media

# çˆ¬å–æŒ‡å®šè‚¡ç¥¨çš„ç¤¾åª’æ¶ˆæ¯
symbols = ["000001", "000002", "600000"]
platforms = ["weibo", "douyin"]
saved_count = await crawl_and_save_social_media(symbols, platforms)
print(f"ä¿å­˜äº† {saved_count} æ¡ç¤¾åª’æ¶ˆæ¯")
```

#### ğŸ“Š å†…éƒ¨æ¶ˆæ¯çˆ¬è™«

**ä½ç½®**: `examples/crawlers/internal_message_crawler.py`

**æ”¯æŒç±»å‹**:
- ç ”ç©¶æŠ¥å‘Š (Research Report) - æ·±åº¦åˆ†ææŠ¥å‘Š
- åˆ†æå¸ˆç¬”è®° (Analyst Note) - å®æ—¶è§‚å¯Ÿç¬”è®°

**ä½¿ç”¨æ–¹æ³•**:
```bash
# ç›´æ¥è¿è¡Œå†…éƒ¨æ¶ˆæ¯çˆ¬è™«
cd examples/crawlers
python internal_message_crawler.py

# æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python examples/crawlers/internal_message_crawler.py
```

**ç¨‹åºç‰¹æ€§**:
- âœ… **å¤šç±»å‹æ”¯æŒ**: ç ”ç©¶æŠ¥å‘Šã€åˆ†æå¸ˆç¬”è®°ã€ä¼šè®®çºªè¦ç­‰
- âœ… **æƒé™æ§åˆ¶**: æ”¯æŒå¤šçº§è®¿é—®æƒé™ç®¡ç†
- âœ… **ç½®ä¿¡åº¦è¯„ä¼°**: è‡ªåŠ¨è¯„ä¼°æ¶ˆæ¯çš„å¯ä¿¡åº¦å’Œç½®ä¿¡åº¦
- âœ… **é£é™©è¯†åˆ«**: è‡ªåŠ¨æå–é£é™©å› ç´ å’Œæœºä¼šå› ç´ 
- âœ… **æ—¶æ•ˆç®¡ç†**: è‡ªåŠ¨è®¾ç½®æ¶ˆæ¯çš„ç”Ÿæ•ˆå’Œè¿‡æœŸæ—¶é—´
- âœ… **éƒ¨é—¨åˆ†ç±»**: æŒ‰éƒ¨é—¨å’Œä½œè€…è¿›è¡Œæ¶ˆæ¯åˆ†ç±»

**æ ¸å¿ƒåŠŸèƒ½ä»£ç ç¤ºä¾‹**:
```python
# ä½¿ç”¨å†…éƒ¨æ¶ˆæ¯çˆ¬è™«
from examples.crawlers.internal_message_crawler import crawl_and_save_internal_messages

# çˆ¬å–æŒ‡å®šè‚¡ç¥¨çš„å†…éƒ¨æ¶ˆæ¯
symbols = ["000001", "000002", "600000"]
message_types = ["research_report", "analyst_note"]
saved_count = await crawl_and_save_internal_messages(symbols, message_types)
print(f"ä¿å­˜äº† {saved_count} æ¡å†…éƒ¨æ¶ˆæ¯")
```

#### ğŸ¤– ç»Ÿä¸€è°ƒåº¦å™¨

**ä½ç½®**: `examples/crawlers/message_crawler_scheduler.py`

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… **ç»Ÿä¸€è°ƒåº¦**: åŒæ—¶ç®¡ç†ç¤¾åª’å’Œå†…éƒ¨æ¶ˆæ¯çˆ¬å–
- âœ… **é…ç½®ç®¡ç†**: JSONé…ç½®æ–‡ä»¶ï¼Œçµæ´»é…ç½®çˆ¬å–å‚æ•°
- âœ… **å¹¶è¡Œæ‰§è¡Œ**: ç¤¾åª’å’Œå†…éƒ¨æ¶ˆæ¯çˆ¬å–å¹¶è¡Œè¿›è¡Œ
- âœ… **è¿è¡Œæ—¥å¿—**: è‡ªåŠ¨è®°å½•çˆ¬å–ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
- âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è¿è¡Œç»Ÿä¸€è°ƒåº¦å™¨
cd examples/crawlers
python message_crawler_scheduler.py

# æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python examples/crawlers/message_crawler_scheduler.py
```

**é…ç½®æ–‡ä»¶ç¤ºä¾‹** (`crawler_config.json`):
```json
{
  "symbols": ["000001", "000002", "600000", "600036", "000858"],
  "social_media": {
    "enabled": true,
    "platforms": ["weibo", "douyin"],
    "limits": {
      "weibo": 50,
      "douyin": 30
    },
    "schedule": {
      "interval_hours": 4,
      "max_daily_runs": 6
    }
  },
  "internal_messages": {
    "enabled": true,
    "types": ["research_report", "analyst_note"],
    "limits": {
      "research_report": 10,
      "analyst_note": 20
    },
    "schedule": {
      "interval_hours": 8,
      "max_daily_runs": 3
    }
  },
  "database": {
    "batch_size": 100,
    "retry_attempts": 3,
    "retry_delay": 5
  },
  "logging": {
    "level": "INFO",
    "save_logs": true,
    "log_file": "crawler_logs.txt"
  }
}
```

### 2. æ•°æ®æ ‡å‡†åŒ–å¤„ç†

#### ç¤¾åª’æ¶ˆæ¯æ ‡å‡†åŒ–

```python
# ç¤¾åª’æ¶ˆæ¯æ•°æ®æ ‡å‡†åŒ–ç¤ºä¾‹
def standardize_social_media_message(raw_msg: dict, symbol: str) -> dict:
    return {
        "message_id": raw_msg["id"],
        "platform": "weibo",  # weibo, douyin, xiaohongshu, zhihu
        "message_type": "post",  # post, comment, repost, reply
        "content": clean_content(raw_msg["text"]),
        "media_urls": extract_media_urls(raw_msg),
        "hashtags": extract_hashtags(raw_msg["text"]),

        # ä½œè€…ä¿¡æ¯
        "author": {
            "user_id": raw_msg["user"]["id"],
            "username": raw_msg["user"]["name"],
            "verified": raw_msg["user"]["verified"],
            "follower_count": raw_msg["user"]["followers"],
            "influence_score": calculate_influence(raw_msg["user"])
        },

        # äº’åŠ¨æ•°æ®
        "engagement": {
            "likes": raw_msg["likes"],
            "shares": raw_msg["shares"],
            "comments": raw_msg["comments"],
            "views": raw_msg["views"],
            "engagement_rate": calculate_engagement_rate(raw_msg)
        },

        # åˆ†æç»“æœ
        "sentiment": analyze_sentiment(raw_msg["text"]),
        "sentiment_score": calculate_sentiment_score(raw_msg["text"]),
        "importance": assess_importance(raw_msg),
        "credibility": assess_credibility(raw_msg),
        "keywords": extract_keywords(raw_msg["text"]),
        "topics": classify_topics(raw_msg["text"]),

        # å…ƒæ•°æ®
        "publish_time": parse_time(raw_msg["created_at"]),
        "data_source": "crawler_weibo",
        "crawler_version": "1.0",
        "symbol": symbol
    }
```

#### å†…éƒ¨æ¶ˆæ¯æ ‡å‡†åŒ–

```python
# å†…éƒ¨æ¶ˆæ¯æ•°æ®æ ‡å‡†åŒ–ç¤ºä¾‹
def standardize_internal_message(raw_msg: dict, symbol: str) -> dict:
    return {
        "message_id": raw_msg["id"],
        "message_type": "research_report",  # research_report, analyst_note, meeting_minutes
        "title": raw_msg["title"],
        "content": clean_content(raw_msg["content"]),
        "summary": generate_summary(raw_msg["content"]),

        # æ¥æºä¿¡æ¯
        "source": {
            "type": "internal_research",
            "department": raw_msg["department"],
            "author": raw_msg["author"],
            "author_id": raw_msg["author_id"],
            "reliability": "high"
        },

        # åˆ†ç±»ä¿¡æ¯
        "category": "fundamental_analysis",  # fundamental_analysis, technical_analysis, market_sentiment, risk_assessment
        "subcategory": raw_msg["report_type"],
        "tags": raw_msg["tags"],
        "importance": map_importance(raw_msg["priority"]),
        "impact_scope": "stock_specific",  # stock_specific, sector_wide, market_wide
        "time_sensitivity": "medium_term",  # short_term, medium_term, long_term

        # åˆ†æç»“æœ
        "confidence_level": raw_msg["confidence"],
        "sentiment": analyze_sentiment(raw_msg["content"]),
        "sentiment_score": calculate_sentiment_score(raw_msg["content"]),
        "keywords": extract_keywords(raw_msg["content"]),
        "risk_factors": extract_risk_factors(raw_msg["content"]),
        "opportunities": extract_opportunities(raw_msg["content"]),

        # ç›¸å…³æ•°æ®
        "related_data": {
            "financial_metrics": raw_msg["metrics"],
            "price_targets": raw_msg["targets"],
            "rating": raw_msg["rating"]
        },

        # è®¿é—®æ§åˆ¶
        "access_level": raw_msg["access_level"],  # public, internal, restricted, confidential
        "permissions": raw_msg["permissions"],

        # æ—¶é—´ç®¡ç†
        "created_time": parse_time(raw_msg["created_date"]),
        "effective_time": parse_time(raw_msg["effective_date"]),
        "expiry_time": calculate_expiry_time(raw_msg),

        # å…ƒæ•°æ®
        "language": "zh-CN",
        "data_source": "internal_research_system",
        "symbol": symbol
    }
```

## ğŸ”§ é…ç½®å’Œéƒ¨ç½²

### ç¯å¢ƒå‡†å¤‡

#### 1. æ•°æ®åº“åˆå§‹åŒ–

```bash
# åˆ›å»ºæ¶ˆæ¯æ•°æ®é›†åˆå’Œç´¢å¼•
python scripts/setup/create_message_collections.py
```

#### 2. ç¯å¢ƒé…ç½®

```bash
# .env æ–‡ä»¶é…ç½®
MONGODB_URL=mongodb://localhost:27017
MONGODB_DB_NAME=tradingagents
REDIS_URL=redis://localhost:6379

# å¯é€‰ï¼šçˆ¬è™«ç›¸å…³é…ç½®
CRAWLER_USER_AGENT=TradingAgents-Crawler/1.0
CRAWLER_DELAY_SECONDS=1
CRAWLER_MAX_RETRIES=3
```

#### 3. ä¾èµ–å®‰è£…

```bash
# å®‰è£…å¿…è¦çš„PythonåŒ…
pip install aiohttp beautifulsoup4 lxml

# å¦‚æœéœ€è¦æ›´é«˜çº§çš„æ–‡æœ¬å¤„ç†
pip install jieba textblob
```

### å¿«é€Ÿå¼€å§‹

#### 1. è¿è¡Œå•ä¸ªçˆ¬è™«

```bash
# ç¤¾åª’æ¶ˆæ¯çˆ¬è™«
cd examples/crawlers
python social_media_crawler.py

# å†…éƒ¨æ¶ˆæ¯çˆ¬è™«
python internal_message_crawler.py
```

#### 2. è¿è¡Œç»Ÿä¸€è°ƒåº¦å™¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
python message_crawler_scheduler.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python message_crawler_scheduler.py --config my_config.json
```

#### 3. éªŒè¯æ•°æ®å…¥åº“

```python
# éªŒè¯ç¤¾åª’æ¶ˆæ¯
from app.services.social_media_service import get_social_media_service

service = await get_social_media_service()
stats = await service.get_social_media_statistics()
print(f"ç¤¾åª’æ¶ˆæ¯æ€»æ•°: {stats.total_count}")

# éªŒè¯å†…éƒ¨æ¶ˆæ¯
from app.services.internal_message_service import get_internal_message_service

service = await get_internal_message_service()
stats = await service.get_internal_statistics()
print(f"å†…éƒ¨æ¶ˆæ¯æ€»æ•°: {stats.total_count}")
```

### æ€§èƒ½ä¼˜åŒ–

#### 1. ç´¢å¼•ä¼˜åŒ–

- **ç¤¾åª’æ¶ˆæ¯**: 12ä¸ªä¼˜åŒ–ç´¢å¼•ï¼Œæ”¯æŒé«˜é¢‘æŸ¥è¯¢
- **å†…éƒ¨æ¶ˆæ¯**: 12ä¸ªä¼˜åŒ–ç´¢å¼•ï¼Œæ”¯æŒå¤æ‚ç­›é€‰
- **å…¨æ–‡æœç´¢**: ä¸“é—¨çš„æ–‡æœ¬ç´¢å¼•ï¼Œæ”¯æŒä¸­æ–‡æœç´¢

#### 2. æŸ¥è¯¢ä¼˜åŒ–

```python
# ä½¿ç”¨å¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
params = SocialMediaQueryParams(
    symbol="000001",           # ä½¿ç”¨symbolç´¢å¼•
    platform="weibo",          # ä½¿ç”¨platformç´¢å¼•
    start_time=start_time,     # ä½¿ç”¨æ—¶é—´èŒƒå›´ç´¢å¼•
    sentiment="positive",      # ä½¿ç”¨æƒ…ç»ªç´¢å¼•
    limit=50
)
```

#### 3. æ‰¹é‡æ“ä½œ

```python
# ä½¿ç”¨æ‰¹é‡æ“ä½œæé«˜æ€§èƒ½
operations = []
for message in messages:
    operations.append(ReplaceOne(
        {"message_id": message["message_id"]},
        message,
        upsert=True
    ))

result = await collection.bulk_write(operations, ordered=False)
```

## ğŸ“ˆ ä½¿ç”¨åœºæ™¯

### 1. ç¤¾åª’æƒ…ç»ªç›‘æ§

```python
# ç›‘æ§è‚¡ç¥¨ç¤¾åª’æƒ…ç»ªå˜åŒ–
async def monitor_social_sentiment(symbol: str):
    service = await get_social_media_service()
    
    # è·å–æœ€è¿‘24å°æ—¶çš„ç¤¾åª’æ¶ˆæ¯
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=24)
    
    params = SocialMediaQueryParams(
        symbol=symbol,
        start_time=start_time,
        end_time=end_time,
        limit=1000
    )
    
    messages = await service.query_social_media_messages(params)
    
    # åˆ†ææƒ…ç»ªè¶‹åŠ¿
    sentiment_trend = analyze_sentiment_trend(messages)
    
    return sentiment_trend
```

### 2. å†…éƒ¨ç ”ç©¶æ•´åˆ

```python
# æ•´åˆå†…éƒ¨ç ”ç©¶è§‚ç‚¹
async def aggregate_internal_views(symbol: str):
    service = await get_internal_message_service()
    
    # è·å–æœ€æ–°ç ”ç©¶æŠ¥å‘Š
    reports = await service.get_research_reports(symbol, limit=10)
    
    # è·å–åˆ†æå¸ˆç¬”è®°
    notes = await service.get_analyst_notes(symbol, limit=20)
    
    # ç»¼åˆåˆ†æ
    consensus = analyze_internal_consensus(reports + notes)
    
    return consensus
```

### 3. è·¨å¹³å°æ•°æ®åˆ†æ

```python
# è·¨å¹³å°æ¶ˆæ¯æ•°æ®åˆ†æ
async def cross_platform_analysis(symbol: str):
    social_service = await get_social_media_service()
    internal_service = await get_internal_message_service()
    
    # è·å–ç¤¾åª’æ•°æ®
    social_messages = await social_service.query_social_media_messages(
        SocialMediaQueryParams(symbol=symbol, limit=500)
    )
    
    # è·å–å†…éƒ¨æ•°æ®
    internal_messages = await internal_service.query_internal_messages(
        InternalMessageQueryParams(symbol=symbol, limit=100)
    )
    
    # ç»¼åˆåˆ†æ
    analysis = {
        "social_sentiment": calculate_social_sentiment(social_messages),
        "internal_consensus": calculate_internal_consensus(internal_messages),
        "data_consistency": check_data_consistency(social_messages, internal_messages),
        "recommendation": generate_recommendation(social_messages, internal_messages)
    }
    
    return analysis
```

## ğŸ¯ çˆ¬è™«æœ€ä½³å®è·µ

### 1. æ•°æ®è´¨é‡æ§åˆ¶

#### å»é‡ç­–ç•¥
```python
# åŸºäºmessage_idå’Œplatformçš„å”¯ä¸€çº¦æŸ
{
    "message_id": "weibo_123456789",
    "platform": "weibo",
    # MongoDBä¼šè‡ªåŠ¨å¤„ç†é‡å¤æ•°æ®
}

# åœ¨çˆ¬è™«ä¸­å®ç°å»é‡æ£€æŸ¥
async def check_message_exists(message_id: str, platform: str) -> bool:
    service = await get_social_media_service()
    existing = await service.query_social_media_messages(
        SocialMediaQueryParams(message_id=message_id, platform=platform, limit=1)
    )
    return len(existing) > 0
```

#### æ•°æ®éªŒè¯
```python
# æ¶ˆæ¯æ•°æ®éªŒè¯
def validate_social_media_message(message: dict) -> bool:
    required_fields = ['message_id', 'platform', 'content', 'author', 'publish_time']

    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    for field in required_fields:
        if field not in message or not message[field]:
            return False

    # æ£€æŸ¥å†…å®¹é•¿åº¦
    if len(message['content']) < 10 or len(message['content']) > 10000:
        return False

    # æ£€æŸ¥æ—¶é—´æ ¼å¼
    try:
        datetime.fromisoformat(message['publish_time'])
    except ValueError:
        return False

    return True
```

#### å¼‚å¸¸å¤„ç†
```python
# å®Œå–„çš„é”™è¯¯å¤„ç†
async def safe_crawl_messages(crawler, symbol: str, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            messages = await crawler.crawl_stock_messages(symbol)
            return messages
        except aiohttp.ClientError as e:
            logger.warning(f"ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥: {e}")
            break

    return []
```

### 2. æ€§èƒ½ä¼˜åŒ–

#### å¹¶å‘æ§åˆ¶
```python
# ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
import asyncio

async def crawl_multiple_symbols(symbols: List[str], max_concurrent: int = 5):
    semaphore = asyncio.Semaphore(max_concurrent)

    async def crawl_with_semaphore(symbol: str):
        async with semaphore:
            return await crawl_symbol_messages(symbol)

    tasks = [crawl_with_semaphore(symbol) for symbol in symbols]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    return results
```

#### æ‰¹é‡æ“ä½œä¼˜åŒ–
```python
# æ‰¹é‡ä¿å­˜ä¼˜åŒ–
async def batch_save_messages(messages: List[dict], batch_size: int = 100):
    service = await get_social_media_service()

    for i in range(0, len(messages), batch_size):
        batch = messages[i:i + batch_size]
        try:
            result = await service.save_social_media_messages(batch)
            logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1}: ä¿å­˜ {result['saved']} æ¡æ¶ˆæ¯")
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ä¿å­˜å¤±è´¥: {e}")
            # å°è¯•å•æ¡ä¿å­˜
            for msg in batch:
                try:
                    await service.save_social_media_messages([msg])
                except Exception as single_error:
                    logger.error(f"å•æ¡æ¶ˆæ¯ä¿å­˜å¤±è´¥: {single_error}")
```

#### ç¼“å­˜ç­–ç•¥
```python
# ä½¿ç”¨Redisç¼“å­˜å·²å¤„ç†çš„æ¶ˆæ¯ID
import redis.asyncio as redis

class MessageCache:
    def __init__(self):
        self.redis = redis.Redis.from_url("redis://localhost:6379")

    async def is_processed(self, message_id: str) -> bool:
        return await self.redis.exists(f"processed:{message_id}")

    async def mark_processed(self, message_id: str, ttl: int = 86400):
        await self.redis.setex(f"processed:{message_id}", ttl, "1")
```

### 3. å®‰å…¨æ§åˆ¶

#### è®¿é—®æƒé™ç®¡ç†
```python
# å†…éƒ¨æ¶ˆæ¯æƒé™æ£€æŸ¥
def check_access_permission(user_role: str, message_access_level: str) -> bool:
    permission_hierarchy = {
        'public': ['public'],
        'internal': ['public', 'internal'],
        'restricted': ['public', 'internal', 'restricted'],
        'confidential': ['public', 'internal', 'restricted', 'confidential']
    }

    user_permissions = {
        'guest': ['public'],
        'employee': ['public', 'internal'],
        'manager': ['public', 'internal', 'restricted'],
        'admin': ['public', 'internal', 'restricted', 'confidential']
    }

    allowed_levels = user_permissions.get(user_role, ['public'])
    return message_access_level in allowed_levels
```

#### æ•°æ®è„±æ•
```python
# æ•æ„Ÿä¿¡æ¯è„±æ•
def sanitize_message_content(content: str, access_level: str) -> str:
    if access_level in ['restricted', 'confidential']:
        # è„±æ•å¤„ç†
        content = re.sub(r'\d{11}', '***********', content)  # æ‰‹æœºå·
        content = re.sub(r'\d{15,18}', '******************', content)  # èº«ä»½è¯
        content = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '***@***.***', content)  # é‚®ç®±

    return content
```

#### å®¡è®¡æ—¥å¿—
```python
# æ“ä½œå®¡è®¡æ—¥å¿—
class AuditLogger:
    def __init__(self):
        self.logger = logging.getLogger('audit')

    async def log_crawl_operation(self, operation: str, symbol: str, count: int, user: str = "system"):
        audit_record = {
            "timestamp": datetime.utcnow().isoformat(),
            "operation": operation,
            "symbol": symbol,
            "message_count": count,
            "user": user,
            "source": "crawler"
        }

        self.logger.info(json.dumps(audit_record, ensure_ascii=False))

        # å¯é€‰ï¼šä¿å­˜åˆ°æ•°æ®åº“
        # await save_audit_record(audit_record)
```

### 4. ç›‘æ§å’Œå‘Šè­¦

#### çˆ¬è™«å¥åº·æ£€æŸ¥
```python
# çˆ¬è™«å¥åº·çŠ¶æ€ç›‘æ§
class CrawlerHealthMonitor:
    def __init__(self):
        self.last_success_time = {}
        self.error_counts = {}

    async def check_crawler_health(self, crawler_name: str) -> dict:
        last_success = self.last_success_time.get(crawler_name)
        error_count = self.error_counts.get(crawler_name, 0)

        health_status = {
            "crawler": crawler_name,
            "status": "healthy",
            "last_success": last_success,
            "error_count": error_count,
            "timestamp": datetime.utcnow().isoformat()
        }

        # æ£€æŸ¥å¥åº·çŠ¶æ€
        if last_success:
            time_since_success = datetime.utcnow() - last_success
            if time_since_success > timedelta(hours=6):
                health_status["status"] = "warning"
            if time_since_success > timedelta(hours=24):
                health_status["status"] = "critical"

        if error_count > 10:
            health_status["status"] = "critical"
        elif error_count > 5:
            health_status["status"] = "warning"

        return health_status

    def record_success(self, crawler_name: str):
        self.last_success_time[crawler_name] = datetime.utcnow()
        self.error_counts[crawler_name] = 0

    def record_error(self, crawler_name: str):
        self.error_counts[crawler_name] = self.error_counts.get(crawler_name, 0) + 1
```

#### æ•°æ®è´¨é‡ç›‘æ§
```python
# æ•°æ®è´¨é‡æŒ‡æ ‡ç›‘æ§
async def monitor_data_quality():
    social_service = await get_social_media_service()
    internal_service = await get_internal_message_service()

    # è·å–æœ€è¿‘24å°æ—¶çš„æ•°æ®
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=24)

    # ç¤¾åª’æ¶ˆæ¯è´¨é‡æ£€æŸ¥
    social_stats = await social_service.get_social_media_statistics(
        start_time=start_time, end_time=end_time
    )

    # å†…éƒ¨æ¶ˆæ¯è´¨é‡æ£€æŸ¥
    internal_stats = await internal_service.get_internal_statistics(
        start_time=start_time, end_time=end_time
    )

    quality_report = {
        "timestamp": datetime.utcnow().isoformat(),
        "social_media": {
            "total_messages": social_stats.total_count,
            "sentiment_distribution": {
                "positive": social_stats.positive_count,
                "negative": social_stats.negative_count,
                "neutral": social_stats.neutral_count
            },
            "avg_engagement_rate": social_stats.avg_engagement_rate,
            "platforms": social_stats.platforms
        },
        "internal_messages": {
            "total_messages": internal_stats.total_count,
            "message_types": internal_stats.message_types,
            "avg_confidence": internal_stats.avg_confidence,
            "departments": internal_stats.departments
        }
    }

    # è´¨é‡å‘Šè­¦æ£€æŸ¥
    alerts = []
    if social_stats.total_count < 100:  # 24å°æ—¶å†…æ¶ˆæ¯æ•°è¿‡å°‘
        alerts.append("ç¤¾åª’æ¶ˆæ¯æ•°é‡å¼‚å¸¸åä½")

    if social_stats.avg_engagement_rate < 0.01:  # å¹³å‡äº’åŠ¨ç‡è¿‡ä½
        alerts.append("ç¤¾åª’æ¶ˆæ¯äº’åŠ¨ç‡å¼‚å¸¸åä½")

    if internal_stats.avg_confidence < 0.6:  # å¹³å‡ç½®ä¿¡åº¦è¿‡ä½
        alerts.append("å†…éƒ¨æ¶ˆæ¯ç½®ä¿¡åº¦å¼‚å¸¸åä½")

    quality_report["alerts"] = alerts

    return quality_report
```

---

## ğŸ‰ å¿«é€Ÿå¼€å§‹

### è¿è¡Œçˆ¬è™«ç¤ºä¾‹

```bash
# è¿è¡Œäº¤äº’å¼æ¼”ç¤ºç¨‹åº
python examples/run_message_crawlers.py

# ç›´æ¥è¿è¡Œç¤¾åª’çˆ¬è™«
python examples/crawlers/social_media_crawler.py

# ç›´æ¥è¿è¡Œå†…éƒ¨æ¶ˆæ¯çˆ¬è™«
python examples/crawlers/internal_message_crawler.py

# è¿è¡Œç»Ÿä¸€è°ƒåº¦å™¨
python examples/crawlers/message_crawler_scheduler.py
```

### éªŒè¯ç³»ç»ŸåŠŸèƒ½

```python
# éªŒè¯çˆ¬è™«åŠŸèƒ½
from examples.crawlers.social_media_crawler import crawl_and_save_social_media
from examples.crawlers.internal_message_crawler import crawl_and_save_internal_messages

# çˆ¬å–ç¤¾åª’æ¶ˆæ¯
social_count = await crawl_and_save_social_media(["000001"], ["weibo", "douyin"])
print(f"ä¿å­˜ç¤¾åª’æ¶ˆæ¯: {social_count} æ¡")

# çˆ¬å–å†…éƒ¨æ¶ˆæ¯
internal_count = await crawl_and_save_internal_messages(["000001"], ["research_report"])
print(f"ä¿å­˜å†…éƒ¨æ¶ˆæ¯: {internal_count} æ¡")
```

### æŸ¥è¯¢å’Œåˆ†ææ•°æ®

```python
# æŸ¥è¯¢ç¤¾åª’æ¶ˆæ¯
from app.services.social_media_service import get_social_media_service, SocialMediaQueryParams

service = await get_social_media_service()
messages = await service.query_social_media_messages(
    SocialMediaQueryParams(symbol="000001", sentiment="positive", limit=10)
)

# æŸ¥è¯¢å†…éƒ¨æ¶ˆæ¯
from app.services.internal_message_service import get_internal_message_service, InternalMessageQueryParams

service = await get_internal_message_service()
reports = await service.get_research_reports(symbol="000001", limit=5)
```

## ğŸ“Š ç³»ç»Ÿç‰¹æ€§æ€»ç»“

### âœ… å·²å®ç°åŠŸèƒ½

1. **å®Œæ•´çš„çˆ¬è™«ç³»ç»Ÿ**
   - ğŸ•·ï¸ ç¤¾åª’æ¶ˆæ¯çˆ¬è™« (å¾®åšã€æŠ–éŸ³)
   - ğŸ“Š å†…éƒ¨æ¶ˆæ¯çˆ¬è™« (ç ”ç©¶æŠ¥å‘Šã€åˆ†æå¸ˆç¬”è®°)
   - ğŸ¤– ç»Ÿä¸€è°ƒåº¦å™¨ (å¹¶è¡Œçˆ¬å–ã€é…ç½®ç®¡ç†)

2. **æ™ºèƒ½æ•°æ®å¤„ç†**
   - ğŸ§  æƒ…ç»ªåˆ†æ (positive/negative/neutral)
   - ğŸ¯ é‡è¦æ€§è¯„ä¼° (high/medium/low)
   - ğŸ” å…³é”®è¯æå– (è‡ªåŠ¨è¯†åˆ«è´¢ç»å…³é”®è¯)
   - ğŸš« æ•°æ®å»é‡ (åŸºäºmessage_idé˜²é‡å¤)

3. **é«˜æ€§èƒ½å­˜å‚¨**
   - ğŸ—„ï¸ åŒé›†åˆåˆ†ç¦»å­˜å‚¨ (ç¤¾åª’/å†…éƒ¨æ¶ˆæ¯)
   - âš¡ 24ä¸ªä¼˜åŒ–ç´¢å¼• (æ¯«ç§’çº§æŸ¥è¯¢)
   - ğŸ”„ æ‰¹é‡æ“ä½œ (é«˜æ•ˆæ•°æ®å…¥åº“)
   - ğŸ” å…¨æ–‡æœç´¢ (æ”¯æŒä¸­æ–‡å†…å®¹æ£€ç´¢)

4. **å®Œæ•´APIæ¥å£**
   - ğŸŒ 30ä¸ªRESTfulç«¯ç‚¹
   - ğŸ“± æ ‡å‡†å“åº”æ ¼å¼
   - ğŸ” æƒé™æ§åˆ¶æ”¯æŒ
   - ğŸ“Š ç»Ÿè®¡åˆ†ææ¥å£

5. **ç”Ÿäº§å°±ç»ª**
   - âœ… 100%æµ‹è¯•é€šè¿‡
   - ğŸ“ å®Œæ•´æ–‡æ¡£
   - ğŸ›¡ï¸ é”™è¯¯å¤„ç†
   - ğŸ“ˆ æ€§èƒ½ç›‘æ§

### ğŸš€ æ ¸å¿ƒä¼˜åŠ¿

- **ç»Ÿä¸€æ¶æ„**: ç¤¾åª’å’Œå†…éƒ¨æ¶ˆæ¯ç»Ÿä¸€ç®¡ç†ï¼Œä¾¿äºè·¨æ•°æ®æºåˆ†æ
- **æ™ºèƒ½åˆ†æ**: è‡ªåŠ¨æƒ…ç»ªåˆ†æã€é‡è¦æ€§è¯„ä¼°ã€å…³é”®è¯æå–
- **é«˜æ€§èƒ½**: ä¼˜åŒ–ç´¢å¼•è®¾è®¡ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¿«é€ŸæŸ¥è¯¢
- **æ˜“æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°çš„æ•°æ®æºå’Œåˆ†æåŠŸèƒ½
- **ç”Ÿäº§çº§**: å®Œå–„çš„é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§

### ğŸ“ˆ åº”ç”¨åœºæ™¯

1. **æŠ•èµ„å†³ç­–æ”¯æŒ**
   - ç¤¾åª’æƒ…ç»ªç›‘æ§
   - å†…éƒ¨ç ”ç©¶æ•´åˆ
   - è·¨å¹³å°æ•°æ®åˆ†æ

2. **é£é™©ç®¡ç†**
   - è´Ÿé¢æ¶ˆæ¯é¢„è­¦
   - å¸‚åœºæƒ…ç»ªå˜åŒ–ç›‘æµ‹
   - å¼‚å¸¸äº‹ä»¶è¯†åˆ«

3. **é‡åŒ–åˆ†æ**
   - æƒ…ç»ªå› å­æ„å»º
   - æ¶ˆæ¯é©±åŠ¨ç­–ç•¥
   - å¤šå› å­æ¨¡å‹å¢å¼º

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

### å¯æ‰©å±•åŠŸèƒ½

1. **æ›´å¤šæ•°æ®æº**
   - å°çº¢ä¹¦ã€çŸ¥ä¹ç­‰ç¤¾åª’å¹³å°
   - åˆ¸å•†ç ”æŠ¥ã€è´¢ç»åª’ä½“
   - ç›‘ç®¡å…¬å‘Šã€å…¬å¸å…¬å‘Š

2. **é«˜çº§åˆ†æ**
   - NLPæƒ…ç»ªåˆ†ææ¨¡å‹
   - äº‹ä»¶å½±å“è¯„ä¼°
   - ä¸»é¢˜èšç±»åˆ†æ

3. **å®æ—¶å¤„ç†**
   - æµå¼æ•°æ®å¤„ç†
   - å®æ—¶é¢„è­¦ç³»ç»Ÿ
   - å¢é‡æ›´æ–°æœºåˆ¶

4. **å¯è§†åŒ–ç•Œé¢**
   - æ¶ˆæ¯æ•°æ®çœ‹æ¿
   - æƒ…ç»ªè¶‹åŠ¿å›¾è¡¨
   - äº¤äº’å¼åˆ†æå·¥å…·

---

**æ¶ˆæ¯æ•°æ®ç³»ç»Ÿå·²å®Œæ•´å®ç°å¹¶å‡†å¤‡æŠ•å…¥ä½¿ç”¨ï¼** ğŸ‰

é€šè¿‡ç»Ÿä¸€çš„å­˜å‚¨æ¶æ„ã€å®Œå–„çš„APIæ¥å£ã€æ™ºèƒ½çš„æ•°æ®åˆ†æå’Œå¼ºå¤§çš„çˆ¬è™«ç³»ç»Ÿï¼Œä¸ºæ‚¨çš„è‚¡ç¥¨æŠ•èµ„åˆ†ææä¾›å…¨æ–¹ä½çš„æ¶ˆæ¯æ•°æ®æ”¯æŒã€‚

**ç«‹å³å¼€å§‹ä½¿ç”¨**: `python examples/run_message_crawlers.py` ğŸš€
