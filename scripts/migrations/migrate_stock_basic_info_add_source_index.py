#!/usr/bin/env python3
"""
æ•°æ®è¿ç§»è„šæœ¬ï¼šä¸º stock_basic_info é›†åˆæ·»åŠ  (code, source) è”åˆå”¯ä¸€ç´¢å¼•

èƒŒæ™¯ï¼š
- åŸæ¥çš„è®¾è®¡ï¼šæ¯åªè‚¡ç¥¨åªæœ‰ä¸€æ¡è®°å½•ï¼Œä½¿ç”¨ code å”¯ä¸€ç´¢å¼•
- æ–°çš„è®¾è®¡ï¼šæ¯åªè‚¡ç¥¨å¯ä»¥æœ‰å¤šæ¡è®°å½•ï¼ˆæ¥è‡ªä¸åŒæ•°æ®æºï¼‰ï¼Œä½¿ç”¨ (code, source) è”åˆå”¯ä¸€ç´¢å¼•

è¿ç§»æ­¥éª¤ï¼š
1. æ£€æŸ¥ç°æœ‰æ•°æ®çš„ source å­—æ®µ
2. ä¸ºæ²¡æœ‰ source å­—æ®µçš„æ•°æ®æ·»åŠ é»˜è®¤å€¼
3. åˆ é™¤æ—§çš„ code å”¯ä¸€ç´¢å¼•
4. åˆ›å»ºæ–°çš„ (code, source) è”åˆå”¯ä¸€ç´¢å¼•
5. éªŒè¯è¿ç§»ç»“æœ

è¿è¡Œæ–¹å¼ï¼š
    python scripts/migrations/migrate_stock_basic_info_add_source_index.py
"""

import asyncio
import logging
import sys
from pathlib import Path
from datetime import datetime
from pymongo import ASCENDING
from motor.motor_asyncio import AsyncIOMotorClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.core.config import get_settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


async def migrate_stock_basic_info():
    """è¿ç§» stock_basic_info é›†åˆ"""

    # ğŸ”¥ ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¿æ¥ä¿¡æ¯
    settings = get_settings()
    client = AsyncIOMotorClient(settings.MONGO_URI)
    db = client[settings.MONGO_DB]
    collection = db["stock_basic_info"]
    
    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹è¿ç§» stock_basic_info é›†åˆ")
        logger.info("=" * 60)
        
        # æ­¥éª¤1ï¼šæ£€æŸ¥ç°æœ‰æ•°æ®
        logger.info("\nğŸ“Š æ­¥éª¤1ï¼šæ£€æŸ¥ç°æœ‰æ•°æ®")
        total_count = await collection.count_documents({})
        logger.info(f"   æ€»è®°å½•æ•°: {total_count}")
        
        # ç»Ÿè®¡å„æ•°æ®æºçš„è®°å½•æ•°
        pipeline = [
            {"$group": {"_id": "$source", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}}
        ]
        source_stats = await collection.aggregate(pipeline).to_list(None)
        
        logger.info("   æ•°æ®æºåˆ†å¸ƒ:")
        for stat in source_stats:
            source = stat["_id"] if stat["_id"] else "æ—  source å­—æ®µ"
            count = stat["count"]
            logger.info(f"      {source}: {count} æ¡")
        
        # æ­¥éª¤2ï¼šä¸ºæ²¡æœ‰ source å­—æ®µçš„æ•°æ®æ·»åŠ é»˜è®¤å€¼
        logger.info("\nğŸ”§ æ­¥éª¤2ï¼šä¸ºæ²¡æœ‰ source å­—æ®µçš„æ•°æ®æ·»åŠ é»˜è®¤å€¼")
        no_source_count = await collection.count_documents({"source": {"$exists": False}})
        
        if no_source_count > 0:
            logger.info(f"   å‘ç° {no_source_count} æ¡è®°å½•æ²¡æœ‰ source å­—æ®µ")
            logger.info("   å°†ä¸ºè¿™äº›è®°å½•æ·»åŠ  source='unknown'")
            
            result = await collection.update_many(
                {"source": {"$exists": False}},
                {"$set": {"source": "unknown", "updated_at": datetime.now()}}
            )
            logger.info(f"   âœ… å·²æ›´æ–° {result.modified_count} æ¡è®°å½•")
        else:
            logger.info("   âœ… æ‰€æœ‰è®°å½•éƒ½æœ‰ source å­—æ®µ")
        
        # æ­¥éª¤3ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„ (code, source) ç»„åˆ
        logger.info("\nğŸ” æ­¥éª¤3ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„ (code, source) ç»„åˆ")
        pipeline = [
            {"$group": {
                "_id": {"code": "$code", "source": "$source"},
                "count": {"$sum": 1},
                "ids": {"$push": "$_id"}
            }},
            {"$match": {"count": {"$gt": 1}}}
        ]
        duplicates = await collection.aggregate(pipeline).to_list(None)
        
        if duplicates:
            logger.warning(f"   âš ï¸ å‘ç° {len(duplicates)} ç»„é‡å¤æ•°æ®")
            logger.info("   å¤„ç†é‡å¤æ•°æ®ï¼ˆä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤æ—§çš„ï¼‰...")
            
            for dup in duplicates:
                code = dup["_id"]["code"]
                source = dup["_id"]["source"]
                ids = dup["ids"]
                
                # è·å–æ‰€æœ‰é‡å¤è®°å½•ï¼ŒæŒ‰ updated_at æ’åº
                docs = await collection.find(
                    {"_id": {"$in": ids}}
                ).sort("updated_at", -1).to_list(None)
                
                # ä¿ç•™ç¬¬ä¸€æ¡ï¼ˆæœ€æ–°çš„ï¼‰ï¼Œåˆ é™¤å…¶ä»–çš„
                keep_id = docs[0]["_id"]
                delete_ids = [doc["_id"] for doc in docs[1:]]
                
                if delete_ids:
                    result = await collection.delete_many({"_id": {"$in": delete_ids}})
                    logger.info(f"      åˆ é™¤é‡å¤è®°å½•: code={code}, source={source}, åˆ é™¤ {result.deleted_count} æ¡")
        else:
            logger.info("   âœ… æ²¡æœ‰é‡å¤çš„ (code, source) ç»„åˆ")
        
        # æ­¥éª¤4ï¼šåˆ é™¤æ—§çš„å”¯ä¸€ç´¢å¼•
        logger.info("\nğŸ—‘ï¸  æ­¥éª¤4ï¼šåˆ é™¤æ—§çš„å”¯ä¸€ç´¢å¼•")
        indexes = await collection.index_information()

        # æŸ¥æ‰¾ code å”¯ä¸€ç´¢å¼•
        code_unique_index = None
        for idx_name, idx_info in indexes.items():
            if idx_info.get("unique") and idx_info.get("key") == [("code", 1)]:
                code_unique_index = idx_name
                break

        if code_unique_index:
            logger.info(f"   å‘ç°æ—§çš„ code å”¯ä¸€ç´¢å¼•: {code_unique_index}")
            await collection.drop_index(code_unique_index)
            logger.info(f"   âœ… å·²åˆ é™¤ç´¢å¼•: {code_unique_index}")
        else:
            logger.info("   âš ï¸ æœªæ‰¾åˆ° code å”¯ä¸€ç´¢å¼•ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰")

        # ğŸ”¥ æŸ¥æ‰¾å¹¶åˆ é™¤ full_symbol å”¯ä¸€ç´¢å¼•
        full_symbol_unique_index = None
        for idx_name, idx_info in indexes.items():
            if idx_info.get("unique") and idx_info.get("key") == [("full_symbol", 1)]:
                full_symbol_unique_index = idx_name
                break

        if full_symbol_unique_index:
            logger.info(f"   å‘ç°æ—§çš„ full_symbol å”¯ä¸€ç´¢å¼•: {full_symbol_unique_index}")
            await collection.drop_index(full_symbol_unique_index)
            logger.info(f"   âœ… å·²åˆ é™¤ç´¢å¼•: {full_symbol_unique_index}")
        else:
            logger.info("   âš ï¸ æœªæ‰¾åˆ° full_symbol å”¯ä¸€ç´¢å¼•ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰")
        
        # æ­¥éª¤5ï¼šåˆ›å»ºæ–°çš„ (code, source) è”åˆå”¯ä¸€ç´¢å¼•
        logger.info("\nğŸ”§ æ­¥éª¤5ï¼šåˆ›å»ºæ–°çš„ (code, source) è”åˆå”¯ä¸€ç´¢å¼•")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_index = None
        indexes = await collection.index_information()
        for idx_name, idx_info in indexes.items():
            if idx_info.get("key") == [("code", 1), ("source", 1)]:
                existing_index = idx_name
                break
        
        if existing_index:
            logger.info(f"   âš ï¸ ç´¢å¼•å·²å­˜åœ¨: {existing_index}")
        else:
            await collection.create_index(
                [("code", ASCENDING), ("source", ASCENDING)],
                unique=True,
                name="uniq_code_source"
            )
            logger.info("   âœ… å·²åˆ›å»ºè”åˆå”¯ä¸€ç´¢å¼•: uniq_code_source")
        
        # æ­¥éª¤6ï¼šåˆ›å»ºè¾…åŠ©ç´¢å¼•
        logger.info("\nğŸ”§ æ­¥éª¤6ï¼šåˆ›å»ºè¾…åŠ©ç´¢å¼•")
        
        # code éå”¯ä¸€ç´¢å¼•ï¼ˆç”¨äºæŸ¥è¯¢æ‰€æœ‰æ•°æ®æºï¼‰
        await collection.create_index([("code", ASCENDING)], name="idx_code")
        logger.info("   âœ… å·²åˆ›å»ºç´¢å¼•: idx_code")
        
        # source ç´¢å¼•ï¼ˆç”¨äºæŒ‰æ•°æ®æºæŸ¥è¯¢ï¼‰
        await collection.create_index([("source", ASCENDING)], name="idx_source")
        logger.info("   âœ… å·²åˆ›å»ºç´¢å¼•: idx_source")
        
        # æ­¥éª¤7ï¼šéªŒè¯è¿ç§»ç»“æœ
        logger.info("\nâœ… æ­¥éª¤7ï¼šéªŒè¯è¿ç§»ç»“æœ")
        
        # é‡æ–°ç»Ÿè®¡æ•°æ®
        total_count_after = await collection.count_documents({})
        logger.info(f"   è¿ç§»åæ€»è®°å½•æ•°: {total_count_after}")
        
        # ç»Ÿè®¡å„æ•°æ®æºçš„è®°å½•æ•°
        source_stats_after = await collection.aggregate(pipeline).to_list(None)
        logger.info("   è¿ç§»åæ•°æ®æºåˆ†å¸ƒ:")
        for stat in source_stats_after:
            source = stat["_id"] if stat["_id"] else "æ—  source å­—æ®µ"
            count = stat["count"]
            logger.info(f"      {source}: {count} æ¡")
        
        # åˆ—å‡ºæ‰€æœ‰ç´¢å¼•
        indexes_after = await collection.index_information()
        logger.info("   å½“å‰ç´¢å¼•:")
        for idx_name, idx_info in indexes_after.items():
            unique = " (å”¯ä¸€)" if idx_info.get("unique") else ""
            logger.info(f"      {idx_name}: {idx_info.get('key')}{unique}")
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… è¿ç§»å®Œæˆï¼")
        logger.info("=" * 60)
        
        # æç¤º
        logger.info("\nğŸ“ åç»­æ­¥éª¤:")
        logger.info("   1. é‡æ–°è¿è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡ï¼Œç¡®ä¿æ¯ä¸ªæ•°æ®æºç‹¬ç«‹å­˜å‚¨")
        logger.info("   2. æŸ¥è¯¢æ—¶å¯ä»¥æŒ‡å®š source å‚æ•°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤ä¼˜å…ˆçº§")
        logger.info("   3. ç›‘æ§æ—¥å¿—ï¼Œç¡®è®¤æ•°æ®æºéš”ç¦»æ­£å¸¸å·¥ä½œ")
        
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}", exc_info=True)
        raise
    finally:
        client.close()


async def rollback_migration():
    """å›æ»šè¿ç§»ï¼ˆæ¢å¤åˆ°å•æ•°æ®æºæ¨¡å¼ï¼‰"""
    
    client = AsyncIOMotorClient("mongodb://localhost:27017")
    db = client["tradingagents"]
    collection = db["stock_basic_info"]
    
    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹å›æ»šè¿ç§»")
        logger.info("=" * 60)
        
        # åˆ é™¤è”åˆå”¯ä¸€ç´¢å¼•
        logger.info("\nğŸ—‘ï¸  åˆ é™¤ (code, source) è”åˆå”¯ä¸€ç´¢å¼•")
        try:
            await collection.drop_index("uniq_code_source")
            logger.info("   âœ… å·²åˆ é™¤ç´¢å¼•: uniq_code_source")
        except Exception as e:
            logger.warning(f"   âš ï¸ åˆ é™¤ç´¢å¼•å¤±è´¥: {e}")
        
        # åˆ é™¤è¾…åŠ©ç´¢å¼•
        try:
            await collection.drop_index("idx_source")
            logger.info("   âœ… å·²åˆ é™¤ç´¢å¼•: idx_source")
        except Exception as e:
            logger.warning(f"   âš ï¸ åˆ é™¤ç´¢å¼•å¤±è´¥: {e}")
        
        # æ¢å¤ code å”¯ä¸€ç´¢å¼•
        logger.info("\nğŸ”§ æ¢å¤ code å”¯ä¸€ç´¢å¼•")
        
        # å…ˆåˆ é™¤é‡å¤æ•°æ®ï¼ˆä¿ç•™ tushare æ•°æ®æºï¼‰
        logger.info("   å¤„ç†é‡å¤æ•°æ®ï¼ˆä¿ç•™ tushare æ•°æ®æºï¼‰...")
        pipeline = [
            {"$group": {
                "_id": "$code",
                "count": {"$sum": 1},
                "docs": {"$push": "$$ROOT"}
            }},
            {"$match": {"count": {"$gt": 1}}}
        ]
        duplicates = await collection.aggregate(pipeline).to_list(None)
        
        if duplicates:
            logger.info(f"   å‘ç° {len(duplicates)} åªè‚¡ç¥¨æœ‰å¤šä¸ªæ•°æ®æº")
            
            for dup in duplicates:
                code = dup["_id"]
                docs = dup["docs"]
                
                # ä¼˜å…ˆä¿ç•™ tushareï¼Œå…¶æ¬¡ multi_sourceï¼Œæœ€åå…¶ä»–
                priority = {"tushare": 3, "multi_source": 2}
                docs_sorted = sorted(docs, key=lambda x: priority.get(x.get("source"), 1), reverse=True)
                
                keep_id = docs_sorted[0]["_id"]
                delete_ids = [doc["_id"] for doc in docs_sorted[1:]]
                
                if delete_ids:
                    result = await collection.delete_many({"_id": {"$in": delete_ids}})
                    logger.info(f"      code={code}: ä¿ç•™ {docs_sorted[0].get('source')}ï¼Œåˆ é™¤ {result.deleted_count} æ¡")
        
        # åˆ›å»º code å”¯ä¸€ç´¢å¼•
        await collection.create_index([("code", ASCENDING)], unique=True, name="uniq_code")
        logger.info("   âœ… å·²åˆ›å»ºç´¢å¼•: uniq_code")
        
        logger.info("\nâœ… å›æ»šå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ å›æ»šå¤±è´¥: {e}", exc_info=True)
        raise
    finally:
        client.close()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "rollback":
        # å›æ»šæ¨¡å¼
        asyncio.run(rollback_migration())
    else:
        # æ­£å¸¸è¿ç§»
        asyncio.run(migrate_stock_basic_info())

