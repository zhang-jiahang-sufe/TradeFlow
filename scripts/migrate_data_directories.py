#!/usr/bin/env python3
"""
æ•°æ®ç›®å½•é‡æ–°ç»„ç»‡è¿ç§»è„šæœ¬
Data Directory Reorganization Migration Script

æ­¤è„šæœ¬å°†é¡¹ç›®ä¸­åˆ†æ•£çš„æ•°æ®ç›®å½•é‡æ–°ç»„ç»‡ä¸ºç»Ÿä¸€çš„ç»“æ„
"""

import os
import shutil
import json
from pathlib import Path
from typing import Dict, List, Tuple
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
os.makedirs(os.path.join('data', 'logs'), exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join('data', 'logs', 'data_migration.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataDirectoryMigrator:
    """æ•°æ®ç›®å½•è¿ç§»å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent
        self.backup_dir = self.project_root / f"data_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ–°çš„ç›®å½•ç»“æ„
        self.new_structure = {
            'data': {
                'cache': ['stock_data', 'news_data', 'fundamentals', 'metadata'],
                'analysis_results': ['summary', 'detailed', 'exports'],
                'databases': ['mongodb', 'redis'],
                'sessions': ['web_sessions', 'cli_sessions'],
                'logs': ['application', 'operations', 'user_activities'],
                'config': ['user_configs', 'system_configs'],
                'temp': ['downloads', 'processing']
            }
        }
        
        # è¿ç§»æ˜ å°„ï¼š(æºè·¯å¾„, ç›®æ ‡è·¯å¾„)
        self.migration_map = [
            # ç¼“å­˜æ•°æ®è¿ç§»
            ('tradingagents/dataflows/data_cache', 'data/cache'),
            
            # åˆ†æç»“æœè¿ç§»
            ('results', 'data/analysis_results/detailed'),
            ('web/data/analysis_results', 'data/analysis_results/summary'),
            
            # æ•°æ®åº“æ•°æ®è¿ç§»
            ('data/mongodb', 'data/databases/mongodb'),
            ('data/redis', 'data/databases/redis'),
            
            # ä¼šè¯æ•°æ®è¿ç§»
            ('data/sessions', 'data/sessions/cli_sessions'),
            ('web/data/sessions', 'data/sessions/web_sessions'),
            
            # æ—¥å¿—æ•°æ®è¿ç§»
            ('web/data/operation_logs', 'data/logs/operations'),
            ('web/data/user_activities', 'data/logs/user_activities'),
            
            # æŠ¥å‘Šæ•°æ®è¿ç§»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            ('data/reports', 'data/analysis_results/exports'),
        ]
    
    def create_backup(self) -> bool:
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºæ•°æ®å¤‡ä»½åˆ°: {self.backup_dir}")
            self.backup_dir.mkdir(exist_ok=True)
            
            # å¤‡ä»½ç°æœ‰æ•°æ®ç›®å½•
            backup_paths = ['data', 'web/data', 'results', 'tradingagents/dataflows/data_cache']
            
            for path in backup_paths:
                source = self.project_root / path
                if source.exists():
                    target = self.backup_dir / path
                    target.parent.mkdir(parents=True, exist_ok=True)
                    
                    if source.is_dir():
                        shutil.copytree(source, target, dirs_exist_ok=True)
                    else:
                        shutil.copy2(source, target)
                    
                    logger.info(f"  âœ… å·²å¤‡ä»½: {path}")
            
            logger.info(f"âœ… æ•°æ®å¤‡ä»½å®Œæˆ: {self.backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def create_new_structure(self) -> bool:
        """åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„"""
        try:
            logger.info("ğŸ”„ åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„...")
            
            for root_dir, subdirs in self.new_structure.items():
                root_path = self.project_root / root_dir
                root_path.mkdir(exist_ok=True)
                
                if isinstance(subdirs, dict):
                    for subdir, sub_subdirs in subdirs.items():
                        subdir_path = root_path / subdir
                        subdir_path.mkdir(exist_ok=True)
                        
                        for sub_subdir in sub_subdirs:
                            (subdir_path / sub_subdir).mkdir(exist_ok=True)
                            
                        logger.info(f"  âœ… åˆ›å»ºç›®å½•: {subdir_path.relative_to(self.project_root)}")
                elif isinstance(subdirs, list):
                    for subdir in subdirs:
                        subdir_path = root_path / subdir
                        subdir_path.mkdir(exist_ok=True)
                        logger.info(f"  âœ… åˆ›å»ºç›®å½•: {subdir_path.relative_to(self.project_root)}")
            
            logger.info("âœ… æ–°ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç›®å½•ç»“æ„å¤±è´¥: {e}")
            return False
    
    def migrate_data(self) -> bool:
        """è¿ç§»æ•°æ®"""
        try:
            logger.info("ğŸ”„ å¼€å§‹æ•°æ®è¿ç§»...")
            
            for source_path, target_path in self.migration_map:
                source = self.project_root / source_path
                target = self.project_root / target_path
                
                if not source.exists():
                    logger.info(f"  â­ï¸ è·³è¿‡ä¸å­˜åœ¨çš„è·¯å¾„: {source_path}")
                    continue
                
                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                target.parent.mkdir(parents=True, exist_ok=True)
                
                try:
                    if source.is_dir():
                        # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œåˆå¹¶å†…å®¹
                        if target.exists():
                            self._merge_directories(source, target)
                        else:
                            shutil.copytree(source, target)
                    else:
                        shutil.copy2(source, target)
                    
                    logger.info(f"  âœ… è¿ç§»å®Œæˆ: {source_path} â†’ {target_path}")
                    
                except Exception as e:
                    logger.error(f"  âŒ è¿ç§»å¤±è´¥: {source_path} â†’ {target_path}, é”™è¯¯: {e}")
            
            logger.info("âœ… æ•°æ®è¿ç§»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}")
            return False
    
    def _merge_directories(self, source: Path, target: Path):
        """åˆå¹¶ç›®å½•å†…å®¹"""
        for item in source.rglob('*'):
            if item.is_file():
                relative_path = item.relative_to(source)
                target_file = target / relative_path
                target_file.parent.mkdir(parents=True, exist_ok=True)
                
                # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œé‡å‘½å
                if target_file.exists():
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    target_file = target_file.with_name(f"{target_file.stem}_{timestamp}{target_file.suffix}")
                
                shutil.copy2(item, target_file)
    
    def update_env_file(self) -> bool:
        """æ›´æ–°.envæ–‡ä»¶"""
        try:
            logger.info("ğŸ”„ æ›´æ–°.envæ–‡ä»¶...")
            
            env_file = self.project_root / '.env'
            if not env_file.exists():
                logger.warning("âš ï¸ .envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
                return True
            
            # è¯»å–ç°æœ‰å†…å®¹
            with open(env_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ·»åŠ æ–°çš„ç¯å¢ƒå˜é‡é…ç½®
            new_config = """
# ===== æ•°æ®ç›®å½•é…ç½® (é‡æ–°ç»„ç»‡å) =====
# ç»Ÿä¸€æ•°æ®æ ¹ç›®å½•
TRADINGAGENTS_DATA_DIR=./data

# å­ç›®å½•é…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
TRADINGAGENTS_CACHE_DIR=${TRADINGAGENTS_DATA_DIR}/cache
TRADINGAGENTS_SESSIONS_DIR=${TRADINGAGENTS_DATA_DIR}/sessions
TRADINGAGENTS_LOGS_DIR=${TRADINGAGENTS_DATA_DIR}/logs
TRADINGAGENTS_CONFIG_DIR=${TRADINGAGENTS_DATA_DIR}/config
TRADINGAGENTS_TEMP_DIR=${TRADINGAGENTS_DATA_DIR}/temp

# æ›´æ–°ç»“æœç›®å½•é…ç½®
TRADINGAGENTS_RESULTS_DIR=${TRADINGAGENTS_DATA_DIR}/analysis_results
"""
            
            # å¦‚æœè¿˜æ²¡æœ‰è¿™äº›é…ç½®ï¼Œåˆ™æ·»åŠ 
            if 'TRADINGAGENTS_DATA_DIR' not in content:
                content += new_config
                
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                logger.info("âœ… .envæ–‡ä»¶æ›´æ–°å®Œæˆ")
            else:
                logger.info("â„¹ï¸ .envæ–‡ä»¶å·²åŒ…å«æ•°æ®ç›®å½•é…ç½®")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°.envæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def create_migration_report(self) -> bool:
        """åˆ›å»ºè¿ç§»æŠ¥å‘Š"""
        try:
            report = {
                'migration_date': datetime.now().isoformat(),
                'project_root': str(self.project_root),
                'backup_location': str(self.backup_dir),
                'new_structure': self.new_structure,
                'migration_map': self.migration_map,
                'status': 'completed'
            }
            
            report_file = self.project_root / 'data_migration_report.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… è¿ç§»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¿ç§»æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def cleanup_old_directories(self, confirm: bool = False) -> bool:
        """æ¸…ç†æ—§ç›®å½•ï¼ˆå¯é€‰ï¼‰"""
        if not confirm:
            logger.info("âš ï¸ è·³è¿‡æ¸…ç†æ—§ç›®å½•ï¼ˆéœ€è¦æ‰‹åŠ¨ç¡®è®¤ï¼‰")
            return True
        
        try:
            logger.info("ğŸ”„ æ¸…ç†æ—§ç›®å½•...")
            
            # è¦æ¸…ç†çš„æ—§ç›®å½•
            old_dirs = [
                'web/data',
                'tradingagents/dataflows/data_cache'
            ]
            
            for old_dir in old_dirs:
                old_path = self.project_root / old_dir
                if old_path.exists():
                    shutil.rmtree(old_path)
                    logger.info(f"  âœ… å·²åˆ é™¤: {old_dir}")
            
            logger.info("âœ… æ—§ç›®å½•æ¸…ç†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§ç›®å½•å¤±è´¥: {e}")
            return False
    
    def run_migration(self, cleanup_old: bool = False) -> bool:
        """è¿è¡Œå®Œæ•´çš„è¿ç§»æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®ç›®å½•é‡æ–°ç»„ç»‡è¿ç§»...")
        
        steps = [
            ("åˆ›å»ºå¤‡ä»½", self.create_backup),
            ("åˆ›å»ºæ–°ç›®å½•ç»“æ„", self.create_new_structure),
            ("è¿ç§»æ•°æ®", self.migrate_data),
            ("æ›´æ–°ç¯å¢ƒå˜é‡", self.update_env_file),
            ("åˆ›å»ºè¿ç§»æŠ¥å‘Š", self.create_migration_report),
        ]
        
        if cleanup_old:
            steps.append(("æ¸…ç†æ—§ç›®å½•", lambda: self.cleanup_old_directories(True)))
        
        for step_name, step_func in steps:
            logger.info(f"\nğŸ“‹ æ‰§è¡Œæ­¥éª¤: {step_name}")
            if not step_func():
                logger.error(f"âŒ æ­¥éª¤å¤±è´¥: {step_name}")
                return False
        
        logger.info("\nğŸ‰ æ•°æ®ç›®å½•é‡æ–°ç»„ç»‡å®Œæˆï¼")
        logger.info(f"ğŸ“ å¤‡ä»½ä½ç½®: {self.backup_dir}")
        logger.info(f"ğŸ“Š æ–°æ•°æ®ç›®å½•: {self.project_root / 'data'}")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®ç›®å½•é‡æ–°ç»„ç»‡è¿ç§»è„šæœ¬')
    parser.add_argument('--project-root', help='é¡¹ç›®æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--cleanup-old', action='store_true', help='è¿ç§»åæ¸…ç†æ—§ç›®å½•')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ˜¾ç¤ºè¿ç§»è®¡åˆ’ï¼Œä¸æ‰§è¡Œå®é™…è¿ç§»')
    
    args = parser.parse_args()
    
    migrator = DataDirectoryMigrator(args.project_root)
    
    if args.dry_run:
        logger.info("ğŸ” è¿ç§»è®¡åˆ’é¢„è§ˆ:")
        logger.info(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {migrator.project_root}")
        logger.info(f"ğŸ“ å¤‡ä»½ç›®å½•: {migrator.backup_dir}")
        logger.info("\nğŸ“‹ è¿ç§»æ˜ å°„:")
        for source, target in migrator.migration_map:
            logger.info(f"  {source} â†’ {target}")
        return
    
    # æ‰§è¡Œè¿ç§»
    success = migrator.run_migration(cleanup_old=args.cleanup_old)
    
    if success:
        logger.info("\nâœ… è¿ç§»æˆåŠŸå®Œæˆï¼")
        logger.info("\nğŸ“ åç»­æ­¥éª¤:")
        logger.info("1. éªŒè¯æ–°ç›®å½•ç»“æ„æ˜¯å¦æ­£ç¡®")
        logger.info("2. æµ‹è¯•åº”ç”¨ç¨‹åºåŠŸèƒ½")
        logger.info("3. ç¡®è®¤æ— è¯¯åå¯åˆ é™¤å¤‡ä»½ç›®å½•")
    else:
        logger.error("\nâŒ è¿ç§»å¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—å¹¶ä»å¤‡ä»½æ¢å¤")


if __name__ == '__main__':
    main()