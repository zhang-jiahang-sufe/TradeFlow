#!/usr/bin/env python3
"""
MongoDB ç´¢å¼•ä¼˜åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
2. ä¸º stock_daily_quotes é›†åˆåˆ›å»ºä¼˜åŒ–ç´¢å¼•
3. åˆ é™¤å†—ä½™ç´¢å¼•
4. ç”Ÿæˆç´¢å¼•ä½¿ç”¨æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/maintenance/optimize_mongodb_indexes.py
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import settings
from app.core.logging_config import logger


async def analyze_existing_indexes(collection):
    """åˆ†æç°æœ‰ç´¢å¼•"""
    logger.info("ğŸ“Š åˆ†æç°æœ‰ç´¢å¼•...")
    
    indexes = await collection.list_indexes().to_list(length=None)
    
    logger.info(f"\nå½“å‰ç´¢å¼•åˆ—è¡¨ï¼ˆå…± {len(indexes)} ä¸ªï¼‰ï¼š")
    for idx in indexes:
        name = idx.get("name", "unknown")
        keys = idx.get("key", {})
        unique = idx.get("unique", False)
        
        # æ ¼å¼åŒ–ç´¢å¼•é”®
        key_str = ", ".join([f"{k}: {v}" for k, v in keys.items()])
        unique_str = " [UNIQUE]" if unique else ""
        
        logger.info(f"  - {name}: {{ {key_str} }}{unique_str}")
    
    return indexes


async def create_optimized_indexes(collection):
    """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
    logger.info("\nğŸ”§ åˆ›å»ºä¼˜åŒ–ç´¢å¼•...")
    
    indexes_to_create = [
        {
            "name": "symbol_date_source_period_unique",
            "keys": [("symbol", 1), ("trade_date", 1), ("data_source", 1), ("period", 1)],
            "unique": True,
            "description": "å¤åˆå”¯ä¸€ç´¢å¼•ï¼šé˜²æ­¢é‡å¤æ•°æ®"
        },
        {
            "name": "symbol_period_date_idx",
            "keys": [("symbol", 1), ("period", 1), ("trade_date", -1)],
            "unique": False,
            "description": "æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•ï¼šæŒ‰è‚¡ç¥¨ä»£ç +å‘¨æœŸæŸ¥è¯¢å†å²æ•°æ®"
        },
        {
            "name": "symbol_date_idx",
            "keys": [("symbol", 1), ("trade_date", -1)],
            "unique": False,
            "description": "æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•ï¼šæŒ‰è‚¡ç¥¨ä»£ç æŸ¥è¯¢å†å²æ•°æ®"
        },
        {
            "name": "date_idx",
            "keys": [("trade_date", -1)],
            "unique": False,
            "description": "æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•ï¼šæŒ‰æ—¥æœŸæŸ¥è¯¢æ•°æ®"
        },
        {
            "name": "data_source_idx",
            "keys": [("data_source", 1)],
            "unique": False,
            "description": "æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•ï¼šæŒ‰æ•°æ®æºæŸ¥è¯¢"
        },
        {
            "name": "symbol_source_date_period_idx",
            "keys": [("symbol", 1), ("data_source", 1), ("trade_date", -1), ("period", 1)],
            "unique": False,
            "description": "ğŸ”¥ æ…¢æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•ï¼šåŒ¹é… update æ“ä½œçš„æŸ¥è¯¢æ¡ä»¶é¡ºåº"
        }
    ]
    
    created_count = 0
    skipped_count = 0
    
    for idx_config in indexes_to_create:
        name = idx_config["name"]
        keys = idx_config["keys"]
        unique = idx_config["unique"]
        description = idx_config["description"]
        
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            existing_indexes = await collection.list_indexes().to_list(length=None)
            index_exists = any(idx.get("name") == name for idx in existing_indexes)
            
            if index_exists:
                logger.info(f"â­ï¸  ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡: {name}")
                skipped_count += 1
                continue
            
            # åˆ›å»ºç´¢å¼•
            await collection.create_index(
                keys,
                unique=unique,
                name=name,
                background=True  # åå°åˆ›å»ºï¼Œä¸é˜»å¡æ•°æ®åº“æ“ä½œ
            )
            
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {name}")
            logger.info(f"   æè¿°: {description}")
            logger.info(f"   é”®: {keys}")
            created_count += 1
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {name}, é”™è¯¯: {e}")
    
    logger.info(f"\nğŸ“Š ç´¢å¼•åˆ›å»ºå®Œæˆ: æ–°å»º {created_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª")
    
    return created_count


async def drop_redundant_indexes(collection):
    """åˆ é™¤å†—ä½™ç´¢å¼•ï¼ˆå¯é€‰ï¼‰"""
    logger.info("\nğŸ—‘ï¸  æ£€æŸ¥å†—ä½™ç´¢å¼•...")
    
    # è¿™é‡Œå¯ä»¥å®šä¹‰éœ€è¦åˆ é™¤çš„å†—ä½™ç´¢å¼•
    # æ³¨æ„ï¼š_id_ ç´¢å¼•ä¸èƒ½åˆ é™¤
    redundant_indexes = [
        # ç¤ºä¾‹ï¼šå¦‚æœæœ‰æ—§çš„ç´¢å¼•éœ€è¦åˆ é™¤ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        # "old_index_name",
    ]
    
    dropped_count = 0
    
    for index_name in redundant_indexes:
        try:
            await collection.drop_index(index_name)
            logger.info(f"âœ… åˆ é™¤å†—ä½™ç´¢å¼•: {index_name}")
            dropped_count += 1
        except Exception as e:
            logger.warning(f"âš ï¸  åˆ é™¤ç´¢å¼•å¤±è´¥: {index_name}, é”™è¯¯: {e}")
    
    if dropped_count == 0:
        logger.info("âœ… æ²¡æœ‰éœ€è¦åˆ é™¤çš„å†—ä½™ç´¢å¼•")
    else:
        logger.info(f"ğŸ“Š åˆ é™¤äº† {dropped_count} ä¸ªå†—ä½™ç´¢å¼•")
    
    return dropped_count


async def get_collection_stats(collection):
    """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
    logger.info("\nğŸ“Š è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = await collection.database.command("collStats", collection.name)
        
        count = stats.get("count", 0)
        size = stats.get("size", 0) / (1024 * 1024)  # MB
        avg_obj_size = stats.get("avgObjSize", 0)
        storage_size = stats.get("storageSize", 0) / (1024 * 1024)  # MB
        total_index_size = stats.get("totalIndexSize", 0) / (1024 * 1024)  # MB
        
        logger.info(f"  - æ–‡æ¡£æ•°é‡: {count:,}")
        logger.info(f"  - æ•°æ®å¤§å°: {size:.2f} MB")
        logger.info(f"  - å¹³å‡æ–‡æ¡£å¤§å°: {avg_obj_size:.2f} bytes")
        logger.info(f"  - å­˜å‚¨å¤§å°: {storage_size:.2f} MB")
        logger.info(f"  - ç´¢å¼•æ€»å¤§å°: {total_index_size:.2f} MB")
        
        return stats
    except Exception as e:
        logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return None


async def test_query_performance(collection):
    """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
    logger.info("\nğŸ§ª æµ‹è¯•æŸ¥è¯¢æ€§èƒ½...")
    
    # æµ‹è¯•æ…¢æŸ¥è¯¢åœºæ™¯
    test_queries = [
        {
            "name": "æ…¢æŸ¥è¯¢åœºæ™¯ï¼ˆupdateæ¡ä»¶ï¼‰",
            "filter": {
                "symbol": "688188",
                "trade_date": "2024-12-10",
                "data_source": "tushare",
                "period": "daily"
            }
        },
        {
            "name": "æŒ‰è‚¡ç¥¨ä»£ç æŸ¥è¯¢",
            "filter": {
                "symbol": "000001"
            }
        },
        {
            "name": "æŒ‰è‚¡ç¥¨ä»£ç +æ—¥æœŸèŒƒå›´æŸ¥è¯¢",
            "filter": {
                "symbol": "000001",
                "trade_date": {"$gte": "2024-01-01", "$lte": "2024-12-31"}
            }
        }
    ]
    
    for test in test_queries:
        name = test["name"]
        filter_query = test["filter"]
        
        try:
            # ä½¿ç”¨ explain åˆ†ææŸ¥è¯¢è®¡åˆ’
            explain = await collection.find(filter_query).explain()
            
            execution_stats = explain.get("executionStats", {})
            execution_time_ms = execution_stats.get("executionTimeMillis", 0)
            total_docs_examined = execution_stats.get("totalDocsExamined", 0)
            total_keys_examined = execution_stats.get("totalKeysExamined", 0)
            n_returned = execution_stats.get("nReturned", 0)
            
            # è·å–æŸ¥è¯¢è®¡åˆ’
            winning_plan = explain.get("queryPlanner", {}).get("winningPlan", {})
            input_stage = winning_plan.get("inputStage", {})
            stage = input_stage.get("stage", "UNKNOWN")
            index_name = input_stage.get("indexName", "æ— ç´¢å¼•")
            
            logger.info(f"\n  æµ‹è¯•: {name}")
            logger.info(f"    - æ‰§è¡Œæ—¶é—´: {execution_time_ms} ms")
            logger.info(f"    - æ‰«ææ–‡æ¡£æ•°: {total_docs_examined}")
            logger.info(f"    - æ‰«æç´¢å¼•é”®æ•°: {total_keys_examined}")
            logger.info(f"    - è¿”å›æ–‡æ¡£æ•°: {n_returned}")
            logger.info(f"    - æŸ¥è¯¢é˜¶æ®µ: {stage}")
            logger.info(f"    - ä½¿ç”¨ç´¢å¼•: {index_name}")
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•
            if stage == "COLLSCAN":
                logger.warning(f"    âš ï¸  è­¦å‘Š: å…¨é›†åˆæ‰«æï¼ˆCOLLSCANï¼‰ï¼Œå»ºè®®æ·»åŠ ç´¢å¼•ï¼")
            elif stage == "IXSCAN":
                logger.info(f"    âœ… ä½¿ç”¨äº†ç´¢å¼•æ‰«æï¼ˆIXSCANï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {name}, é”™è¯¯: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ MongoDB ç´¢å¼•ä¼˜åŒ–...")
    logger.info(f"ğŸ“ æ•°æ®åº“: {settings.MONGO_DB}")
    logger.info(f"ğŸ“ é›†åˆ: stock_daily_quotes")
    
    try:
        # è¿æ¥ MongoDB
        client = AsyncIOMotorClient(settings.MONGO_URI)
        db = client[settings.MONGO_DB]
        collection = db.stock_daily_quotes
        
        # 1. åˆ†æç°æœ‰ç´¢å¼•
        await analyze_existing_indexes(collection)
        
        # 2. è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        await get_collection_stats(collection)
        
        # 3. åˆ›å»ºä¼˜åŒ–ç´¢å¼•
        created_count = await create_optimized_indexes(collection)
        
        # 4. åˆ é™¤å†—ä½™ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
        # await drop_redundant_indexes(collection)
        
        # 5. æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        await test_query_performance(collection)
        
        # å…³é—­è¿æ¥
        client.close()
        
        logger.info("\nâœ… MongoDB ç´¢å¼•ä¼˜åŒ–å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ–°å»ºç´¢å¼•: {created_count} ä¸ª")
        
        if created_count > 0:
            logger.info("\nğŸ’¡ å»ºè®®:")
            logger.info("  1. ç›‘æ§æ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œç¡®è®¤ä¼˜åŒ–æ•ˆæœ")
            logger.info("  2. å®šæœŸè¿è¡Œæ­¤è„šæœ¬ï¼Œä¿æŒç´¢å¼•æœ€æ–°")
            logger.info("  3. å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œç´¢å¼•åˆ›å»ºå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

