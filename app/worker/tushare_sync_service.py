"""
Tushareæ•°æ®åŒæ­¥æœåŠ¡
è´Ÿè´£å°†Tushareæ•°æ®åŒæ­¥åˆ°MongoDBæ ‡å‡†åŒ–é›†åˆ
"""
import asyncio
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional
import logging

from tradingagents.dataflows.providers.china.tushare import TushareProvider
from app.services.stock_data_service import get_stock_data_service
from app.services.historical_data_service import get_historical_data_service
from app.services.news_data_service import get_news_data_service
from app.core.database import get_mongo_db
from app.core.config import settings
from app.core.rate_limiter import get_tushare_rate_limiter
from app.utils.timezone import now_tz

logger = logging.getLogger(__name__)

# UTC+8 æ—¶åŒº
UTC_8 = timezone(timedelta(hours=8))


def get_utc8_now():
    """
    è·å– UTC+8 å½“å‰æ—¶é—´ï¼ˆnaive datetimeï¼‰

    æ³¨æ„ï¼šè¿”å› naive datetimeï¼ˆä¸å¸¦æ—¶åŒºä¿¡æ¯ï¼‰ï¼ŒMongoDB ä¼šæŒ‰åŸæ ·å­˜å‚¨æœ¬åœ°æ—¶é—´å€¼
    è¿™æ ·å‰ç«¯å¯ä»¥ç›´æ¥æ·»åŠ  +08:00 åç¼€æ˜¾ç¤º
    """
    return now_tz().replace(tzinfo=None)


class TushareSyncService:
    """
    Tushareæ•°æ®åŒæ­¥æœåŠ¡
    è´Ÿè´£å°†Tushareæ•°æ®åŒæ­¥åˆ°MongoDBæ ‡å‡†åŒ–é›†åˆ
    """
    
    def __init__(self):
        self.provider = TushareProvider()
        self.stock_service = get_stock_data_service()
        self.historical_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.news_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.db = get_mongo_db()
        self.settings = settings

        # åŒæ­¥é…ç½®
        self.batch_size = 100  # æ‰¹é‡å¤„ç†å¤§å°
        self.rate_limit_delay = 0.1  # APIè°ƒç”¨é—´éš”(ç§’) - å·²å¼ƒç”¨ï¼Œä½¿ç”¨rate_limiter
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

        # é€Ÿç‡é™åˆ¶å™¨ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼‰
        tushare_tier = getattr(settings, "TUSHARE_TIER", "standard")  # free/basic/standard/premium/vip
        safety_margin = float(getattr(settings, "TUSHARE_RATE_LIMIT_SAFETY_MARGIN", "0.8"))
        self.rate_limiter = get_tushare_rate_limiter(tier=tushare_tier, safety_margin=safety_margin)
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥æœåŠ¡"""
        success = await self.provider.connect()
        if not success:
            raise RuntimeError("âŒ Tushareè¿æ¥å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨åŒæ­¥æœåŠ¡")

        # åˆå§‹åŒ–å†å²æ•°æ®æœåŠ¡
        self.historical_service = await get_historical_data_service()

        # åˆå§‹åŒ–æ–°é—»æ•°æ®æœåŠ¡
        self.news_service = await get_news_data_service()

        logger.info("âœ… TushareåŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    # ==================== åŸºç¡€ä¿¡æ¯åŒæ­¥ ====================
    
    async def sync_stock_basic_info(self, force_update: bool = False, job_id: str = None) -> Dict[str, Any]:
        """
        åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯

        Args:
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°æ‰€æœ‰æ•°æ®
            job_id: ä»»åŠ¡IDï¼ˆç”¨äºè¿›åº¦è·Ÿè¸ªï¼‰

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "start_time": datetime.utcnow(),
            "errors": []
        }
        
        try:
            # 1. ä»Tushareè·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = await self.provider.get_stock_list(market="CN")
            if not stock_list:
                logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return stats
            
            stats["total_processed"] = len(stock_list)
            logger.info(f"ğŸ“Š è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨ä¿¡æ¯")

            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(stock_list), self.batch_size):
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                if job_id and await self._should_stop(job_id):
                    logger.warning(f"âš ï¸ ä»»åŠ¡ {job_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
                    stats["stopped"] = True
                    break

                batch = stock_list[i:i + self.batch_size]
                batch_stats = await self._process_basic_info_batch(batch, force_update)

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["skipped_count"] += batch_stats["skipped_count"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—å’Œè¿›åº¦æ›´æ–°
                progress = min(i + self.batch_size, len(stock_list))
                progress_percent = int((progress / len(stock_list)) * 100)
                logger.info(f"ğŸ“ˆ åŸºç¡€ä¿¡æ¯åŒæ­¥è¿›åº¦: {progress}/{len(stock_list)} ({progress_percent}%) "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")

                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                if job_id:
                    await self._update_progress(
                        job_id,
                        progress_percent,
                        f"å·²å¤„ç† {progress}/{len(stock_list)} åªè‚¡ç¥¨"
                    )

                # APIé™æµ
                if i + self.batch_size < len(stock_list):
                    await asyncio.sleep(self.rate_limit_delay)
            
            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
            logger.info(f"âœ… è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆ: "
                       f"æ€»è®¡ {stats['total_processed']} åª, "
                       f"æˆåŠŸ {stats['success_count']} åª, "
                       f"é”™è¯¯ {stats['error_count']} åª, "
                       f"è·³è¿‡ {stats['skipped_count']} åª, "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_stock_basic_info"})
            return stats
    
    async def _process_basic_info_batch(self, batch: List[Dict[str, Any]], force_update: bool) -> Dict[str, Any]:
        """å¤„ç†åŸºç¡€ä¿¡æ¯æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "errors": []
        }
        
        for stock_info in batch:
            try:
                # ğŸ”¥ å…ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼‰
                if hasattr(stock_info, 'model_dump'):
                    stock_data = stock_info.model_dump()
                elif hasattr(stock_info, 'dict'):
                    stock_data = stock_info.dict()
                else:
                    stock_data = stock_info

                code = stock_data["code"]

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if not force_update:
                    existing = await self.stock_service.get_stock_basic_info(code)
                    if existing:
                        # ğŸ”¥ existing ä¹Ÿå¯èƒ½æ˜¯ Pydantic æ¨¡å‹ï¼Œéœ€è¦å®‰å…¨è·å–å±æ€§
                        existing_dict = existing.model_dump() if hasattr(existing, 'model_dump') else (existing.dict() if hasattr(existing, 'dict') else existing)
                        if self._is_data_fresh(existing_dict.get("updated_at"), hours=24):
                            batch_stats["skipped_count"] += 1
                            continue

                # æ›´æ–°åˆ°æ•°æ®åº“ï¼ˆæŒ‡å®šæ•°æ®æºä¸º tushareï¼‰
                success = await self.stock_service.update_stock_basic_info(code, stock_data, source="tushare")
                if success:
                    batch_stats["success_count"] += 1
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": code,
                        "error": "æ•°æ®åº“æ›´æ–°å¤±è´¥",
                        "context": "update_stock_basic_info"
                    })

            except Exception as e:
                batch_stats["error_count"] += 1
                # ğŸ”¥ å®‰å…¨è·å– codeï¼ˆå¤„ç† Pydantic æ¨¡å‹å’Œå­—å…¸ï¼‰
                try:
                    if hasattr(stock_info, 'code'):
                        code = stock_info.code
                    elif hasattr(stock_info, 'model_dump'):
                        code = stock_info.model_dump().get("code", "unknown")
                    elif hasattr(stock_info, 'dict'):
                        code = stock_info.dict().get("code", "unknown")
                    else:
                        code = stock_info.get("code", "unknown")
                except:
                    code = "unknown"

                batch_stats["errors"].append({
                    "code": code,
                    "error": str(e),
                    "context": "_process_basic_info_batch"
                })
        
        return batch_stats
    
    # ==================== å®æ—¶è¡Œæƒ…åŒæ­¥ ====================
    
    async def sync_realtime_quotes(self, symbols: List[str] = None, force: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥å®æ—¶è¡Œæƒ…æ•°æ®

        ç­–ç•¥ï¼š
        - å¦‚æœæŒ‡å®šäº†å°‘é‡è‚¡ç¥¨ï¼ˆâ‰¤10åªï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° AKShare æ¥å£ï¼ˆé¿å…æµªè´¹ Tushare rt_k é…é¢ï¼‰
        - å¦‚æœæŒ‡å®šäº†å¤§é‡è‚¡ç¥¨æˆ–å…¨å¸‚åœºï¼Œä½¿ç”¨ Tushare æ‰¹é‡æ¥å£ä¸€æ¬¡æ€§è·å–

        Args:
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™åŒæ­¥æ‰€æœ‰è‚¡ç¥¨ï¼›å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨åˆ—è¡¨ï¼Œåˆ™åªä¿å­˜è¿™äº›è‚¡ç¥¨çš„æ•°æ®
            force: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œï¼ˆè·³è¿‡äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼‰ï¼Œé»˜è®¤ False

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "errors": [],
            "stopped_by_rate_limit": False,
            "skipped_non_trading_time": False,
            "switched_to_akshare": False  # æ˜¯å¦åˆ‡æ¢åˆ° AKShare
        }

        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´ï¼ˆæ‰‹åŠ¨åŒæ­¥æ—¶å¯ä»¥è·³è¿‡æ£€æŸ¥ï¼‰
            if not force and not self._is_trading_time():
                logger.info("â¸ï¸ å½“å‰ä¸åœ¨äº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡å®æ—¶è¡Œæƒ…åŒæ­¥ï¼ˆä½¿ç”¨ force=True å¯å¼ºåˆ¶æ‰§è¡Œï¼‰")
                stats["skipped_non_trading_time"] = True
                return stats

            # ğŸ”¥ ç­–ç•¥é€‰æ‹©ï¼šå°‘é‡è‚¡ç¥¨åˆ‡æ¢åˆ° AKShareï¼Œå¤§é‡è‚¡ç¥¨æˆ–å…¨å¸‚åœºç”¨ Tushare æ‰¹é‡æ¥å£
            USE_AKSHARE_THRESHOLD = 10  # å°‘äºç­‰äº10åªè‚¡ç¥¨æ—¶åˆ‡æ¢åˆ° AKShare

            if symbols and len(symbols) <= USE_AKSHARE_THRESHOLD:
                # ğŸ”¥ è‡ªåŠ¨åˆ‡æ¢åˆ° AKShareï¼ˆé¿å…æµªè´¹ Tushare rt_k é…é¢ï¼Œæ¯å°æ—¶åªèƒ½è°ƒç”¨2æ¬¡ï¼‰
                logger.info(
                    f"ğŸ’¡ è‚¡ç¥¨æ•°é‡ â‰¤{USE_AKSHARE_THRESHOLD} åªï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° AKShare æ¥å£"
                    f"ï¼ˆé¿å…æµªè´¹ Tushare rt_k é…é¢ï¼Œæ¯å°æ—¶åªèƒ½è°ƒç”¨2æ¬¡ï¼‰"
                )
                logger.info(f"ğŸ¯ ä½¿ç”¨ AKShare åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…: {symbols}")

                # è°ƒç”¨ AKShare æœåŠ¡
                from app.worker.akshare_sync_service import get_akshare_sync_service
                akshare_service = await get_akshare_sync_service()

                if not akshare_service:
                    logger.error("âŒ AKShare æœåŠ¡ä¸å¯ç”¨ï¼Œå›é€€åˆ° Tushare æ‰¹é‡æ¥å£")
                    # å›é€€åˆ° Tushare æ‰¹é‡æ¥å£
                    quotes_map = await self.provider.get_realtime_quotes_batch()
                    if quotes_map and symbols:
                        quotes_map = {symbol: quotes_map[symbol] for symbol in symbols if symbol in quotes_map}
                else:
                    # ä½¿ç”¨ AKShare åŒæ­¥
                    akshare_result = await akshare_service.sync_realtime_quotes(
                        symbols=symbols,
                        force=force
                    )
                    stats["switched_to_akshare"] = True
                    stats["success_count"] = akshare_result.get("success_count", 0)
                    stats["error_count"] = akshare_result.get("error_count", 0)
                    stats["total_processed"] = akshare_result.get("total_processed", 0)
                    stats["errors"] = akshare_result.get("errors", [])
                    stats["end_time"] = datetime.utcnow()
                    stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

                    logger.info(
                        f"âœ… AKShare å®æ—¶è¡Œæƒ…åŒæ­¥å®Œæˆ: "
                        f"æ€»è®¡ {stats['total_processed']} åª, "
                        f"æˆåŠŸ {stats['success_count']} åª, "
                        f"é”™è¯¯ {stats['error_count']} åª, "
                        f"è€—æ—¶ {stats['duration']:.2f} ç§’"
                    )
                    return stats
            else:
                # ä½¿ç”¨ Tushare æ‰¹é‡æ¥å£ä¸€æ¬¡æ€§è·å–å…¨å¸‚åœºè¡Œæƒ…
                if symbols:
                    logger.info(f"ğŸ“Š ä½¿ç”¨ Tushare æ‰¹é‡æ¥å£åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…ï¼ˆä»å…¨å¸‚åœºæ•°æ®ä¸­ç­›é€‰ï¼‰")
                else:
                    logger.info("ğŸ“Š ä½¿ç”¨ Tushare æ‰¹é‡æ¥å£åŒæ­¥å…¨å¸‚åœºå®æ—¶è¡Œæƒ…...")

                logger.info("ğŸ“¡ è°ƒç”¨ rt_k æ‰¹é‡æ¥å£è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…...")
                quotes_map = await self.provider.get_realtime_quotes_batch()

                if not quotes_map:
                    logger.warning("âš ï¸ æœªè·å–åˆ°å®æ—¶è¡Œæƒ…æ•°æ®")
                    return stats

                logger.info(f"âœ… è·å–åˆ° {len(quotes_map)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…")

                # ğŸ”¥ å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨åˆ—è¡¨ï¼Œåªå¤„ç†è¿™äº›è‚¡ç¥¨
                if symbols:
                    # è¿‡æ»¤å‡ºæŒ‡å®šçš„è‚¡ç¥¨
                    filtered_quotes_map = {symbol: quotes_map[symbol] for symbol in symbols if symbol in quotes_map}

                    # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨æœªæ‰¾åˆ°
                    missing_symbols = [s for s in symbols if s not in quotes_map]
                    if missing_symbols:
                        logger.warning(f"âš ï¸ ä»¥ä¸‹è‚¡ç¥¨æœªåœ¨å®æ—¶è¡Œæƒ…ä¸­æ‰¾åˆ°: {missing_symbols}")

                    quotes_map = filtered_quotes_map
                    logger.info(f"ğŸ” è¿‡æ»¤åä¿ç•™ {len(quotes_map)} åªæŒ‡å®šè‚¡ç¥¨çš„è¡Œæƒ…")

            if not quotes_map:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å®æ—¶è¡Œæƒ…æ•°æ®")
                return stats

            stats["total_processed"] = len(quotes_map)

            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            success_count = 0
            error_count = 0

            for symbol, quote_data in quotes_map.items():
                try:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    result = await self.stock_service.update_market_quotes(symbol, quote_data)
                    if result:
                        success_count += 1
                    else:
                        error_count += 1
                        stats["errors"].append({
                            "code": symbol,
                            "error": "æ›´æ–°æ•°æ®åº“å¤±è´¥",
                            "context": "sync_realtime_quotes"
                        })
                except Exception as e:
                    error_count += 1
                    stats["errors"].append({
                        "code": symbol,
                        "error": str(e),
                        "context": "sync_realtime_quotes"
                    })

            stats["success_count"] = success_count
            stats["error_count"] = error_count

            # å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"âœ… å®æ—¶è¡Œæƒ…åŒæ­¥å®Œæˆ: "
                      f"æ€»è®¡ {stats['total_processed']} åª, "
                      f"æˆåŠŸ {stats['success_count']} åª, "
                      f"é”™è¯¯ {stats['error_count']} åª, "
                      f"è€—æ—¶ {stats['duration']:.2f} ç§’")

            return stats

        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦ä¸ºé™æµé”™è¯¯
            error_msg = str(e)
            if self._is_rate_limit_error(error_msg):
                stats["stopped_by_rate_limit"] = True
                logger.error(f"âŒ å®æ—¶è¡Œæƒ…åŒæ­¥å¤±è´¥ï¼ˆAPIé™æµï¼‰: {e}")
            else:
                logger.error(f"âŒ å®æ—¶è¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")

            stats["errors"].append({"error": str(e), "context": "sync_realtime_quotes"})
            return stats

    # ğŸ”¥ å·²åºŸå¼ƒï¼šä¸å†ä½¿ç”¨ Tushare å•åªæ¥å£ï¼ˆrt_k æ¯å°æ—¶åªèƒ½è°ƒç”¨2æ¬¡ï¼Œå¤ªå®è´µï¼‰
    # å°‘é‡è‚¡ç¥¨ï¼ˆâ‰¤10åªï¼‰è‡ªåŠ¨åˆ‡æ¢åˆ° AKShare æ¥å£
    # async def _get_quotes_individually(self, symbols: List[str]) -> Dict[str, Dict[str, Any]]:
    #     """
    #     ä½¿ç”¨å•åªæ¥å£é€ä¸ªè·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…ï¼ˆå·²åºŸå¼ƒï¼‰
    #
    #     Args:
    #         symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
    #
    #     Returns:
    #         Dict[symbol, quote_data]
    #     """
    #     quotes_map = {}
    #
    #     for symbol in symbols:
    #         try:
    #             quote_data = await self.provider.get_stock_quotes(symbol)
    #             if quote_data:
    #                 quotes_map[symbol] = quote_data
    #                 logger.info(f"âœ… è·å– {symbol} å®æ—¶è¡Œæƒ…æˆåŠŸ")
    #             else:
    #                 logger.warning(f"âš ï¸ æœªè·å–åˆ° {symbol} çš„å®æ—¶è¡Œæƒ…")
    #         except Exception as e:
    #             logger.error(f"âŒ è·å– {symbol} å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
    #             continue
    #
    #     logger.info(f"âœ… å•åªæ¥å£è·å–å®Œæˆï¼ŒæˆåŠŸ {len(quotes_map)}/{len(symbols)} åª")
    #     return quotes_map

    async def _process_quotes_batch(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è¡Œæƒ…æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": [],
            "rate_limit_hit": False
        }

        # å¹¶å‘è·å–è¡Œæƒ…æ•°æ®
        tasks = []
        for symbol in batch:
            task = self._get_and_save_quotes(symbol)
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_msg = str(result)
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": batch[i],
                    "error": error_msg,
                    "context": "_process_quotes_batch"
                })

                # æ£€æµ‹ API é™æµé”™è¯¯
                if self._is_rate_limit_error(error_msg):
                    batch_stats["rate_limit_hit"] = True
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ° API é™æµé”™è¯¯: {error_msg}")

            elif result:
                batch_stats["success_count"] += 1
            else:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": batch[i],
                    "error": "è·å–è¡Œæƒ…æ•°æ®å¤±è´¥",
                    "context": "_process_quotes_batch"
                })

        return batch_stats

    def _is_rate_limit_error(self, error_msg: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º API é™æµé”™è¯¯"""
        rate_limit_keywords = [
            "æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®",
            "æ¯åˆ†é’Ÿæœ€å¤š",
            "rate limit",
            "too many requests",
            "è®¿é—®é¢‘ç‡",
            "è¯·æ±‚è¿‡äºé¢‘ç¹"
        ]
        error_msg_lower = error_msg.lower()
        return any(keyword in error_msg_lower for keyword in rate_limit_keywords)

    def _is_trading_time(self) -> bool:
        """
        åˆ¤æ–­å½“å‰æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´
        Aè‚¡äº¤æ˜“æ—¶é—´ï¼š
        - å‘¨ä¸€åˆ°å‘¨äº”ï¼ˆæ’é™¤èŠ‚å‡æ—¥ï¼‰
        - ä¸Šåˆï¼š9:30-11:30
        - ä¸‹åˆï¼š13:00-15:00

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¸æ£€æŸ¥èŠ‚å‡æ—¥ï¼Œä»…æ£€æŸ¥æ—¶é—´æ®µ
        """
        from datetime import datetime
        import pytz

        # ä½¿ç”¨ä¸Šæµ·æ—¶åŒº
        tz = pytz.timezone('Asia/Shanghai')
        now = datetime.now(tz)

        # æ£€æŸ¥æ˜¯å¦æ˜¯å‘¨æœ«
        if now.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
            return False

        # æ£€æŸ¥æ—¶é—´æ®µ
        current_time = now.time()

        # ä¸Šåˆäº¤æ˜“æ—¶é—´ï¼š9:30-11:30
        morning_start = datetime.strptime("09:30", "%H:%M").time()
        morning_end = datetime.strptime("11:30", "%H:%M").time()

        # ä¸‹åˆäº¤æ˜“æ—¶é—´ï¼š13:00-15:00
        afternoon_start = datetime.strptime("13:00", "%H:%M").time()
        afternoon_end = datetime.strptime("15:00", "%H:%M").time()

        # åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´æ®µå†…
        is_morning = morning_start <= current_time <= morning_end
        is_afternoon = afternoon_start <= current_time <= afternoon_end

        return is_morning or is_afternoon

    async def _get_and_save_quotes(self, symbol: str) -> bool:
        """è·å–å¹¶ä¿å­˜å•ä¸ªè‚¡ç¥¨è¡Œæƒ…"""
        try:
            quotes = await self.provider.get_stock_quotes(symbol)
            if quotes:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼‰
                if hasattr(quotes, 'model_dump'):
                    quotes_data = quotes.model_dump()
                elif hasattr(quotes, 'dict'):
                    quotes_data = quotes.dict()
                else:
                    quotes_data = quotes

                return await self.stock_service.update_market_quotes(symbol, quotes_data)
            return False
        except Exception as e:
            error_msg = str(e)
            # æ£€æµ‹é™æµé”™è¯¯ï¼Œç›´æ¥æŠ›å‡ºè®©ä¸Šå±‚å¤„ç†
            if self._is_rate_limit_error(error_msg):
                logger.error(f"âŒ è·å– {symbol} è¡Œæƒ…å¤±è´¥ï¼ˆé™æµï¼‰: {e}")
                raise  # æŠ›å‡ºé™æµé”™è¯¯
            logger.error(f"âŒ è·å– {symbol} è¡Œæƒ…å¤±è´¥: {e}")
            return False

    # ==================== å†å²æ•°æ®åŒæ­¥ ====================

    async def sync_historical_data(
        self,
        symbols: List[str] = None,
        start_date: str = None,
        end_date: str = None,
        incremental: bool = True,
        all_history: bool = False,
        period: str = "daily",
        job_id: str = None
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å†å²æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            incremental: æ˜¯å¦å¢é‡åŒæ­¥
            all_history: æ˜¯å¦åŒæ­¥æ‰€æœ‰å†å²æ•°æ®
            period: æ•°æ®å‘¨æœŸ (daily/weekly/monthly)
            job_id: ä»»åŠ¡IDï¼ˆç”¨äºè¿›åº¦è·Ÿè¸ªï¼‰

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        period_name = {"daily": "æ—¥çº¿", "weekly": "å‘¨çº¿", "monthly": "æœˆçº¿"}.get(period, period)
        logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥{period_name}å†å²æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "total_records": 0,
            "start_time": datetime.utcnow(),
            "errors": []
        }

        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰
            if symbols is None:
                # æŸ¥è¯¢æ‰€æœ‰Aè‚¡è‚¡ç¥¨ï¼ˆå…¼å®¹ä¸åŒçš„æ•°æ®ç»“æ„ï¼‰ï¼Œæ’é™¤é€€å¸‚è‚¡ç¥¨
                # ä¼˜å…ˆä½¿ç”¨ market_info.marketï¼Œé™çº§åˆ° category å­—æ®µ
                cursor = self.db.stock_basic_info.find(
                    {
                        "$and": [
                            {
                                "$or": [
                                    {"market_info.market": "CN"},  # æ–°æ•°æ®ç»“æ„
                                    {"category": "stock_cn"},      # æ—§æ•°æ®ç»“æ„
                                    {"market": {"$in": ["ä¸»æ¿", "åˆ›ä¸šæ¿", "ç§‘åˆ›æ¿", "åŒ—äº¤æ‰€"]}}  # æŒ‰å¸‚åœºç±»å‹
                                ]
                            },
                            # æ’é™¤é€€å¸‚è‚¡ç¥¨
                            {
                                "$or": [
                                    {"status": {"$ne": "D"}},  # status ä¸æ˜¯ Dï¼ˆé€€å¸‚ï¼‰
                                    {"status": {"$exists": False}}  # æˆ–è€… status å­—æ®µä¸å­˜åœ¨
                                ]
                            }
                        ]
                    },
                    {"code": 1}
                )
                symbols = [doc["code"] async for doc in cursor]
                logger.info(f"ğŸ“‹ ä» stock_basic_info è·å–åˆ° {len(symbols)} åªè‚¡ç¥¨ï¼ˆå·²æ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰")

            stats["total_processed"] = len(symbols)

            # 2. ç¡®å®šå…¨å±€ç»“æŸæ—¥æœŸ
            if not end_date:
                end_date = datetime.now().strftime('%Y-%m-%d')

            # 3. ç¡®å®šå…¨å±€èµ·å§‹æ—¥æœŸï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
            global_start_date = start_date
            if not global_start_date:
                if all_history:
                    global_start_date = "1990-01-01"
                elif incremental:
                    global_start_date = "å„è‚¡ç¥¨æœ€åæ—¥æœŸ"
                else:
                    global_start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

            logger.info(f"ğŸ“Š å†å²æ•°æ®åŒæ­¥: ç»“æŸæ—¥æœŸ={end_date}, è‚¡ç¥¨æ•°é‡={len(symbols)}, æ¨¡å¼={'å¢é‡' if incremental else 'å…¨é‡'}")

            # 4. æ‰¹é‡å¤„ç†
            for i, symbol in enumerate(symbols):
                # è®°å½•å•ä¸ªè‚¡ç¥¨å¼€å§‹æ—¶é—´
                stock_start_time = datetime.now()

                try:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                    if job_id and await self._should_stop(job_id):
                        logger.warning(f"âš ï¸ ä»»åŠ¡ {job_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
                        stats["stopped"] = True
                        break

                    # é€Ÿç‡é™åˆ¶
                    await self.rate_limiter.acquire()

                    # ç¡®å®šè¯¥è‚¡ç¥¨çš„èµ·å§‹æ—¥æœŸ
                    symbol_start_date = start_date
                    if not symbol_start_date:
                        if all_history:
                            symbol_start_date = "1990-01-01"
                        elif incremental:
                            # å¢é‡åŒæ­¥ï¼šè·å–è¯¥è‚¡ç¥¨çš„æœ€åæ—¥æœŸ
                            symbol_start_date = await self._get_last_sync_date(symbol)
                            logger.debug(f"ğŸ“… {symbol}: ä» {symbol_start_date} å¼€å§‹åŒæ­¥")
                        else:
                            symbol_start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

                    # è®°å½•è¯·æ±‚å‚æ•°
                    logger.debug(
                        f"ğŸ” {symbol}: è¯·æ±‚{period_name}æ•°æ® "
                        f"start={symbol_start_date}, end={end_date}, period={period}"
                    )

                    # â±ï¸ æ€§èƒ½ç›‘æ§ï¼šAPI è°ƒç”¨
                    api_start = datetime.now()
                    df = await self.provider.get_historical_data(symbol, symbol_start_date, end_date, period=period)
                    api_duration = (datetime.now() - api_start).total_seconds()

                    if df is not None and not df.empty:
                        # â±ï¸ æ€§èƒ½ç›‘æ§ï¼šæ•°æ®ä¿å­˜
                        save_start = datetime.now()
                        records_saved = await self._save_historical_data(symbol, df, period=period)
                        save_duration = (datetime.now() - save_start).total_seconds()

                        stats["success_count"] += 1
                        stats["total_records"] += records_saved

                        # è®¡ç®—å•ä¸ªè‚¡ç¥¨è€—æ—¶
                        stock_duration = (datetime.now() - stock_start_time).total_seconds()
                        logger.info(
                            f"âœ… {symbol}: ä¿å­˜ {records_saved} æ¡{period_name}è®°å½•ï¼Œ"
                            f"æ€»è€—æ—¶ {stock_duration:.2f}ç§’ "
                            f"(API: {api_duration:.2f}ç§’, ä¿å­˜: {save_duration:.2f}ç§’)"
                        )
                    else:
                        stock_duration = (datetime.now() - stock_start_time).total_seconds()
                        logger.warning(
                            f"âš ï¸ {symbol}: æ— {period_name}æ•°æ® "
                            f"(start={symbol_start_date}, end={end_date})ï¼Œè€—æ—¶ {stock_duration:.2f}ç§’"
                        )

                    # æ¯ä¸ªè‚¡ç¥¨éƒ½æ›´æ–°è¿›åº¦
                    progress_percent = int(((i + 1) / len(symbols)) * 100)

                    # æ›´æ–°ä»»åŠ¡è¿›åº¦
                    if job_id:
                        await self._update_progress(
                            job_id,
                            progress_percent,
                            f"æ­£åœ¨åŒæ­¥ {symbol} ({i + 1}/{len(symbols)})"
                        )

                    # æ¯50ä¸ªè‚¡ç¥¨è¾“å‡ºä¸€æ¬¡è¯¦ç»†æ—¥å¿—
                    if (i + 1) % 50 == 0 or (i + 1) == len(symbols):
                        logger.info(f"ğŸ“ˆ {period_name}æ•°æ®åŒæ­¥è¿›åº¦: {i + 1}/{len(symbols)} ({progress_percent}%) "
                                   f"(æˆåŠŸ: {stats['success_count']}, è®°å½•: {stats['total_records']})")

                        # è¾“å‡ºé€Ÿç‡é™åˆ¶å™¨ç»Ÿè®¡
                        limiter_stats = self.rate_limiter.get_stats()
                        logger.info(f"   é€Ÿç‡é™åˆ¶: {limiter_stats['current_calls']}/{limiter_stats['max_calls']}æ¬¡, "
                                   f"ç­‰å¾…æ¬¡æ•°: {limiter_stats['total_waits']}, "
                                   f"æ€»ç­‰å¾…æ—¶é—´: {limiter_stats['total_wait_time']:.1f}ç§’")

                except Exception as e:
                    import traceback
                    error_details = traceback.format_exc()
                    stats["error_count"] += 1
                    stats["errors"].append({
                        "code": symbol,
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "context": f"sync_historical_data_{period}",
                        "traceback": error_details
                    })
                    logger.error(
                        f"âŒ {symbol} {period_name}æ•°æ®åŒæ­¥å¤±è´¥\n"
                        f"   å‚æ•°: start={symbol_start_date if 'symbol_start_date' in locals() else 'N/A'}, "
                        f"end={end_date}, period={period}\n"
                        f"   é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                        f"   é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                        f"   å †æ ˆè·Ÿè¸ª:\n{error_details}"
                    )

            # 4. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"âœ… {period_name}æ•°æ®åŒæ­¥å®Œæˆ: "
                       f"è‚¡ç¥¨ {stats['success_count']}/{stats['total_processed']}, "
                       f"è®°å½• {stats['total_records']} æ¡, "
                       f"é”™è¯¯ {stats['error_count']} ä¸ª, "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")

            return stats

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(
                f"âŒ å†å²æ•°æ®åŒæ­¥å¤±è´¥ï¼ˆå¤–å±‚å¼‚å¸¸ï¼‰\n"
                f"   é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"   é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"   å †æ ˆè·Ÿè¸ª:\n{error_details}"
            )
            stats["errors"].append({
                "error": str(e),
                "error_type": type(e).__name__,
                "context": "sync_historical_data",
                "traceback": error_details
            })
            return stats

    async def _save_historical_data(self, symbol: str, df, period: str = "daily") -> int:
        """ä¿å­˜å†å²æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            if self.historical_service is None:
                self.historical_service = await get_historical_data_service()

            # ä½¿ç”¨ç»Ÿä¸€å†å²æ•°æ®æœåŠ¡ä¿å­˜ï¼ˆæŒ‡å®šå‘¨æœŸï¼‰
            saved_count = await self.historical_service.save_historical_data(
                symbol=symbol,
                data=df,
                data_source="tushare",
                market="CN",
                period=period
            )

            return saved_count

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜{period}æ•°æ®å¤±è´¥ {symbol}: {e}")
            return 0

    async def _get_last_sync_date(self, symbol: str = None) -> str:
        """
        è·å–æœ€ååŒæ­¥æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœæä¾›åˆ™è¿”å›è¯¥è‚¡ç¥¨çš„æœ€åæ—¥æœŸ+1å¤©

        Returns:
            æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        """
        try:
            if self.historical_service is None:
                self.historical_service = await get_historical_data_service()

            if symbol:
                # è·å–ç‰¹å®šè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ
                latest_date = await self.historical_service.get_latest_date(symbol, "tushare")
                if latest_date:
                    # è¿”å›æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©ï¼ˆé¿å…é‡å¤åŒæ­¥ï¼‰
                    try:
                        last_date_obj = datetime.strptime(latest_date, '%Y-%m-%d')
                        next_date = last_date_obj + timedelta(days=1)
                        return next_date.strftime('%Y-%m-%d')
                    except:
                        # å¦‚æœæ—¥æœŸæ ¼å¼ä¸å¯¹ï¼Œç›´æ¥è¿”å›
                        return latest_date
                else:
                    # ğŸ”¥ æ²¡æœ‰å†å²æ•°æ®æ—¶ï¼Œä»ä¸Šå¸‚æ—¥æœŸå¼€å§‹å…¨é‡åŒæ­¥
                    stock_info = await self.db.stock_basic_info.find_one(
                        {"code": symbol},
                        {"list_date": 1}
                    )
                    if stock_info and stock_info.get("list_date"):
                        list_date = stock_info["list_date"]
                        # å¤„ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                        if isinstance(list_date, str):
                            # æ ¼å¼å¯èƒ½æ˜¯ "20100101" æˆ– "2010-01-01"
                            if len(list_date) == 8 and list_date.isdigit():
                                return f"{list_date[:4]}-{list_date[4:6]}-{list_date[6:]}"
                            else:
                                return list_date
                        else:
                            return list_date.strftime('%Y-%m-%d')

                    # å¦‚æœæ²¡æœ‰ä¸Šå¸‚æ—¥æœŸï¼Œä»1990å¹´å¼€å§‹
                    logger.warning(f"âš ï¸ {symbol}: æœªæ‰¾åˆ°ä¸Šå¸‚æ—¥æœŸï¼Œä»1990-01-01å¼€å§‹åŒæ­¥")
                    return "1990-01-01"

            # é»˜è®¤è¿”å›30å¤©å‰ï¼ˆç¡®ä¿ä¸æ¼æ•°æ®ï¼‰
            return (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€ååŒæ­¥æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            # å‡ºé”™æ—¶è¿”å›30å¤©å‰ï¼Œç¡®ä¿ä¸æ¼æ•°æ®
            return (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

    # ==================== è´¢åŠ¡æ•°æ®åŒæ­¥ ====================

    async def sync_financial_data(self, symbols: List[str] = None, limit: int = 20, job_id: str = None) -> Dict[str, Any]:
        """
        åŒæ­¥è´¢åŠ¡æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŒæ­¥æ‰€æœ‰è‚¡ç¥¨
            limit: è·å–è´¢æŠ¥æœŸæ•°ï¼Œé»˜è®¤20æœŸï¼ˆçº¦5å¹´æ•°æ®ï¼‰
            job_id: ä»»åŠ¡IDï¼ˆç”¨äºè¿›åº¦è·Ÿè¸ªï¼‰
        """
        logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥è´¢åŠ¡æ•°æ® (è·å–æœ€è¿‘ {limit} æœŸ)...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "errors": []
        }

        try:
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                cursor = self.db.stock_basic_info.find(
                    {
                        "$or": [
                            {"market_info.market": "CN"},  # æ–°æ•°æ®ç»“æ„
                            {"category": "stock_cn"},      # æ—§æ•°æ®ç»“æ„
                            {"market": {"$in": ["ä¸»æ¿", "åˆ›ä¸šæ¿", "ç§‘åˆ›æ¿", "åŒ—äº¤æ‰€"]}}  # æŒ‰å¸‚åœºç±»å‹
                        ]
                    },
                    {"code": 1}
                )
                symbols = [doc["code"] async for doc in cursor]
                logger.info(f"ğŸ“‹ ä» stock_basic_info è·å–åˆ° {len(symbols)} åªè‚¡ç¥¨")

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š éœ€è¦åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨è´¢åŠ¡æ•°æ®")

            # æ‰¹é‡å¤„ç†
            for i, symbol in enumerate(symbols):
                try:
                    # é€Ÿç‡é™åˆ¶
                    await self.rate_limiter.acquire()

                    # è·å–è´¢åŠ¡æ•°æ®ï¼ˆæŒ‡å®šè·å–æœŸæ•°ï¼‰
                    financial_data = await self.provider.get_financial_data(symbol, limit=limit)

                    if financial_data:
                        # ä¿å­˜è´¢åŠ¡æ•°æ®
                        success = await self._save_financial_data(symbol, financial_data)
                        if success:
                            stats["success_count"] += 1
                        else:
                            stats["error_count"] += 1
                    else:
                        logger.warning(f"âš ï¸ {symbol}: æ— è´¢åŠ¡æ•°æ®")

                    # è¿›åº¦æ—¥å¿—å’Œè¿›åº¦è·Ÿè¸ª
                    if (i + 1) % 20 == 0:
                        progress = int((i + 1) / len(symbols) * 100)
                        logger.info(f"ğŸ“ˆ è´¢åŠ¡æ•°æ®åŒæ­¥è¿›åº¦: {i + 1}/{len(symbols)} ({progress}%) "
                                   f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")
                        # è¾“å‡ºé€Ÿç‡é™åˆ¶å™¨ç»Ÿè®¡
                        limiter_stats = self.rate_limiter.get_stats()
                        logger.info(f"   é€Ÿç‡é™åˆ¶: {limiter_stats['current_calls']}/{limiter_stats['max_calls']}æ¬¡")

                        # æ›´æ–°ä»»åŠ¡è¿›åº¦
                        if job_id:
                            from app.services.scheduler_service import update_job_progress, TaskCancelledException
                            try:
                                await update_job_progress(
                                    job_id=job_id,
                                    progress=progress,
                                    message=f"æ­£åœ¨åŒæ­¥ {symbol} è´¢åŠ¡æ•°æ®",
                                    current_item=symbol,
                                    total_items=len(symbols),
                                    processed_items=i + 1
                                )
                            except TaskCancelledException:
                                # ä»»åŠ¡è¢«å–æ¶ˆï¼Œè®°å½•å¹¶é€€å‡º
                                logger.warning(f"âš ï¸ è´¢åŠ¡æ•°æ®åŒæ­¥ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ (å·²å¤„ç† {i + 1}/{len(symbols)})")
                                stats["end_time"] = datetime.utcnow()
                                stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
                                stats["cancelled"] = True
                                raise

                except Exception as e:
                    stats["error_count"] += 1
                    stats["errors"].append({
                        "code": symbol,
                        "error": str(e),
                        "context": "sync_financial_data"
                    })
                    logger.error(f"âŒ {symbol} è´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")

            # å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"âœ… è´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆ: "
                       f"æˆåŠŸ {stats['success_count']}/{stats['total_processed']}, "
                       f"é”™è¯¯ {stats['error_count']} ä¸ª, "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ è´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_financial_data"})
            return stats

    async def _save_financial_data(self, symbol: str, financial_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è´¢åŠ¡æ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è´¢åŠ¡æ•°æ®æœåŠ¡
            from app.services.financial_data_service import get_financial_data_service

            financial_service = await get_financial_data_service()

            # ä¿å­˜è´¢åŠ¡æ•°æ®
            saved_count = await financial_service.save_financial_data(
                symbol=symbol,
                financial_data=financial_data,
                data_source="tushare",
                market="CN",
                report_period=financial_data.get("report_period"),
                report_type=financial_data.get("report_type", "quarterly")
            )

            return saved_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {symbol} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            return False

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _is_data_fresh(self, updated_at: datetime, hours: int = 24) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        if not updated_at:
            return False

        threshold = datetime.utcnow() - timedelta(hours=hours)
        return updated_at > threshold

    async def get_sync_status(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥çŠ¶æ€"""
        try:
            # ç»Ÿè®¡å„é›†åˆçš„æ•°æ®é‡
            basic_info_count = await self.db.stock_basic_info.count_documents({})
            quotes_count = await self.db.market_quotes.count_documents({})

            # è·å–æœ€æ–°æ›´æ–°æ—¶é—´
            latest_basic = await self.db.stock_basic_info.find_one(
                {},
                sort=[("updated_at", -1)]
            )
            latest_quotes = await self.db.market_quotes.find_one(
                {},
                sort=[("updated_at", -1)]
            )

            return {
                "provider_connected": self.provider.is_available(),
                "collections": {
                    "stock_basic_info": {
                        "count": basic_info_count,
                        "latest_update": latest_basic.get("updated_at") if (latest_basic and isinstance(latest_basic, dict)) else None
                    },
                    "market_quotes": {
                        "count": quotes_count,
                        "latest_update": latest_quotes.get("updated_at") if (latest_quotes and isinstance(latest_quotes, dict)) else None
                    }
                },
                "status_time": datetime.utcnow()
            }

        except Exception as e:
            logger.error(f"âŒ è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
            return {"error": str(e)}

    # ==================== æ–°é—»æ•°æ®åŒæ­¥ ====================

    async def sync_news_data(
        self,
        symbols: List[str] = None,
        hours_back: int = 24,
        max_news_per_stock: int = 20,
        force_update: bool = False,
        job_id: str = None
    ) -> Dict[str, Any]:
        """
        åŒæ­¥æ–°é—»æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸ºNoneæ—¶è·å–æ‰€æœ‰è‚¡ç¥¨
            hours_back: å›æº¯å°æ—¶æ•°ï¼Œé»˜è®¤24å°æ—¶
            max_news_per_stock: æ¯åªè‚¡ç¥¨æœ€å¤§æ–°é—»æ•°é‡
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            job_id: ä»»åŠ¡IDï¼ˆç”¨äºè¿›åº¦è·Ÿè¸ªï¼‰

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥æ–°é—»æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "start_time": datetime.utcnow(),
            "errors": []
        }

        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                stock_list = await self.stock_service.get_all_stocks()
                symbols = [stock["code"] for stock in stock_list]

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦åŒæ­¥æ–°é—»çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š éœ€è¦åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„æ–°é—»")

            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                if job_id and await self._should_stop(job_id):
                    logger.warning(f"âš ï¸ ä»»åŠ¡ {job_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
                    stats["stopped"] = True
                    break

                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_news_batch(
                    batch, hours_back, max_news_per_stock
                )

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["news_count"] += batch_stats["news_count"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—å’Œè¿›åº¦æ›´æ–°
                progress = min(i + self.batch_size, len(symbols))
                progress_percent = int((progress / len(symbols)) * 100)
                logger.info(f"ğŸ“ˆ æ–°é—»åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} ({progress_percent}%) "
                           f"(æˆåŠŸ: {stats['success_count']}, æ–°é—»: {stats['news_count']})")

                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                if job_id:
                    await self._update_progress(
                        job_id,
                        progress_percent,
                        f"å·²å¤„ç† {progress}/{len(symbols)} åªè‚¡ç¥¨ï¼Œè·å– {stats['news_count']} æ¡æ–°é—»"
                    )

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"âœ… æ–°é—»æ•°æ®åŒæ­¥å®Œæˆ: "
                       f"æ€»è®¡ {stats['total_processed']} åªè‚¡ç¥¨, "
                       f"æˆåŠŸ {stats['success_count']} åª, "
                       f"è·å– {stats['news_count']} æ¡æ–°é—», "
                       f"é”™è¯¯ {stats['error_count']} åª, "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ æ–°é—»æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_news_data"})
            return stats

    async def _process_news_batch(
        self,
        batch: List[str],
        hours_back: int,
        max_news_per_stock: int
    ) -> Dict[str, Any]:
        """å¤„ç†æ–°é—»æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # ä»Tushareè·å–æ–°é—»æ•°æ®
                news_data = await self.provider.get_stock_news(
                    symbol=symbol,
                    limit=max_news_per_stock,
                    hours_back=hours_back
                )

                if news_data:
                    # ä¿å­˜æ–°é—»æ•°æ®
                    saved_count = await self.news_service.save_news_data(
                        news_data=news_data,
                        data_source="tushare",
                        market="CN"
                    )

                    batch_stats["success_count"] += 1
                    batch_stats["news_count"] += saved_count

                    logger.debug(f"âœ… {symbol} æ–°é—»åŒæ­¥æˆåŠŸ: {saved_count}æ¡")
                else:
                    logger.debug(f"âš ï¸ {symbol} æœªè·å–åˆ°æ–°é—»æ•°æ®")
                    batch_stats["success_count"] += 1  # æ²¡æœ‰æ–°é—»ä¹Ÿç®—æˆåŠŸ

                # ğŸ”¥ APIé™æµï¼šæˆåŠŸåä¼‘çœ 
                await asyncio.sleep(0.2)

            except Exception as e:
                batch_stats["error_count"] += 1
                error_msg = f"{symbol}: {str(e)}"
                batch_stats["errors"].append(error_msg)
                logger.error(f"âŒ {symbol} æ–°é—»åŒæ­¥å¤±è´¥: {e}")

                # ğŸ”¥ å¤±è´¥åä¹Ÿè¦ä¼‘çœ ï¼Œé¿å…"å¤±è´¥é›ªå´©"
                # å¤±è´¥æ—¶ä¼‘çœ æ›´é•¿æ—¶é—´ï¼Œç»™APIæœåŠ¡å™¨æ¢å¤çš„æœºä¼š
                await asyncio.sleep(1.0)

        return batch_stats

    # ==================== è¿›åº¦è·Ÿè¸ªè¾…åŠ©æ–¹æ³• ====================

    async def _should_stop(self, job_id: str) -> bool:
        """
        æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥åœæ­¢

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦åº”è¯¥åœæ­¢
        """
        try:
            # æŸ¥è¯¢æ‰§è¡Œè®°å½•ï¼Œæ£€æŸ¥ cancel_requested æ ‡è®°
            execution = await self.db.scheduler_executions.find_one(
                {"job_id": job_id, "status": "running"},
                sort=[("timestamp", -1)]
            )

            if execution and execution.get("cancel_requested"):
                return True

            return False

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ä»»åŠ¡åœæ­¢æ ‡è®°å¤±è´¥: {e}")
            return False

    async def _update_progress(self, job_id: str, progress: int, message: str):
        """
        æ›´æ–°ä»»åŠ¡è¿›åº¦

        Args:
            job_id: ä»»åŠ¡ID
            progress: è¿›åº¦ç™¾åˆ†æ¯” (0-100)
            message: è¿›åº¦æ¶ˆæ¯
        """
        try:
            from app.services.scheduler_service import TaskCancelledException
            from pymongo import MongoClient
            from app.core.config import settings

            logger.info(f"ğŸ“Š [è¿›åº¦æ›´æ–°] å¼€å§‹æ›´æ–°ä»»åŠ¡ {job_id} è¿›åº¦: {progress}% - {message}")

            # ä½¿ç”¨åŒæ­¥ PyMongo å®¢æˆ·ç«¯ï¼ˆé¿å…äº‹ä»¶å¾ªç¯å†²çªï¼‰
            sync_client = MongoClient(settings.MONGO_URI)
            sync_db = sync_client[settings.MONGODB_DATABASE]

            # æŸ¥æ‰¾æœ€æ–°çš„ running è®°å½•
            execution = sync_db.scheduler_executions.find_one(
                {"job_id": job_id, "status": "running"},
                sort=[("timestamp", -1)]
            )

            if not execution:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»åŠ¡ {job_id} çš„æ‰§è¡Œè®°å½•")
                sync_client.close()
                return

            logger.info(f"ğŸ“Š [è¿›åº¦æ›´æ–°] æ‰¾åˆ°æ‰§è¡Œè®°å½•: _id={execution['_id']}, å½“å‰è¿›åº¦={execution.get('progress', 0)}%")

            # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°å–æ¶ˆè¯·æ±‚
            if execution.get("cancel_requested"):
                sync_client.close()
                raise TaskCancelledException(f"ä»»åŠ¡ {job_id} å·²è¢«ç”¨æˆ·å–æ¶ˆ")

            # æ›´æ–°è¿›åº¦ï¼ˆä½¿ç”¨ UTC+8 æ—¶é—´ï¼‰
            result = sync_db.scheduler_executions.update_one(
                {"_id": execution["_id"]},
                {
                    "$set": {
                        "progress": progress,
                        "progress_message": message,
                        "updated_at": get_utc8_now()
                    }
                }
            )

            logger.info(f"ğŸ“Š [è¿›åº¦æ›´æ–°] æ›´æ–°ç»“æœ: matched={result.matched_count}, modified={result.modified_count}")

            sync_client.close()
            logger.info(f"âœ… ä»»åŠ¡ {job_id} è¿›åº¦æ›´æ–°æˆåŠŸ: {progress}% - {message}")

        except Exception as e:
            if "TaskCancelledException" in str(type(e).__name__):
                raise
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}", exc_info=True)


# å…¨å±€åŒæ­¥æœåŠ¡å®ä¾‹
_tushare_sync_service = None

async def get_tushare_sync_service() -> TushareSyncService:
    """è·å–TushareåŒæ­¥æœåŠ¡å®ä¾‹"""
    global _tushare_sync_service
    if _tushare_sync_service is None:
        _tushare_sync_service = TushareSyncService()
        await _tushare_sync_service.initialize()
    return _tushare_sync_service


# APSchedulerå…¼å®¹çš„ä»»åŠ¡å‡½æ•°
async def run_tushare_basic_info_sync(force_update: bool = False):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
    try:
        service = await get_tushare_sync_service()
        result = await service.sync_stock_basic_info(force_update, job_id="tushare_basic_info_sync")
        logger.info(f"âœ… TushareåŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ TushareåŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_tushare_quotes_sync(force: bool = False):
    """
    APSchedulerä»»åŠ¡ï¼šåŒæ­¥å®æ—¶è¡Œæƒ…

    Args:
        force: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œï¼ˆè·³è¿‡äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼‰ï¼Œé»˜è®¤ False
    """
    try:
        service = await get_tushare_sync_service()
        result = await service.sync_realtime_quotes(force=force)
        logger.info(f"âœ… Tushareè¡Œæƒ…åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Tushareè¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_tushare_historical_sync(incremental: bool = True):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥å†å²æ•°æ®"""
    logger.info(f"ğŸš€ [APScheduler] å¼€å§‹æ‰§è¡Œ Tushare å†å²æ•°æ®åŒæ­¥ä»»åŠ¡ (incremental={incremental})")
    try:
        service = await get_tushare_sync_service()
        logger.info(f"âœ… [APScheduler] Tushare åŒæ­¥æœåŠ¡å·²åˆå§‹åŒ–")
        result = await service.sync_historical_data(incremental=incremental, job_id="tushare_historical_sync")
        logger.info(f"âœ… [APScheduler] Tushareå†å²æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ [APScheduler] Tushareå†å²æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise


async def run_tushare_financial_sync():
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è´¢åŠ¡æ•°æ®ï¼ˆè·å–æœ€è¿‘20æœŸï¼Œçº¦5å¹´ï¼‰"""
    try:
        service = await get_tushare_sync_service()
        result = await service.sync_financial_data(limit=20, job_id="tushare_financial_sync")  # è·å–æœ€è¿‘20æœŸï¼ˆçº¦5å¹´æ•°æ®ï¼‰
        logger.info(f"âœ… Tushareè´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Tushareè´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_tushare_status_check():
    """APSchedulerä»»åŠ¡ï¼šæ£€æŸ¥åŒæ­¥çŠ¶æ€"""
    try:
        service = await get_tushare_sync_service()
        result = await service.get_sync_status()
        logger.info(f"âœ… TushareçŠ¶æ€æ£€æŸ¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ TushareçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return {"error": str(e)}


async def run_tushare_news_sync(hours_back: int = 24, max_news_per_stock: int = 20):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥æ–°é—»æ•°æ®"""
    try:
        service = await get_tushare_sync_service()
        result = await service.sync_news_data(
            hours_back=hours_back,
            max_news_per_stock=max_news_per_stock,
            job_id="tushare_news_sync"
        )
        logger.info(f"âœ… Tushareæ–°é—»æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Tushareæ–°é—»æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise
