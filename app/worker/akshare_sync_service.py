"""
AKShareæ•°æ®åŒæ­¥æœåŠ¡
åŸºäºAKShareæä¾›å™¨çš„ç»Ÿä¸€æ•°æ®åŒæ­¥æ–¹æ¡ˆ
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

from app.core.database import get_mongo_db
from app.services.historical_data_service import get_historical_data_service
from app.services.news_data_service import get_news_data_service
from tradingagents.dataflows.providers.china.akshare import get_akshare_provider

logger = logging.getLogger(__name__)


class AKShareSyncService:
    """
    AKShareæ•°æ®åŒæ­¥æœåŠ¡
    
    æä¾›å®Œæ•´çš„æ•°æ®åŒæ­¥åŠŸèƒ½ï¼š
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥
    - å®æ—¶è¡Œæƒ…åŒæ­¥
    - å†å²æ•°æ®åŒæ­¥
    - è´¢åŠ¡æ•°æ®åŒæ­¥
    """
    
    def __init__(self):
        self.provider = None
        self.historical_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.news_service = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.db = None
        self.batch_size = 100
        self.rate_limit_delay = 0.2  # AKShareå»ºè®®çš„å»¶è¿Ÿ
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥æœåŠ¡"""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            self.db = get_mongo_db()

            # åˆå§‹åŒ–å†å²æ•°æ®æœåŠ¡
            self.historical_service = await get_historical_data_service()

            # åˆå§‹åŒ–æ–°é—»æ•°æ®æœåŠ¡
            self.news_service = await get_news_data_service()

            # åˆå§‹åŒ–AKShareæä¾›å™¨ï¼ˆä½¿ç”¨å…¨å±€å•ä¾‹ï¼Œç¡®ä¿monkey patchç”Ÿæ•ˆï¼‰
            self.provider = get_akshare_provider()

            # æµ‹è¯•è¿æ¥
            if not await self.provider.test_connection():
                raise RuntimeError("âŒ AKShareè¿æ¥å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨åŒæ­¥æœåŠ¡")

            logger.info("âœ… AKShareåŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ AKShareåŒæ­¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def sync_stock_basic_info(self, force_update: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        
        Args:
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
        
        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }
        
        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = await self.provider.get_stock_list()
            if not stock_list:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
                return stats
            
            stats["total_processed"] = len(stock_list)
            logger.info(f"ğŸ“Š è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨ä¿¡æ¯")
            
            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(stock_list), self.batch_size):
                batch = stock_list[i:i + self.batch_size]
                batch_stats = await self._process_basic_info_batch(batch, force_update)
                
                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["skipped_count"] += batch_stats["skipped_count"]
                stats["errors"].extend(batch_stats["errors"])
                
                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(stock_list))
                logger.info(f"ğŸ“ˆ åŸºç¡€ä¿¡æ¯åŒæ­¥è¿›åº¦: {progress}/{len(stock_list)} "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")
                
                # APIé™æµ
                if i + self.batch_size < len(stock_list):
                    await asyncio.sleep(self.rate_limit_delay)
            
            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
            logger.info(f"ğŸ‰ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åª, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è·³è¿‡: {stats['skipped_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_stock_basic_info"})
            return stats
    
    async def _process_basic_info_batch(self, batch: List[Dict[str, Any]], force_update: bool) -> Dict[str, Any]:
        """å¤„ç†åŸºç¡€ä¿¡æ¯æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "skipped_count": 0,
            "errors": []
        }
        
        for stock_info in batch:
            try:
                code = stock_info["code"]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if not force_update:
                    existing = await self.db.stock_basic_info.find_one({"code": code})
                    if existing and self._is_data_fresh(existing.get("updated_at"), hours=24):
                        batch_stats["skipped_count"] += 1
                        continue
                
                # è·å–è¯¦ç»†åŸºç¡€ä¿¡æ¯
                basic_info = await self.provider.get_stock_basic_info(code)
                
                if basic_info:
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    if hasattr(basic_info, 'model_dump'):
                        basic_data = basic_info.model_dump()
                    elif hasattr(basic_info, 'dict'):
                        basic_data = basic_info.dict()
                    else:
                        basic_data = basic_info
                    
                    # ğŸ”¥ ç¡®ä¿ source å­—æ®µå­˜åœ¨
                    if "source" not in basic_data:
                        basic_data["source"] = "akshare"

                    # ğŸ”¥ ç¡®ä¿ symbol å­—æ®µå­˜åœ¨
                    if "symbol" not in basic_data:
                        basic_data["symbol"] = code

                    # æ›´æ–°åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨ code + source è”åˆæŸ¥è¯¢ï¼‰
                    try:
                        await self.db.stock_basic_info.update_one(
                            {"code": code, "source": "akshare"},
                            {"$set": basic_data},
                            upsert=True
                        )
                        batch_stats["success_count"] += 1
                    except Exception as e:
                        batch_stats["error_count"] += 1
                        batch_stats["errors"].append({
                            "code": code,
                            "error": f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(e)}",
                            "context": "update_stock_basic_info"
                        })
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": code,
                        "error": "è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥",
                        "context": "get_stock_basic_info"
                    })
                
            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": stock_info.get("code", "unknown"),
                    "error": str(e),
                    "context": "_process_basic_info_batch"
                })
        
        return batch_stats
    
    def _is_data_fresh(self, updated_at: Any, hours: int = 24) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        if not updated_at:
            return False
        
        try:
            if isinstance(updated_at, str):
                updated_at = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
            elif isinstance(updated_at, datetime):
                pass
            else:
                return False
            
            # è½¬æ¢ä¸ºUTCæ—¶é—´è¿›è¡Œæ¯”è¾ƒ
            if updated_at.tzinfo is None:
                updated_at = updated_at.replace(tzinfo=None)
            else:
                updated_at = updated_at.replace(tzinfo=None)
            
            now = datetime.utcnow()
            time_diff = now - updated_at
            
            return time_diff.total_seconds() < (hours * 3600)
            
        except Exception as e:
            logger.debug(f"æ£€æŸ¥æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")
            return False
    
    async def sync_realtime_quotes(self, symbols: List[str] = None, force: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥å®æ—¶è¡Œæƒ…æ•°æ®

        Args:
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™åŒæ­¥æ‰€æœ‰è‚¡ç¥¨
            force: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œï¼ˆè·³è¿‡äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼‰ï¼Œé»˜è®¤ False

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        # ğŸ”¥ å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨åˆ—è¡¨ï¼Œè®°å½•æ—¥å¿—
        if symbols:
            logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥æŒ‡å®šè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…ï¼ˆå…± {len(symbols)} åªï¼‰: {symbols}")
        else:
            logger.info("ğŸ”„ å¼€å§‹åŒæ­¥å…¨å¸‚åœºå®æ—¶è¡Œæƒ…...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }

        try:
            # 1. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                # ä»æ•°æ®åº“è·å–æ‰€æœ‰ä¸Šå¸‚çŠ¶æ€çš„è‚¡ç¥¨ä»£ç ï¼ˆæ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰
                basic_info_cursor = self.db.stock_basic_info.find(
                    {"list_status": "L"},  # åªè·å–ä¸Šå¸‚çŠ¶æ€çš„è‚¡ç¥¨
                    {"code": 1}
                )
                symbols = [doc["code"] async for doc in basic_info_cursor]

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„è¡Œæƒ…")

            # ğŸ”¥ ä¼˜åŒ–ï¼šå¦‚æœåªåŒæ­¥1åªè‚¡ç¥¨ï¼Œç›´æ¥è°ƒç”¨å•ä¸ªè‚¡ç¥¨æ¥å£ï¼Œä¸èµ°æ‰¹é‡æ¥å£
            if len(symbols) == 1:
                logger.info(f"ğŸ“ˆ å•ä¸ªè‚¡ç¥¨åŒæ­¥ï¼Œç›´æ¥ä½¿ç”¨ get_stock_quotes æ¥å£")
                symbol = symbols[0]
                success = await self._get_and_save_quotes(symbol)
                if success:
                    stats["success_count"] = 1
                else:
                    stats["error_count"] = 1
                    stats["errors"].append({
                        "code": symbol,
                        "error": "è·å–è¡Œæƒ…å¤±è´¥",
                        "context": "sync_realtime_quotes_single"
                    })

                logger.info(f"ğŸ“ˆ è¡Œæƒ…åŒæ­¥è¿›åº¦: 1/1 (æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")
            else:
                # 2. æ‰¹é‡åŒæ­¥ï¼šä¸€æ¬¡æ€§è·å–å…¨å¸‚åœºå¿«ç…§ï¼ˆé¿å…å¤šæ¬¡è°ƒç”¨æ¥å£è¢«é™æµï¼‰
                logger.info("ğŸ“¡ è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…å¿«ç…§...")
                quotes_map = await self.provider.get_batch_stock_quotes(symbols)

                if not quotes_map:
                    logger.warning("âš ï¸ è·å–å…¨å¸‚åœºå¿«ç…§å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªè·å–æ¨¡å¼")
                    # å›é€€åˆ°é€ä¸ªè·å–æ¨¡å¼
                    for i in range(0, len(symbols), self.batch_size):
                        batch = symbols[i:i + self.batch_size]
                        batch_stats = await self._process_quotes_batch_fallback(batch)

                        # æ›´æ–°ç»Ÿè®¡
                        stats["success_count"] += batch_stats["success_count"]
                        stats["error_count"] += batch_stats["error_count"]
                        stats["errors"].extend(batch_stats["errors"])

                        # è¿›åº¦æ—¥å¿—
                        progress = min(i + self.batch_size, len(symbols))
                        logger.info(f"ğŸ“ˆ è¡Œæƒ…åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                                   f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")

                        # APIé™æµ
                        if i + self.batch_size < len(symbols):
                            await asyncio.sleep(self.rate_limit_delay)
                else:
                    # 3. ä½¿ç”¨è·å–åˆ°çš„å…¨å¸‚åœºæ•°æ®ï¼Œåˆ†æ‰¹ä¿å­˜åˆ°æ•°æ®åº“
                    logger.info(f"âœ… è·å–åˆ° {len(quotes_map)} åªè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®ï¼Œå¼€å§‹ä¿å­˜...")

                    for i in range(0, len(symbols), self.batch_size):
                        batch = symbols[i:i + self.batch_size]

                        # ä»å…¨å¸‚åœºæ•°æ®ä¸­æå–å½“å‰æ‰¹æ¬¡çš„æ•°æ®å¹¶ä¿å­˜
                        for symbol in batch:
                            try:
                                quotes = quotes_map.get(symbol)
                                if quotes:
                                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                                    if hasattr(quotes, 'model_dump'):
                                        quotes_data = quotes.model_dump()
                                    elif hasattr(quotes, 'dict'):
                                        quotes_data = quotes.dict()
                                    else:
                                        quotes_data = quotes

                                    # ç¡®ä¿ symbol å’Œ code å­—æ®µå­˜åœ¨
                                    if "symbol" not in quotes_data:
                                        quotes_data["symbol"] = symbol
                                    if "code" not in quotes_data:
                                        quotes_data["code"] = symbol

                                    # æ›´æ–°åˆ°æ•°æ®åº“
                                    await self.db.market_quotes.update_one(
                                        {"code": symbol},
                                        {"$set": quotes_data},
                                        upsert=True
                                    )
                                    stats["success_count"] += 1
                                else:
                                    stats["error_count"] += 1
                                    stats["errors"].append({
                                        "code": symbol,
                                        "error": "æœªæ‰¾åˆ°è¡Œæƒ…æ•°æ®",
                                        "context": "sync_realtime_quotes"
                                    })
                            except Exception as e:
                                stats["error_count"] += 1
                                stats["errors"].append({
                                    "code": symbol,
                                    "error": str(e),
                                    "context": "sync_realtime_quotes"
                                })

                        # è¿›åº¦æ—¥å¿—
                        progress = min(i + self.batch_size, len(symbols))
                        logger.info(f"ğŸ“ˆ è¡Œæƒ…ä¿å­˜è¿›åº¦: {progress}/{len(symbols)} "
                                   f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")

            # 4. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"ğŸ‰ å®æ—¶è¡Œæƒ…åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åª, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ å®æ—¶è¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_realtime_quotes"})
            return stats
    
    async def _process_quotes_batch(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è¡Œæƒ…æ‰¹æ¬¡ - ä¼˜åŒ–ç‰ˆï¼šä¸€æ¬¡è·å–å…¨å¸‚åœºå¿«ç…§"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }

        try:
            # ä¸€æ¬¡æ€§è·å–å…¨å¸‚åœºå¿«ç…§ï¼ˆé¿å…é¢‘ç¹è°ƒç”¨æ¥å£ï¼‰
            logger.debug(f"ğŸ“Š è·å–å…¨å¸‚åœºå¿«ç…§ä»¥å¤„ç† {len(batch)} åªè‚¡ç¥¨...")
            quotes_map = await self.provider.get_batch_stock_quotes(batch)

            if not quotes_map:
                logger.warning("âš ï¸ è·å–å…¨å¸‚åœºå¿«ç…§å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªè·å–")
                # å›é€€åˆ°åŸæ¥çš„é€ä¸ªè·å–æ–¹å¼
                return await self._process_quotes_batch_fallback(batch)

            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            for symbol in batch:
                try:
                    quotes = quotes_map.get(symbol)
                    if quotes:
                        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        if hasattr(quotes, 'model_dump'):
                            quotes_data = quotes.model_dump()
                        elif hasattr(quotes, 'dict'):
                            quotes_data = quotes.dict()
                        else:
                            quotes_data = quotes

                        # ç¡®ä¿ symbol å’Œ code å­—æ®µå­˜åœ¨
                        if "symbol" not in quotes_data:
                            quotes_data["symbol"] = symbol
                        if "code" not in quotes_data:
                            quotes_data["code"] = symbol

                        # æ›´æ–°åˆ°æ•°æ®åº“
                        await self.db.market_quotes.update_one(
                            {"code": symbol},
                            {"$set": quotes_data},
                            upsert=True
                        )
                        batch_stats["success_count"] += 1
                    else:
                        batch_stats["error_count"] += 1
                        batch_stats["errors"].append({
                            "code": symbol,
                            "error": "æœªæ‰¾åˆ°è¡Œæƒ…æ•°æ®",
                            "context": "_process_quotes_batch"
                        })
                except Exception as e:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": str(e),
                        "context": "_process_quotes_batch"
                    })

            return batch_stats

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†è¡Œæƒ…å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæ¥çš„é€ä¸ªè·å–æ–¹å¼
            return await self._process_quotes_batch_fallback(batch)

    async def _process_quotes_batch_fallback(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è¡Œæƒ…æ‰¹æ¬¡ - å›é€€æ–¹æ¡ˆï¼šé€ä¸ªè·å–"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }

        # é€ä¸ªè·å–è¡Œæƒ…æ•°æ®ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶ï¼‰
        for symbol in batch:
            try:
                success = await self._get_and_save_quotes(symbol)
                if success:
                    batch_stats["success_count"] += 1
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": "è·å–è¡Œæƒ…æ•°æ®å¤±è´¥",
                        "context": "_process_quotes_batch_fallback"
                    })

                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                await asyncio.sleep(0.1)

            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": symbol,
                    "error": str(e),
                    "context": "_process_quotes_batch_fallback"
                })

        return batch_stats
    
    async def _get_and_save_quotes(self, symbol: str) -> bool:
        """è·å–å¹¶ä¿å­˜å•ä¸ªè‚¡ç¥¨è¡Œæƒ…"""
        try:
            quotes = await self.provider.get_stock_quotes(symbol)
            if quotes:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if hasattr(quotes, 'model_dump'):
                    quotes_data = quotes.model_dump()
                elif hasattr(quotes, 'dict'):
                    quotes_data = quotes.dict()
                else:
                    quotes_data = quotes

                # ç¡®ä¿ symbol å­—æ®µå­˜åœ¨
                if "symbol" not in quotes_data:
                    quotes_data["symbol"] = symbol

                # ğŸ”¥ æ‰“å°å³å°†ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°æ®
                logger.info(f"ğŸ’¾ å‡†å¤‡ä¿å­˜ {symbol} è¡Œæƒ…åˆ°æ•°æ®åº“:")
                logger.info(f"   - æœ€æ–°ä»·(price): {quotes_data.get('price')}")
                logger.info(f"   - æœ€é«˜ä»·(high): {quotes_data.get('high')}")
                logger.info(f"   - æœ€ä½ä»·(low): {quotes_data.get('low')}")
                logger.info(f"   - å¼€ç›˜ä»·(open): {quotes_data.get('open')}")
                logger.info(f"   - æ˜¨æ”¶ä»·(pre_close): {quotes_data.get('pre_close')}")
                logger.info(f"   - æˆäº¤é‡(volume): {quotes_data.get('volume')}")
                logger.info(f"   - æˆäº¤é¢(amount): {quotes_data.get('amount')}")
                logger.info(f"   - æ¶¨è·Œå¹…(change_percent): {quotes_data.get('change_percent')}%")

                # æ›´æ–°åˆ°æ•°æ®åº“
                result = await self.db.market_quotes.update_one(
                    {"code": symbol},
                    {"$set": quotes_data},
                    upsert=True
                )

                logger.info(f"âœ… {symbol} è¡Œæƒ…å·²ä¿å­˜åˆ°æ•°æ®åº“ (matched={result.matched_count}, modified={result.modified_count}, upserted_id={result.upserted_id})")
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ è·å– {symbol} è¡Œæƒ…å¤±è´¥: {e}", exc_info=True)
            return False

    async def sync_historical_data(
        self,
        start_date: str = None,
        end_date: str = None,
        symbols: List[str] = None,
        incremental: bool = True,
        period: str = "daily"
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å†å²æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨
            incremental: æ˜¯å¦å¢é‡åŒæ­¥
            period: æ•°æ®å‘¨æœŸ (daily/weekly/monthly)

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        period_name = {"daily": "æ—¥çº¿", "weekly": "å‘¨çº¿", "monthly": "æœˆçº¿"}.get(period, "æ—¥çº¿")
        logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥{period_name}å†å²æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "total_records": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }

        try:
            # 1. ç¡®å®šå…¨å±€ç»“æŸæ—¥æœŸ
            if not end_date:
                end_date = datetime.now().strftime('%Y-%m-%d')

            # 2. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                basic_info_cursor = self.db.stock_basic_info.find({}, {"code": 1})
                symbols = [doc["code"] async for doc in basic_info_cursor]

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)

            # 3. ç¡®å®šå…¨å±€èµ·å§‹æ—¥æœŸï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
            global_start_date = start_date
            if not global_start_date:
                if incremental:
                    global_start_date = "å„è‚¡ç¥¨æœ€åæ—¥æœŸ"
                else:
                    global_start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

            logger.info(f"ğŸ“Š å†å²æ•°æ®åŒæ­¥: ç»“æŸæ—¥æœŸ={end_date}, è‚¡ç¥¨æ•°é‡={len(symbols)}, æ¨¡å¼={'å¢é‡' if incremental else 'å…¨é‡'}")

            # 4. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_historical_batch(
                    batch, start_date, end_date, period, incremental
                )

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["total_records"] += batch_stats["total_records"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ å†å²æ•°æ®åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, è®°å½•: {stats['total_records']})")

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 4. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"ğŸ‰ å†å²æ•°æ®åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åªè‚¡ç¥¨, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"è®°å½•: {stats['total_records']}æ¡, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ å†å²æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_historical_data"})
            return stats

    async def _process_historical_batch(
        self,
        batch: List[str],
        start_date: str,
        end_date: str,
        period: str = "daily",
        incremental: bool = False
    ) -> Dict[str, Any]:
        """å¤„ç†å†å²æ•°æ®æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "total_records": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # ç¡®å®šè¯¥è‚¡ç¥¨çš„èµ·å§‹æ—¥æœŸ
                symbol_start_date = start_date
                if not symbol_start_date:
                    if incremental:
                        # å¢é‡åŒæ­¥ï¼šè·å–è¯¥è‚¡ç¥¨çš„æœ€åæ—¥æœŸ
                        symbol_start_date = await self._get_last_sync_date(symbol)
                        logger.debug(f"ğŸ“… {symbol}: ä» {symbol_start_date} å¼€å§‹åŒæ­¥")
                    else:
                        # å…¨é‡åŒæ­¥ï¼šæœ€è¿‘1å¹´
                        symbol_start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')

                # è·å–å†å²æ•°æ®
                hist_data = await self.provider.get_historical_data(symbol, symbol_start_date, end_date, period)

                if hist_data is not None and not hist_data.empty:
                    # ä¿å­˜åˆ°ç»Ÿä¸€å†å²æ•°æ®é›†åˆ
                    if self.historical_service is None:
                        self.historical_service = await get_historical_data_service()

                    saved_count = await self.historical_service.save_historical_data(
                        symbol=symbol,
                        data=hist_data,
                        data_source="akshare",
                        market="CN",
                        period=period
                    )

                    batch_stats["success_count"] += 1
                    batch_stats["total_records"] += saved_count
                    logger.debug(f"âœ… {symbol}å†å²æ•°æ®åŒæ­¥æˆåŠŸ: {saved_count}æ¡è®°å½•")
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": "å†å²æ•°æ®ä¸ºç©º",
                        "context": "_process_historical_batch"
                    })

            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": symbol,
                    "error": str(e),
                    "context": "_process_historical_batch"
                })

        return batch_stats

    async def _get_last_sync_date(self, symbol: str = None) -> str:
        """
        è·å–æœ€ååŒæ­¥æ—¥æœŸ

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœæä¾›åˆ™è¿”å›è¯¥è‚¡ç¥¨çš„æœ€åæ—¥æœŸ+1å¤©

        Returns:
            æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        """
        try:
            if self.historical_service is None:
                self.historical_service = await get_historical_data_service()

            if symbol:
                # è·å–ç‰¹å®šè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ
                latest_date = await self.historical_service.get_latest_date(symbol, "akshare")
                if latest_date:
                    # è¿”å›æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©ï¼ˆé¿å…é‡å¤åŒæ­¥ï¼‰
                    try:
                        last_date_obj = datetime.strptime(latest_date, '%Y-%m-%d')
                        next_date = last_date_obj + timedelta(days=1)
                        return next_date.strftime('%Y-%m-%d')
                    except ValueError:
                        # å¦‚æœæ—¥æœŸæ ¼å¼ä¸å¯¹ï¼Œç›´æ¥è¿”å›
                        return latest_date
                else:
                    # ğŸ”¥ æ²¡æœ‰å†å²æ•°æ®æ—¶ï¼Œä»ä¸Šå¸‚æ—¥æœŸå¼€å§‹å…¨é‡åŒæ­¥
                    stock_info = await self.db.stock_basic_info.find_one(
                        {"code": symbol},
                        {"list_date": 1}
                    )
                    if stock_info and stock_info.get("list_date"):
                        list_date = stock_info["list_date"]
                        # å¤„ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                        if isinstance(list_date, str):
                            # æ ¼å¼å¯èƒ½æ˜¯ "20100101" æˆ– "2010-01-01"
                            if len(list_date) == 8 and list_date.isdigit():
                                return f"{list_date[:4]}-{list_date[4:6]}-{list_date[6:]}"
                            else:
                                return list_date
                        else:
                            return list_date.strftime('%Y-%m-%d')

                    # å¦‚æœæ²¡æœ‰ä¸Šå¸‚æ—¥æœŸï¼Œä»1990å¹´å¼€å§‹
                    logger.warning(f"âš ï¸ {symbol}: æœªæ‰¾åˆ°ä¸Šå¸‚æ—¥æœŸï¼Œä»1990-01-01å¼€å§‹åŒæ­¥")
                    return "1990-01-01"

            # é»˜è®¤è¿”å›30å¤©å‰ï¼ˆç¡®ä¿ä¸æ¼æ•°æ®ï¼‰
            return (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€ååŒæ­¥æ—¥æœŸå¤±è´¥ {symbol}: {e}")
            # å‡ºé”™æ—¶è¿”å›30å¤©å‰ï¼Œç¡®ä¿ä¸æ¼æ•°æ®
            return (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

    async def sync_financial_data(self, symbols: List[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥è´¢åŠ¡æ•°æ®

        Args:
            symbols: æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥è´¢åŠ¡æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "start_time": datetime.utcnow(),
            "end_time": None,
            "duration": 0,
            "errors": []
        }

        try:
            # 1. ç¡®å®šè¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                basic_info_cursor = self.db.stock_basic_info.find(
                    {
                        "$or": [
                            {"market_info.market": "CN"},  # æ–°æ•°æ®ç»“æ„
                            {"category": "stock_cn"},      # æ—§æ•°æ®ç»“æ„
                            {"market": {"$in": ["ä¸»æ¿", "åˆ›ä¸šæ¿", "ç§‘åˆ›æ¿", "åŒ—äº¤æ‰€"]}}  # æŒ‰å¸‚åœºç±»å‹
                        ]
                    },
                    {"code": 1}
                )
                symbols = [doc["code"] async for doc in basic_info_cursor]
                logger.info(f"ğŸ“‹ ä» stock_basic_info è·å–åˆ° {len(symbols)} åªè‚¡ç¥¨")

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦åŒæ­¥çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®")

            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_financial_batch(batch)

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ è´¢åŠ¡æ•°æ®åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, é”™è¯¯: {stats['error_count']})")

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"ğŸ‰ è´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»è®¡: {stats['total_processed']}åªè‚¡ç¥¨, "
                       f"æˆåŠŸ: {stats['success_count']}, "
                       f"é”™è¯¯: {stats['error_count']}, "
                       f"è€—æ—¶: {stats['duration']:.2f}ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ è´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_financial_data"})
            return stats

    async def _process_financial_batch(self, batch: List[str]) -> Dict[str, Any]:
        """å¤„ç†è´¢åŠ¡æ•°æ®æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # è·å–è´¢åŠ¡æ•°æ®
                financial_data = await self.provider.get_financial_data(symbol)

                if financial_data:
                    # ä½¿ç”¨ç»Ÿä¸€çš„è´¢åŠ¡æ•°æ®æœåŠ¡ä¿å­˜æ•°æ®
                    success = await self._save_financial_data(symbol, financial_data)
                    if success:
                        batch_stats["success_count"] += 1
                        logger.debug(f"âœ… {symbol}è´¢åŠ¡æ•°æ®ä¿å­˜æˆåŠŸ")
                    else:
                        batch_stats["error_count"] += 1
                        batch_stats["errors"].append({
                            "code": symbol,
                            "error": "è´¢åŠ¡æ•°æ®ä¿å­˜å¤±è´¥",
                            "context": "_process_financial_batch"
                        })
                else:
                    batch_stats["error_count"] += 1
                    batch_stats["errors"].append({
                        "code": symbol,
                        "error": "è´¢åŠ¡æ•°æ®ä¸ºç©º",
                        "context": "_process_financial_batch"
                    })

            except Exception as e:
                batch_stats["error_count"] += 1
                batch_stats["errors"].append({
                    "code": symbol,
                    "error": str(e),
                    "context": "_process_financial_batch"
                })

        return batch_stats

    async def _save_financial_data(self, symbol: str, financial_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è´¢åŠ¡æ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è´¢åŠ¡æ•°æ®æœåŠ¡
            from app.services.financial_data_service import get_financial_data_service

            financial_service = await get_financial_data_service()

            # ä¿å­˜è´¢åŠ¡æ•°æ®
            saved_count = await financial_service.save_financial_data(
                symbol=symbol,
                financial_data=financial_data,
                data_source="akshare",
                market="CN",
                report_type="quarterly"
            )

            return saved_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {symbol} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            return False

    async def run_status_check(self) -> Dict[str, Any]:
        """è¿è¡ŒçŠ¶æ€æ£€æŸ¥"""
        try:
            logger.info("ğŸ” å¼€å§‹AKShareçŠ¶æ€æ£€æŸ¥...")

            # æ£€æŸ¥æä¾›å™¨è¿æ¥
            provider_connected = await self.provider.test_connection()

            # æ£€æŸ¥æ•°æ®åº“é›†åˆçŠ¶æ€
            collections_status = {}

            # æ£€æŸ¥åŸºç¡€ä¿¡æ¯é›†åˆ
            basic_count = await self.db.stock_basic_info.count_documents({})
            latest_basic = await self.db.stock_basic_info.find_one(
                {}, sort=[("updated_at", -1)]
            )
            collections_status["stock_basic_info"] = {
                "count": basic_count,
                "latest_update": latest_basic.get("updated_at") if latest_basic else None
            }

            # æ£€æŸ¥è¡Œæƒ…æ•°æ®é›†åˆ
            quotes_count = await self.db.market_quotes.count_documents({})
            latest_quotes = await self.db.market_quotes.find_one(
                {}, sort=[("updated_at", -1)]
            )
            collections_status["market_quotes"] = {
                "count": quotes_count,
                "latest_update": latest_quotes.get("updated_at") if latest_quotes else None
            }

            status_result = {
                "provider_connected": provider_connected,
                "collections": collections_status,
                "status_time": datetime.utcnow()
            }

            logger.info(f"âœ… AKShareçŠ¶æ€æ£€æŸ¥å®Œæˆ: {status_result}")
            return status_result

        except Exception as e:
            logger.error(f"âŒ AKShareçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "provider_connected": False,
                "error": str(e),
                "status_time": datetime.utcnow()
            }

    # ==================== æ–°é—»æ•°æ®åŒæ­¥ ====================

    async def _get_favorite_stocks(self) -> List[str]:
        """
        è·å–æ‰€æœ‰ç”¨æˆ·çš„è‡ªé€‰è‚¡åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        æ³¨æ„ï¼šåªè·å–æœ€æ–°çš„æ–‡æ¡£ï¼Œé¿å…è·å–å†å²æ—§æ•°æ®

        Returns:
            è‡ªé€‰è‚¡ä»£ç åˆ—è¡¨
        """
        try:
            favorite_codes = set()

            # æ–¹æ³•1ï¼šä» users é›†åˆçš„ favorite_stocks å­—æ®µè·å–
            users_cursor = self.db.users.find(
                {"favorite_stocks": {"$exists": True, "$ne": []}},
                {"favorite_stocks.stock_code": 1, "_id": 0}
            )

            async for user in users_cursor:
                for fav in user.get("favorite_stocks", []):
                    code = fav.get("stock_code")
                    if code:
                        favorite_codes.add(code)

            # æ–¹æ³•2ï¼šä» user_favorites é›†åˆè·å–ï¼ˆå…¼å®¹æ—§æ•°æ®ç»“æ„ï¼‰
            # ğŸ”¥ åªè·å–æœ€æ–°çš„ä¸€ä¸ªæ–‡æ¡£ï¼ˆæŒ‰ updated_at é™åºæ’åºï¼‰
            latest_doc = await self.db.user_favorites.find_one(
                {"favorites": {"$exists": True, "$ne": []}},
                {"favorites.stock_code": 1, "_id": 0},
                sort=[("updated_at", -1)]  # æŒ‰æ›´æ–°æ—¶é—´é™åºï¼Œè·å–æœ€æ–°çš„
            )

            if latest_doc:
                logger.info(f"ğŸ“Œ ä» user_favorites è·å–æœ€æ–°æ–‡æ¡£çš„è‡ªé€‰è‚¡")
                for fav in latest_doc.get("favorites", []):
                    code = fav.get("stock_code")
                    if code:
                        favorite_codes.add(code)

            result = sorted(list(favorite_codes))
            logger.info(f"ğŸ“Œ è·å–åˆ° {len(result)} åªè‡ªé€‰è‚¡")
            return result

        except Exception as e:
            logger.error(f"âŒ è·å–è‡ªé€‰è‚¡åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def sync_news_data(
        self,
        symbols: List[str] = None,
        max_news_per_stock: int = 20,
        force_update: bool = False,
        favorites_only: bool = True
    ) -> Dict[str, Any]:
        """
        åŒæ­¥æ–°é—»æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸ºNoneæ—¶æ ¹æ®favorites_onlyå†³å®šåŒæ­¥èŒƒå›´
            max_news_per_stock: æ¯åªè‚¡ç¥¨æœ€å¤§æ–°é—»æ•°é‡
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            favorites_only: æ˜¯å¦åªåŒæ­¥è‡ªé€‰è‚¡ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥AKShareæ–°é—»æ•°æ®...")

        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "start_time": datetime.utcnow(),
            "favorites_only": favorites_only,
            "errors": []
        }

        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                if favorites_only:
                    # åªåŒæ­¥è‡ªé€‰è‚¡
                    symbols = await self._get_favorite_stocks()
                    logger.info(f"ğŸ“Œ åªåŒæ­¥è‡ªé€‰è‚¡ï¼Œå…± {len(symbols)} åª")
                else:
                    # è·å–æ‰€æœ‰è‚¡ç¥¨ï¼ˆä¸é™åˆ¶æ•°æ®æºï¼‰
                    stock_list = await self.db.stock_basic_info.find(
                        {},
                        {"code": 1, "_id": 0}
                    ).to_list(None)
                    symbols = [stock["code"] for stock in stock_list if stock.get("code")]
                    logger.info(f"ğŸ“Š åŒæ­¥æ‰€æœ‰è‚¡ç¥¨ï¼Œå…± {len(symbols)} åª")

            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦åŒæ­¥æ–°é—»çš„è‚¡ç¥¨")
                return stats

            stats["total_processed"] = len(symbols)
            logger.info(f"ğŸ“Š éœ€è¦åŒæ­¥ {len(symbols)} åªè‚¡ç¥¨çš„æ–°é—»")

            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_news_batch(
                    batch, max_news_per_stock
                )

                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["news_count"] += batch_stats["news_count"]
                stats["errors"].extend(batch_stats["errors"])

                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ æ–°é—»åŒæ­¥è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, æ–°é—»: {stats['news_count']})")

                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(self.rate_limit_delay)

            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

            logger.info(f"âœ… AKShareæ–°é—»æ•°æ®åŒæ­¥å®Œæˆ: "
                       f"æ€»è®¡ {stats['total_processed']} åªè‚¡ç¥¨, "
                       f"æˆåŠŸ {stats['success_count']} åª, "
                       f"è·å– {stats['news_count']} æ¡æ–°é—», "
                       f"é”™è¯¯ {stats['error_count']} åª, "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")

            return stats

        except Exception as e:
            logger.error(f"âŒ AKShareæ–°é—»æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_news_data"})
            return stats

    async def _process_news_batch(
        self,
        batch: List[str],
        max_news_per_stock: int
    ) -> Dict[str, Any]:
        """å¤„ç†æ–°é—»æ‰¹æ¬¡"""
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "errors": []
        }

        for symbol in batch:
            try:
                # ä»AKShareè·å–æ–°é—»æ•°æ®
                news_data = await self.provider.get_stock_news(
                    symbol=symbol,
                    limit=max_news_per_stock
                )

                if news_data:
                    # ä¿å­˜æ–°é—»æ•°æ®
                    saved_count = await self.news_service.save_news_data(
                        news_data=news_data,
                        data_source="akshare",
                        market="CN"
                    )

                    batch_stats["success_count"] += 1
                    batch_stats["news_count"] += saved_count

                    logger.debug(f"âœ… {symbol} æ–°é—»åŒæ­¥æˆåŠŸ: {saved_count}æ¡")
                else:
                    logger.debug(f"âš ï¸ {symbol} æœªè·å–åˆ°æ–°é—»æ•°æ®")
                    batch_stats["success_count"] += 1  # æ²¡æœ‰æ–°é—»ä¹Ÿç®—æˆåŠŸ

                # ğŸ”¥ APIé™æµï¼šæˆåŠŸåä¼‘çœ 
                await asyncio.sleep(0.2)

            except Exception as e:
                batch_stats["error_count"] += 1
                error_msg = f"{symbol}: {str(e)}"
                batch_stats["errors"].append(error_msg)
                logger.error(f"âŒ {symbol} æ–°é—»åŒæ­¥å¤±è´¥: {e}")

                # ğŸ”¥ å¤±è´¥åä¹Ÿè¦ä¼‘çœ ï¼Œé¿å…"å¤±è´¥é›ªå´©"
                # å¤±è´¥æ—¶ä¼‘çœ æ›´é•¿æ—¶é—´ï¼Œç»™APIæœåŠ¡å™¨æ¢å¤çš„æœºä¼š
                await asyncio.sleep(1.0)

        return batch_stats


# å…¨å±€åŒæ­¥æœåŠ¡å®ä¾‹
_akshare_sync_service = None

async def get_akshare_sync_service() -> AKShareSyncService:
    """è·å–AKShareåŒæ­¥æœåŠ¡å®ä¾‹"""
    global _akshare_sync_service
    if _akshare_sync_service is None:
        _akshare_sync_service = AKShareSyncService()
        await _akshare_sync_service.initialize()
    return _akshare_sync_service


# APSchedulerå…¼å®¹çš„ä»»åŠ¡å‡½æ•°
async def run_akshare_basic_info_sync(force_update: bool = False):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_stock_basic_info(force_update=force_update)
        logger.info(f"âœ… AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_quotes_sync(force: bool = False):
    """
    APSchedulerä»»åŠ¡ï¼šåŒæ­¥å®æ—¶è¡Œæƒ…

    Args:
        force: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œï¼ˆè·³è¿‡äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼‰ï¼Œé»˜è®¤ False
    """
    try:
        service = await get_akshare_sync_service()
        # æ³¨æ„ï¼šAKShare æ²¡æœ‰äº¤æ˜“æ—¶é—´æ£€æŸ¥é€»è¾‘ï¼Œforce å‚æ•°ä»…ç”¨äºæ¥å£ä¸€è‡´æ€§
        result = await service.sync_realtime_quotes(force=force)
        logger.info(f"âœ… AKShareè¡Œæƒ…åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareè¡Œæƒ…åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_historical_sync(incremental: bool = True):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥å†å²æ•°æ®"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_historical_data(incremental=incremental)
        logger.info(f"âœ… AKShareå†å²æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareå†å²æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_financial_sync():
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥è´¢åŠ¡æ•°æ®"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_financial_data()
        logger.info(f"âœ… AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise


async def run_akshare_status_check():
    """APSchedulerä»»åŠ¡ï¼šçŠ¶æ€æ£€æŸ¥"""
    try:
        service = await get_akshare_sync_service()
        result = await service.run_status_check()
        logger.info(f"âœ… AKShareçŠ¶æ€æ£€æŸ¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        raise


async def run_akshare_news_sync(max_news_per_stock: int = 20):
    """APSchedulerä»»åŠ¡ï¼šåŒæ­¥æ–°é—»æ•°æ®"""
    try:
        service = await get_akshare_sync_service()
        result = await service.sync_news_data(
            max_news_per_stock=max_news_per_stock
        )
        logger.info(f"âœ… AKShareæ–°é—»æ•°æ®åŒæ­¥å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ AKShareæ–°é—»æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        raise
