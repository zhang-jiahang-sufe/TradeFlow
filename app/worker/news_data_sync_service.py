"""
æ–°é—»æ•°æ®åŒæ­¥æœåŠ¡
æ”¯æŒå¤šæ•°æ®æºæ–°é—»æ•°æ®åŒæ­¥å’Œæƒ…ç»ªåˆ†æ
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, field

from app.services.news_data_service import get_news_data_service
from tradingagents.dataflows.providers.china.tushare import get_tushare_provider
from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
from tradingagents.dataflows.news.realtime_news import RealtimeNewsAggregator

logger = logging.getLogger(__name__)


@dataclass
class NewsSyncStats:
    """æ–°é—»åŒæ­¥ç»Ÿè®¡"""
    total_processed: int = 0
    successful_saves: int = 0
    failed_saves: int = 0
    duplicate_skipped: int = 0
    sources_used: List[str] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.utcnow)
    end_time: Optional[datetime] = None
    
    @property
    def duration_seconds(self) -> float:
        """åŒæ­¥è€—æ—¶ï¼ˆç§’ï¼‰"""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_processed == 0:
            return 0.0
        return (self.successful_saves / self.total_processed) * 100


class NewsDataSyncService:
    """æ–°é—»æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._news_service = None
        self._tushare_provider = None
        self._akshare_provider = None
        self._realtime_aggregator = None
    
    async def _get_news_service(self):
        """è·å–æ–°é—»æ•°æ®æœåŠ¡"""
        if self._news_service is None:
            self._news_service = await get_news_data_service()
        return self._news_service
    
    async def _get_tushare_provider(self):
        """è·å–Tushareæä¾›è€…"""
        if self._tushare_provider is None:
            self._tushare_provider = get_tushare_provider()
            await self._tushare_provider.connect()
        return self._tushare_provider
    
    async def _get_tushare_provider(self):
        """è·å–Tushareæä¾›è€…"""
        if self._tushare_provider is None:
            from tradingagents.dataflows.providers.china.tushare import get_tushare_provider
            self._tushare_provider = get_tushare_provider()
            await self._tushare_provider.connect()
        return self._tushare_provider

    async def _get_akshare_provider(self):
        """è·å–AKShareæä¾›è€…"""
        if self._akshare_provider is None:
            self._akshare_provider = get_akshare_provider()
            await self._akshare_provider.connect()
        return self._akshare_provider
    
    async def _get_realtime_aggregator(self):
        """è·å–å®æ—¶æ–°é—»èšåˆå™¨"""
        if self._realtime_aggregator is None:
            self._realtime_aggregator = RealtimeNewsAggregator()
        return self._realtime_aggregator
    
    async def sync_stock_news(
        self,
        symbol: str,
        data_sources: List[str] = None,
        hours_back: int = 24,
        max_news_per_source: int = 50
    ) -> NewsSyncStats:
        """
        åŒæ­¥å•åªè‚¡ç¥¨çš„æ–°é—»æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_sources: æ•°æ®æºåˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰å¯ç”¨æº
            hours_back: å›æº¯å°æ—¶æ•°
            max_news_per_source: æ¯ä¸ªæ•°æ®æºæœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        """
        stats = NewsSyncStats()
        
        try:
            self.logger.info(f"ğŸ“° å¼€å§‹åŒæ­¥è‚¡ç¥¨æ–°é—»: {symbol}")
            
            if data_sources is None:
                data_sources = ["tushare", "akshare", "realtime"]
            
            news_service = await self._get_news_service()
            all_news = []
            
            # 1. Tushareæ–°é—»
            if "tushare" in data_sources:
                try:
                    tushare_news = await self._sync_tushare_news(
                        symbol, hours_back, max_news_per_source
                    )
                    if tushare_news:
                        all_news.extend(tushare_news)
                        stats.sources_used.append("tushare")
                        self.logger.info(f"âœ… Tushareæ–°é—»è·å–æˆåŠŸ: {len(tushare_news)}æ¡")
                except Exception as e:
                    self.logger.error(f"âŒ Tushareæ–°é—»è·å–å¤±è´¥: {e}")
            
            # 2. AKShareæ–°é—»
            if "akshare" in data_sources:
                try:
                    akshare_news = await self._sync_akshare_news(
                        symbol, hours_back, max_news_per_source
                    )
                    if akshare_news:
                        all_news.extend(akshare_news)
                        stats.sources_used.append("akshare")
                        self.logger.info(f"âœ… AKShareæ–°é—»è·å–æˆåŠŸ: {len(akshare_news)}æ¡")
                except Exception as e:
                    self.logger.error(f"âŒ AKShareæ–°é—»è·å–å¤±è´¥: {e}")
            
            # 3. å®æ—¶æ–°é—»èšåˆ
            if "realtime" in data_sources:
                try:
                    realtime_news = await self._sync_realtime_news(
                        symbol, hours_back, max_news_per_source
                    )
                    if realtime_news:
                        all_news.extend(realtime_news)
                        stats.sources_used.append("realtime")
                        self.logger.info(f"âœ… å®æ—¶æ–°é—»è·å–æˆåŠŸ: {len(realtime_news)}æ¡")
                except Exception as e:
                    self.logger.error(f"âŒ å®æ—¶æ–°é—»è·å–å¤±è´¥: {e}")
            
            # ä¿å­˜æ–°é—»æ•°æ®
            if all_news:
                stats.total_processed = len(all_news)
                
                # å»é‡å¤„ç†
                unique_news = self._deduplicate_news(all_news)
                stats.duplicate_skipped = len(all_news) - len(unique_news)
                
                # æ‰¹é‡ä¿å­˜
                saved_count = await news_service.save_news_data(
                    unique_news, "multi_source", "CN"
                )
                stats.successful_saves = saved_count
                stats.failed_saves = len(unique_news) - saved_count
                
                self.logger.info(f"ğŸ’¾ {symbol} æ–°é—»åŒæ­¥å®Œæˆ: {saved_count}æ¡ä¿å­˜æˆåŠŸ")
            
            stats.end_time = datetime.utcnow()
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ åŒæ­¥è‚¡ç¥¨æ–°é—»å¤±è´¥ {symbol}: {e}")
            stats.end_time = datetime.utcnow()
            return stats
    
    async def _sync_tushare_news(
        self,
        symbol: str,
        hours_back: int,
        max_news: int
    ) -> List[Dict[str, Any]]:
        """åŒæ­¥Tushareæ–°é—»"""
        try:
            provider = await self._get_tushare_provider()

            if not provider.is_available():
                self.logger.warning("âš ï¸ Tushareæä¾›è€…ä¸å¯ç”¨")
                return []

            # è·å–æ–°é—»æ•°æ®ï¼Œä¼ é€’hours_backå‚æ•°
            news_data = await provider.get_stock_news(
                symbol=symbol,
                limit=max_news,
                hours_back=hours_back
            )

            if news_data:
                # æ ‡å‡†åŒ–æ–°é—»æ•°æ®
                standardized_news = []
                for news in news_data:
                    standardized = self._standardize_tushare_news(news, symbol)
                    if standardized:
                        standardized_news.append(standardized)

                self.logger.info(f"âœ… Tushareæ–°é—»è·å–æˆåŠŸ: {len(standardized_news)}æ¡")
                return standardized_news
            else:
                self.logger.debug("âš ï¸ Tushareæœªè¿”å›æ–°é—»æ•°æ®")
                return []

        except Exception as e:
            # è¯¦ç»†çš„é”™è¯¯å¤„ç†
            if any(keyword in str(e).lower() for keyword in ['æƒé™', 'permission', 'unauthorized']):
                self.logger.warning(f"âš ï¸ Tushareæ–°é—»æ¥å£éœ€è¦å•ç‹¬å¼€é€šæƒé™: {e}")
            elif "ç§¯åˆ†" in str(e) or "point" in str(e).lower():
                self.logger.warning(f"âš ï¸ Tushareç§¯åˆ†ä¸è¶³: {e}")
            else:
                self.logger.error(f"âŒ Tushareæ–°é—»åŒæ­¥å¤±è´¥: {e}")
            return []
    
    async def _sync_akshare_news(
        self, 
        symbol: str, 
        hours_back: int, 
        max_news: int
    ) -> List[Dict[str, Any]]:
        """åŒæ­¥AKShareæ–°é—»"""
        try:
            provider = await self._get_akshare_provider()
            
            if not provider.is_available():
                return []
            
            # è·å–æ–°é—»æ•°æ®
            news_data = await provider.get_stock_news(symbol, limit=max_news)
            
            if news_data:
                # æ ‡å‡†åŒ–æ–°é—»æ•°æ®
                standardized_news = []
                for news in news_data:
                    standardized = self._standardize_akshare_news(news, symbol)
                    if standardized:
                        standardized_news.append(standardized)
                
                return standardized_news
            
            return []
            
        except Exception as e:
            self.logger.error(f"âŒ AKShareæ–°é—»åŒæ­¥å¤±è´¥: {e}")
            return []
    
    async def _sync_realtime_news(
        self, 
        symbol: str, 
        hours_back: int, 
        max_news: int
    ) -> List[Dict[str, Any]]:
        """åŒæ­¥å®æ—¶æ–°é—»"""
        try:
            aggregator = await self._get_realtime_aggregator()
            
            # è·å–å®æ—¶æ–°é—»
            news_items = aggregator.get_realtime_stock_news(
                symbol, hours_back, max_news
            )
            
            if news_items:
                # æ ‡å‡†åŒ–æ–°é—»æ•°æ®
                standardized_news = []
                for news_item in news_items:
                    standardized = self._standardize_realtime_news(news_item, symbol)
                    if standardized:
                        standardized_news.append(standardized)
                
                return standardized_news
            
            return []
            
        except Exception as e:
            self.logger.error(f"âŒ å®æ—¶æ–°é—»åŒæ­¥å¤±è´¥: {e}")
            return []
    
    def _standardize_tushare_news(self, news: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–Tushareæ–°é—»æ•°æ®"""
        try:
            return {
                "symbol": symbol,
                "title": news.get("title", ""),
                "content": news.get("content", ""),
                "summary": news.get("summary", ""),
                "url": news.get("url", ""),
                "source": news.get("source", "Tushare"),
                "author": news.get("author", ""),
                "publish_time": news.get("publish_time"),
                "category": self._classify_news_category(news.get("title", "")),
                "sentiment": self._analyze_sentiment(news.get("title", "") + " " + news.get("content", "")),
                "importance": self._assess_importance(news.get("title", "")),
                "keywords": self._extract_keywords(news.get("title", "") + " " + news.get("content", "")),
                "data_source": "tushare"
            }
        except Exception as e:
            self.logger.error(f"âŒ æ ‡å‡†åŒ–Tushareæ–°é—»å¤±è´¥: {e}")
            return None
    
    def _standardize_akshare_news(self, news: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–AKShareæ–°é—»æ•°æ®"""
        try:
            return {
                "symbol": symbol,
                "title": news.get("title", ""),
                "content": news.get("content", ""),
                "summary": news.get("summary", ""),
                "url": news.get("url", ""),
                "source": news.get("source", "AKShare"),
                "author": news.get("author", ""),
                "publish_time": news.get("publish_time"),
                "category": self._classify_news_category(news.get("title", "")),
                "sentiment": self._analyze_sentiment(news.get("title", "") + " " + news.get("content", "")),
                "importance": self._assess_importance(news.get("title", "")),
                "keywords": self._extract_keywords(news.get("title", "") + " " + news.get("content", "")),
                "data_source": "akshare"
            }
        except Exception as e:
            self.logger.error(f"âŒ æ ‡å‡†åŒ–AKShareæ–°é—»å¤±è´¥: {e}")
            return None
    
    def _standardize_realtime_news(self, news_item, symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–å®æ—¶æ–°é—»æ•°æ®"""
        try:
            return {
                "symbol": symbol,
                "title": news_item.title,
                "content": news_item.content,
                "summary": news_item.content[:200] + "..." if len(news_item.content) > 200 else news_item.content,
                "url": news_item.url,
                "source": news_item.source,
                "author": "",
                "publish_time": news_item.publish_time,
                "category": self._classify_news_category(news_item.title),
                "sentiment": self._analyze_sentiment(news_item.title + " " + news_item.content),
                "importance": self._assess_importance(news_item.title),
                "keywords": self._extract_keywords(news_item.title + " " + news_item.content),
                "data_source": "realtime"
            }
        except Exception as e:
            self.logger.error(f"âŒ æ ‡å‡†åŒ–å®æ—¶æ–°é—»å¤±è´¥: {e}")
            return None
    
    def _classify_news_category(self, title: str) -> str:
        """åˆ†ç±»æ–°é—»ç±»åˆ«"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ["å¹´æŠ¥", "å­£æŠ¥", "ä¸šç»©", "è´¢æŠ¥", "å…¬å‘Š"]):
            return "company_announcement"
        elif any(word in title_lower for word in ["æ”¿ç­–", "å¤®è¡Œ", "ç›‘ç®¡", "æ³•è§„"]):
            return "policy_news"
        elif any(word in title_lower for word in ["å¸‚åœº", "è¡Œæƒ…", "æŒ‡æ•°", "æ¿å—"]):
            return "market_news"
        elif any(word in title_lower for word in ["ç ”æŠ¥", "åˆ†æ", "è¯„çº§", "æ¨è"]):
            return "research_report"
        else:
            return "general"
    
    def _analyze_sentiment(self, text: str) -> str:
        """åˆ†ææƒ…ç»ª"""
        text_lower = text.lower()
        
        positive_words = ["å¢é•¿", "ä¸Šæ¶¨", "åˆ©å¥½", "ç›ˆåˆ©", "æˆåŠŸ", "çªç ´", "åˆ›æ–°", "ä¼˜ç§€"]
        negative_words = ["ä¸‹è·Œ", "äºæŸ", "é£é™©", "é—®é¢˜", "å›°éš¾", "ä¸‹æ»‘", "å‡å°‘", "è­¦å‘Š"]
        
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def _assess_importance(self, title: str) -> str:
        """è¯„ä¼°é‡è¦æ€§"""
        title_lower = title.lower()
        
        high_importance_words = ["é‡å¤§", "ç´§æ€¥", "çªå‘", "å¹´æŠ¥", "ä¸šç»©", "é‡ç»„", "æ”¶è´­"]
        medium_importance_words = ["å…¬å‘Š", "é€šçŸ¥", "å˜æ›´", "è°ƒæ•´", "è®¡åˆ’"]
        
        if any(word in title_lower for word in high_importance_words):
            return "high"
        elif any(word in title_lower for word in medium_importance_words):
            return "medium"
        else:
            return "low"
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
        keywords = []
        
        common_keywords = [
            "ä¸šç»©", "å¹´æŠ¥", "å­£æŠ¥", "å¢é•¿", "åˆ©æ¶¦", "è¥æ”¶", "è‚¡ä»·", "æŠ•èµ„",
            "å¸‚åœº", "è¡Œä¸š", "æ”¿ç­–", "ç›‘ç®¡", "é£é™©", "æœºä¼š", "åˆ›æ–°", "å‘å±•"
        ]
        
        for keyword in common_keywords:
            if keyword in text:
                keywords.append(keyword)
        
        return keywords[:10]  # æœ€å¤šè¿”å›10ä¸ªå…³é”®è¯
    
    def _deduplicate_news(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ–°é—»"""
        seen = set()
        unique_news = []
        
        for news in news_list:
            # ä½¿ç”¨æ ‡é¢˜å’ŒURLä½œä¸ºå»é‡æ ‡è¯†
            key = (news.get("title", ""), news.get("url", ""))
            if key not in seen:
                seen.add(key)
                unique_news.append(news)
        
        return unique_news
    
    async def sync_market_news(
        self,
        data_sources: List[str] = None,
        hours_back: int = 24,
        max_news_per_source: int = 100
    ) -> NewsSyncStats:
        """
        åŒæ­¥å¸‚åœºæ–°é—»
        
        Args:
            data_sources: æ•°æ®æºåˆ—è¡¨
            hours_back: å›æº¯å°æ—¶æ•°
            max_news_per_source: æ¯ä¸ªæ•°æ®æºæœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        """
        stats = NewsSyncStats()
        
        try:
            self.logger.info("ğŸ“° å¼€å§‹åŒæ­¥å¸‚åœºæ–°é—»...")
            
            if data_sources is None:
                data_sources = ["realtime"]
            
            news_service = await self._get_news_service()
            all_news = []
            
            # å®æ—¶å¸‚åœºæ–°é—»
            if "realtime" in data_sources:
                try:
                    aggregator = await self._get_realtime_aggregator()
                    
                    # è·å–å¸‚åœºæ–°é—»ï¼ˆä¸æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼‰
                    news_items = aggregator.get_realtime_stock_news(
                        None, hours_back, max_news_per_source
                    )
                    
                    if news_items:
                        for news_item in news_items:
                            standardized = self._standardize_realtime_news(news_item, None)
                            if standardized:
                                all_news.append(standardized)
                        
                        stats.sources_used.append("realtime")
                        self.logger.info(f"âœ… å¸‚åœºæ–°é—»è·å–æˆåŠŸ: {len(all_news)}æ¡")
                        
                except Exception as e:
                    self.logger.error(f"âŒ å¸‚åœºæ–°é—»è·å–å¤±è´¥: {e}")
            
            # ä¿å­˜æ–°é—»æ•°æ®
            if all_news:
                stats.total_processed = len(all_news)
                
                # å»é‡å¤„ç†
                unique_news = self._deduplicate_news(all_news)
                stats.duplicate_skipped = len(all_news) - len(unique_news)
                
                # æ‰¹é‡ä¿å­˜
                saved_count = await news_service.save_news_data(
                    unique_news, "market_news", "CN"
                )
                stats.successful_saves = saved_count
                stats.failed_saves = len(unique_news) - saved_count
                
                self.logger.info(f"ğŸ’¾ å¸‚åœºæ–°é—»åŒæ­¥å®Œæˆ: {saved_count}æ¡ä¿å­˜æˆåŠŸ")
            
            stats.end_time = datetime.utcnow()
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ åŒæ­¥å¸‚åœºæ–°é—»å¤±è´¥: {e}")
            stats.end_time = datetime.utcnow()
            return stats


# å…¨å±€æœåŠ¡å®ä¾‹
_sync_service_instance = None

async def get_news_data_sync_service() -> NewsDataSyncService:
    """è·å–æ–°é—»æ•°æ®åŒæ­¥æœåŠ¡å®ä¾‹"""
    global _sync_service_instance
    if _sync_service_instance is None:
        _sync_service_instance = NewsDataSyncService()
        logger.info("âœ… æ–°é—»æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    return _sync_service_instance
