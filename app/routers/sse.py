from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
import json
import logging
import time

from app.routers.auth_db import get_current_user
from app.core.database import get_redis_client
from app.core.config import settings

from app.services.queue_service import get_queue_service, QueueService

router = APIRouter()
logger = logging.getLogger("webapi.sse")


async def task_progress_generator(task_id: str, user_id: str):
    """Generate SSE events for task progress updates"""
    r = get_redis_client()
    pubsub = None
    channel = f"task_progress:{task_id}"

    try:
        # Load dynamic SSE settings
        try:
            from app.services.config_provider import provider as config_provider
            eff = await config_provider.get_effective_system_settings()
            poll_timeout = float(eff.get("sse_poll_timeout_seconds", 1.0))
            heartbeat_every = int(eff.get("sse_heartbeat_interval_seconds", 10))
            max_idle_seconds = int(eff.get("sse_task_max_idle_seconds", 300))
        except Exception:
            poll_timeout = float(getattr(settings, "SSE_POLL_TIMEOUT_SECONDS", 1.0))
            heartbeat_every = int(getattr(settings, "SSE_HEARTBEAT_INTERVAL_SECONDS", 10))
            max_idle_seconds = int(getattr(settings, "SSE_TASK_MAX_IDLE_SECONDS", 300))

        # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»º PubSub è¿æ¥
        pubsub = r.pubsub()
        logger.info(f"ğŸ“¡ [SSE-Task] åˆ›å»º PubSub è¿æ¥: task={task_id}, user={user_id}")

        # ğŸ”¥ ä¿®å¤ï¼šè®¢é˜…é¢‘é“ï¼ˆå¯èƒ½å¤±è´¥ï¼Œéœ€è¦ç¡®ä¿ pubsub è¢«æ¸…ç†ï¼‰
        try:
            await pubsub.subscribe(channel)
            logger.info(f"âœ… [SSE-Task] è®¢é˜…é¢‘é“æˆåŠŸ: {channel}")
            # Send initial connection confirmation
            yield f"event: connected\ndata: {{\"task_id\": \"{task_id}\", \"message\": \"å·²è¿æ¥è¿›åº¦æµ\"}}\n\n"
        except Exception as subscribe_error:
            # ğŸ”¥ è®¢é˜…å¤±è´¥æ—¶ç«‹å³æ¸…ç† pubsub è¿æ¥
            logger.error(f"âŒ [SSE-Task] è®¢é˜…é¢‘é“å¤±è´¥: {subscribe_error}")
            try:
                await pubsub.close()
                logger.info(f"ğŸ§¹ [SSE-Task] è®¢é˜…å¤±è´¥åå·²å…³é—­ PubSub è¿æ¥")
            except Exception as close_error:
                logger.error(f"âŒ [SSE-Task] å…³é—­ PubSub è¿æ¥å¤±è´¥: {close_error}")
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚ except å¤„ç†
            raise

        # Listen for progress updates
        idle_elapsed = 0.0
        last_hb = time.monotonic()

        while idle_elapsed < max_idle_seconds:
            try:
                message = await asyncio.wait_for(pubsub.get_message(ignore_subscribe_messages=True), timeout=poll_timeout)
                if message and message['type'] == 'message':
                    # Reset idle timer on valid message
                    idle_elapsed = 0.0
                    try:
                        progress_data = json.loads(message['data'])
                        yield f"event: progress\ndata: {json.dumps(progress_data, ensure_ascii=False)}\n\n"
                    except json.JSONDecodeError:
                        logger.warning(f"Invalid JSON in progress message: {message['data']}")
                else:
                    # No update: accumulate idle time and send heartbeat if due
                    idle_elapsed += poll_timeout
                    now = time.monotonic()
                    if now - last_hb >= heartbeat_every:
                        yield f"event: heartbeat\ndata: {{\"timestamp\": \"{asyncio.get_event_loop().time()}\"}}\n\n"
                        last_hb = now

            except asyncio.TimeoutError:
                idle_elapsed += poll_timeout
                continue

    except Exception as e:
        logger.exception(f"SSE error for task {task_id}: {e}")
        yield f"event: error\ndata: {{\"error\": \"è¿æ¥å¼‚å¸¸: {str(e)}\"}}\n\n"
    finally:
        # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½é‡Šæ”¾è¿æ¥
        if pubsub:
            logger.info(f"ğŸ§¹ [SSE-Task] æ¸…ç† PubSub è¿æ¥: task={task_id}")

            # åˆ†æ­¥éª¤å…³é—­ï¼Œç¡®ä¿å³ä½¿ unsubscribe å¤±è´¥ä¹Ÿèƒ½å…³é—­è¿æ¥
            try:
                await pubsub.unsubscribe(channel)
                logger.debug(f"âœ… [SSE-Task] å·²å–æ¶ˆè®¢é˜…é¢‘é“: {channel}")
            except Exception as e:
                logger.warning(f"âš ï¸ [SSE-Task] å–æ¶ˆè®¢é˜…å¤±è´¥ï¼ˆå°†ç»§ç»­å…³é—­è¿æ¥ï¼‰: {e}")

            try:
                await pubsub.close()
                logger.info(f"âœ… [SSE-Task] PubSub è¿æ¥å·²å…³é—­: task={task_id}")
            except Exception as e:
                logger.error(f"âŒ [SSE-Task] å…³é—­ PubSub è¿æ¥å¤±è´¥: {e}", exc_info=True)
                # å³ä½¿å…³é—­å¤±è´¥ï¼Œä¹Ÿå°è¯•é‡ç½®è¿æ¥
                try:
                    await pubsub.reset()
                    logger.info(f"ğŸ”„ [SSE-Task] PubSub è¿æ¥å·²é‡ç½®: task={task_id}")
                except Exception as reset_error:
                    logger.error(f"âŒ [SSE-Task] é‡ç½® PubSub è¿æ¥ä¹Ÿå¤±è´¥: {reset_error}")


async def batch_progress_generator(batch_id: str, user_id: str):
    """Generate SSE events for batch progress updates"""
    svc = get_queue_service()

    try:
        # Load dynamic SSE settings for batch stream
        try:
            from app.services.config_provider import provider as config_provider
            eff = await config_provider.get_effective_system_settings()
            batch_poll_interval = float(eff.get("sse_batch_poll_interval_seconds", 2))
            batch_max_idle_seconds = int(eff.get("sse_batch_max_idle_seconds", 600))
        except Exception:
            batch_poll_interval = float(getattr(settings, "SSE_BATCH_POLL_INTERVAL_SECONDS", 2.0))
            batch_max_idle_seconds = int(getattr(settings, "SSE_BATCH_MAX_IDLE_SECONDS", 600))

        # Send initial connection confirmation
        yield f"event: connected\ndata: {{\"batch_id\": \"{batch_id}\", \"message\": \"å·²è¿æ¥æ‰¹æ¬¡è¿›åº¦æµ\"}}\n\n"

        idle_elapsed = 0.0

        while idle_elapsed < batch_max_idle_seconds:
            try:
                # Get current batch status
                batch_data = await svc.get_batch(batch_id)
                if not batch_data:
                    yield f"event: error\ndata: {{\"error\": \"æ‰¹æ¬¡ä¸å­˜åœ¨\"}}\n\n"
                    break

                # Check if batch belongs to user
                if batch_data.get("user") != user_id:
                    yield f"event: error\ndata: {{\"error\": \"æ— æƒé™è®¿é—®æ­¤æ‰¹æ¬¡\"}}\n\n"
                    break

                # Calculate batch progress based on task statuses
                task_ids = batch_data.get("tasks", [])
                if not task_ids:
                    yield f"event: progress\ndata: {{\"batch_id\": \"{batch_id}\", \"message\": \"æ‰¹æ¬¡æ— ä»»åŠ¡\", \"progress\": 0}}\n\n"
                    await asyncio.sleep(batch_poll_interval)
                    idle_elapsed += batch_poll_interval
                    continue

                completed_count = 0
                failed_count = 0
                processing_count = 0

                for task_id in task_ids:
                    task_data = await svc.get_task(task_id)
                    if task_data:
                        status = task_data.get("status", "queued")
                        if status == "completed":
                            completed_count += 1
                        elif status == "failed":
                            failed_count += 1
                        elif status == "processing":
                            processing_count += 1

                total_tasks = len(task_ids)
                finished_tasks = completed_count + failed_count
                progress = round((finished_tasks / total_tasks) * 100, 1) if total_tasks > 0 else 0

                # Determine batch status
                if finished_tasks == total_tasks:
                    if failed_count == 0:
                        batch_status = "completed"
                        message = f"æ‰¹æ¬¡å®Œæˆ: {completed_count}/{total_tasks} æˆåŠŸ"
                    elif completed_count == 0:
                        batch_status = "failed"
                        message = f"æ‰¹æ¬¡å¤±è´¥: {failed_count}/{total_tasks} å¤±è´¥"
                    else:
                        batch_status = "partial"
                        message = f"æ‰¹æ¬¡éƒ¨åˆ†æˆåŠŸ: {completed_count} æˆåŠŸ, {failed_count} å¤±è´¥"
                elif processing_count > 0 or finished_tasks < total_tasks:
                    batch_status = "processing"
                    message = f"æ‰¹æ¬¡å¤„ç†ä¸­: {finished_tasks}/{total_tasks} å·²å®Œæˆ, {processing_count} å¤„ç†ä¸­"
                else:
                    batch_status = "queued"
                    message = f"æ‰¹æ¬¡æ’é˜Ÿä¸­: {total_tasks} ä»»åŠ¡å¾…å¤„ç†"

                progress_data = {
                    "batch_id": batch_id,
                    "status": batch_status,
                    "message": message,
                    "progress": progress,
                    "total_tasks": total_tasks,
                    "completed": completed_count,
                    "failed": failed_count,
                    "processing": processing_count,
                    "timestamp": asyncio.get_event_loop().time()
                }

                yield f"event: progress\ndata: {json.dumps(progress_data, ensure_ascii=False)}\n\n"

                # Break if batch is finished
                if batch_status in ["completed", "failed", "partial"]:
                    yield f"event: finished\ndata: {{\"batch_id\": \"{batch_id}\", \"final_status\": \"{batch_status}\"}}\n\n"
                    break

                # Wait before next update
                await asyncio.sleep(batch_poll_interval)
                idle_elapsed += batch_poll_interval

            except Exception as e:
                logger.exception(f"Batch progress error: {e}")
                yield f"event: error\ndata: {{\"error\": \"è·å–æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}\"}}\n\n"
                break

    except Exception as e:
        logger.exception(f"SSE batch error for {batch_id}: {e}")
        yield f"event: error\ndata: {{\"error\": \"è¿æ¥å¼‚å¸¸: {str(e)}\"}}\n\n"


@router.get("/tasks/{task_id}")
async def stream_task_progress(task_id: str, user: dict = Depends(get_current_user), svc: QueueService = Depends(get_queue_service)):
    """Stream real-time progress updates for a specific task"""
    # Verify task exists and belongs to user
    task_data = await svc.get_task(task_id)
    if not task_data or task_data.get("user") != user["id"]:
        raise HTTPException(status_code=404, detail="Task not found")

    return StreamingResponse(
        task_progress_generator(task_id, user["id"]),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get("/batches/{batch_id}")
async def stream_batch_progress(batch_id: str, user: dict = Depends(get_current_user), svc: QueueService = Depends(get_queue_service)):
    """Stream real-time progress updates for a batch"""
    # Verify batch exists and belongs to user
    batch_data = await svc.get_batch(batch_id)
    if not batch_data or batch_data.get("user") != user["id"]:
        raise HTTPException(status_code=404, detail="Batch not found")

    return StreamingResponse(
        batch_progress_generator(batch_id, user["id"]),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )