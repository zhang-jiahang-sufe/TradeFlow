"""
è‚¡ç¥¨åˆ†ææœåŠ¡
å°†ç°æœ‰çš„TradingAgentsåˆ†æåŠŸèƒ½åŒ…è£…æˆAPIæœåŠ¡
"""

import asyncio
import uuid
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆå§‹åŒ–TradingAgentsæ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import init_logging
init_logging()

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from app.services.simple_analysis_service import create_analysis_config, get_provider_by_model_name
from app.models.analysis import (
    AnalysisParameters, AnalysisResult, AnalysisTask, AnalysisBatch,
    AnalysisStatus, BatchStatus, SingleAnalysisRequest, BatchAnalysisRequest
)
from app.models.user import PyObjectId
from bson import ObjectId
from app.core.database import get_mongo_db
from app.core.redis_client import get_redis_service, RedisKeys
from app.services.queue_service import QueueService
from app.core.database import get_redis_client
from app.services.redis_progress_tracker import RedisProgressTracker
from app.services.config_provider import provider as config_provider
from app.services.queue import DEFAULT_USER_CONCURRENT_LIMIT, GLOBAL_CONCURRENT_LIMIT, VISIBILITY_TIMEOUT_SECONDS
from app.services.usage_statistics_service import UsageStatisticsService
from app.models.config import UsageRecord

import logging
logger = logging.getLogger(__name__)


class AnalysisService:
    """è‚¡ç¥¨åˆ†ææœåŠ¡ç±»"""

    def __init__(self):
        # è·å–Rediså®¢æˆ·ç«¯
        redis_client = get_redis_client()
        self.queue_service = QueueService(redis_client)
        # åˆå§‹åŒ–ä½¿ç”¨ç»Ÿè®¡æœåŠ¡
        self.usage_service = UsageStatisticsService()
        self._trading_graph_cache = {}
        # è¿›åº¦è·Ÿè¸ªå™¨ç¼“å­˜
        self._progress_trackers: Dict[str, RedisProgressTracker] = {}

    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # å¦‚æœæ˜¯adminç”¨æˆ·ï¼Œä½¿ç”¨å›ºå®šçš„ObjectId
            if user_id == "admin":
                # ä½¿ç”¨å›ºå®šçš„ObjectIdä½œä¸ºadminç”¨æˆ·ID
                admin_object_id = ObjectId("507f1f77bcf86cd799439011")
                logger.info(f"ğŸ”„ è½¬æ¢adminç”¨æˆ·ID: {user_id} -> {admin_object_id}")
                return PyObjectId(admin_object_id)
            else:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºObjectId
                object_id = ObjectId(user_id)
                logger.info(f"ğŸ”„ è½¬æ¢ç”¨æˆ·ID: {user_id} -> {object_id}")
                return PyObjectId(object_id)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·IDè½¬æ¢å¤±è´¥: {user_id} -> {e}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ObjectId
            new_object_id = ObjectId()
            logger.warning(f"âš ï¸ ç”Ÿæˆæ–°çš„ç”¨æˆ·ID: {new_object_id}")
            return PyObjectId(new_object_id)
    
    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså›¾å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰- ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´"""
        config_key = json.dumps(config, sort_keys=True)

        if config_key not in self._trading_graph_cache:
            # ç›´æ¥ä½¿ç”¨å®Œæ•´é…ç½®ï¼Œä¸å†åˆå¹¶DEFAULT_CONFIGï¼ˆå› ä¸ºcreate_analysis_configå·²ç»å¤„ç†äº†ï¼‰
            # è¿™ä¸å•è‚¡åˆ†ææœåŠ¡å’Œwebç›®å½•çš„æ–¹å¼ä¸€è‡´
            self._trading_graph_cache[config_key] = TradingAgentsGraph(
                selected_analysts=config.get("selected_analysts", ["market", "fundamentals"]),
                debug=config.get("debug", False),
                config=config
            )

            logger.info(f"åˆ›å»ºæ–°çš„TradingAgentså®ä¾‹: {config.get('llm_provider', 'default')}")

        return self._trading_graph_cache[config_key]

    def _execute_analysis_sync_with_progress(self, task: AnalysisTask, progress_tracker: RedisProgressTracker) -> AnalysisResult:
        """åŒæ­¥æ‰§è¡Œåˆ†æä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œå¸¦è¿›åº¦è·Ÿè¸ªï¼‰"""
        try:
            # åœ¨çº¿ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
            from tradingagents.utils.logging_init import init_logging, get_logger
            init_logging()
            thread_logger = get_logger('analysis_thread')

            thread_logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # ç¯å¢ƒæ£€æŸ¥
            progress_tracker.update_progress("ğŸ”§ æ£€æŸ¥ç¯å¢ƒé…ç½®")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # ğŸ”§ ä» MongoDB æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°ï¼ˆè€Œä¸æ˜¯ä» JSON æ–‡ä»¶ï¼‰
            quick_model_config = None
            deep_model_config = None

            try:
                from pymongo import MongoClient
                from app.core.config import settings

                # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯
                client = MongoClient(settings.MONGO_URI)
                db = client[settings.MONGO_DB]
                collection = db.system_configs

                # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
                doc = collection.find_one({"is_active": True}, sort=[("version", -1)])

                if doc and "llm_configs" in doc:
                    llm_configs = doc["llm_configs"]
                    logger.info(f"âœ… ä» MongoDB è¯»å–åˆ° {len(llm_configs)} ä¸ªæ¨¡å‹é…ç½®")

                    for llm_config in llm_configs:
                        if llm_config.get("model_name") == quick_model:
                            quick_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base")
                            }
                            logger.info(f"âœ… è¯»å–å¿«é€Ÿæ¨¡å‹é…ç½®: {quick_model}")
                            logger.info(f"   max_tokens={quick_model_config['max_tokens']}, temperature={quick_model_config['temperature']}")
                            logger.info(f"   timeout={quick_model_config['timeout']}, retry_times={quick_model_config['retry_times']}")
                            logger.info(f"   api_base={quick_model_config['api_base']}")

                        if llm_config.get("model_name") == deep_model:
                            deep_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base")
                            }
                            logger.info(f"âœ… è¯»å–æ·±åº¦æ¨¡å‹é…ç½®: {deep_model} - {deep_model_config}")
                else:
                    logger.warning("âš ï¸ MongoDB ä¸­æ²¡æœ‰æ‰¾åˆ°ç³»ç»Ÿé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
            except Exception as e:
                logger.warning(f"âš ï¸ ä» MongoDB è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")

            # æˆæœ¬ä¼°ç®—
            progress_tracker.update_progress("ğŸ’° é¢„ä¼°åˆ†ææˆæœ¬")

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            llm_provider = "dashscope"  # é»˜è®¤ä½¿ç”¨dashscope

            # å‚æ•°é…ç½®
            progress_tracker.update_progress("âš™ï¸ é…ç½®åˆ†æå‚æ•°")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.services.simple_analysis_service import create_analysis_config
            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, 'market_type', "Aè‚¡"),
                quick_model_config=quick_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
                deep_model_config=deep_model_config     # ä¼ é€’æ¨¡å‹é…ç½®
            )

            # å¯åŠ¨å¼•æ“
            progress_tracker.update_progress("ğŸš€ åˆå§‹åŒ–AIåˆ†æå¼•æ“")

            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)

            # æ‰§è¡Œåˆ†æ
            from datetime import timezone
            start_time = datetime.now(timezone.utc)
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")

            # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(message: str):
                progress_tracker.update_progress(message)

            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä¼ é€’è¿›åº¦å›è°ƒï¼‰
            _, decision = trading_graph.propagate(task.symbol, analysis_date, progress_callback)

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()

            # ç”ŸæˆæŠ¥å‘Š
            progress_tracker.update_progress("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š")

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = decision.get('model_info', 'Unknown') if isinstance(decision, dict) else 'Unknown'

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0),
                model_info=model_info  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
            )

            logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’")
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            raise

    def _execute_analysis_sync(self, task: AnalysisTask) -> AnalysisResult:
        """åŒæ­¥æ‰§è¡Œåˆ†æä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        try:
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # ğŸ”§ ä» MongoDB æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°ï¼ˆè€Œä¸æ˜¯ä» JSON æ–‡ä»¶ï¼‰
            quick_model_config = None
            deep_model_config = None

            try:
                from pymongo import MongoClient
                from app.core.config import settings

                # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯
                client = MongoClient(settings.MONGO_URI)
                db = client[settings.MONGO_DB]
                collection = db.system_configs

                # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
                doc = collection.find_one({"is_active": True}, sort=[("version", -1)])

                if doc and "llm_configs" in doc:
                    llm_configs = doc["llm_configs"]
                    logger.info(f"âœ… ä» MongoDB è¯»å–åˆ° {len(llm_configs)} ä¸ªæ¨¡å‹é…ç½®")

                    for llm_config in llm_configs:
                        if llm_config.get("model_name") == quick_model:
                            quick_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base")
                            }
                            logger.info(f"âœ… è¯»å–å¿«é€Ÿæ¨¡å‹é…ç½®: {quick_model}")
                            logger.info(f"   max_tokens={quick_model_config['max_tokens']}, temperature={quick_model_config['temperature']}")
                            logger.info(f"   timeout={quick_model_config['timeout']}, retry_times={quick_model_config['retry_times']}")
                            logger.info(f"   api_base={quick_model_config['api_base']}")

                        if llm_config.get("model_name") == deep_model:
                            deep_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base")
                            }
                            logger.info(f"âœ… è¯»å–æ·±åº¦æ¨¡å‹é…ç½®: {deep_model} - {deep_model_config}")
                else:
                    logger.warning("âš ï¸ MongoDB ä¸­æ²¡æœ‰æ‰¾åˆ°ç³»ç»Ÿé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
            except Exception as e:
                logger.warning(f"âš ï¸ ä» MongoDB è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            llm_provider = "dashscope"  # é»˜è®¤ä½¿ç”¨dashscope

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.services.simple_analysis_service import create_analysis_config
            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, 'market_type', "Aè‚¡"),
                quick_model_config=quick_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
                deep_model_config=deep_model_config     # ä¼ é€’æ¨¡å‹é…ç½®
            )

            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)

            # æ‰§è¡Œåˆ†æ
            from datetime import timezone
            start_time = datetime.now(timezone.utc)
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")

            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
            _, decision = trading_graph.propagate(task.symbol, analysis_date)

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = decision.get('model_info', 'Unknown') if isinstance(decision, dict) else 'Unknown'

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0),
                model_info=model_info  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
            )

            logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’")
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            raise

    async def _execute_single_analysis_async(self, task: AnalysisTask):
        """å¼‚æ­¥æ‰§è¡Œå•è‚¡åˆ†æä»»åŠ¡ï¼ˆåœ¨åå°è¿è¡Œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
        progress_tracker = None
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            progress_tracker = RedisProgressTracker(
                task_id=task.task_id,
                analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                research_depth=task.parameters.research_depth or "æ ‡å‡†",
                llm_provider="dashscope"
            )

            # ç¼“å­˜è¿›åº¦è·Ÿè¸ªå™¨
            self._progress_trackers[task.task_id] = progress_tracker

            # åˆå§‹åŒ–è¿›åº¦
            progress_tracker.update_progress("ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ")
            await self._update_task_status_with_tracker(task.task_id, AnalysisStatus.PROCESSING, progress_tracker)

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œåˆ†æï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            import asyncio
            import concurrent.futures

            loop = asyncio.get_event_loop()

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨è¿è¡ŒåŒæ­¥çš„åˆ†æä»£ç 
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    self._execute_analysis_sync_with_progress,
                    task,
                    progress_tracker
                )

            # æ ‡è®°å®Œæˆ
            progress_tracker.mark_completed("âœ… åˆ†æå®Œæˆ")
            await self._update_task_status_with_tracker(task.task_id, AnalysisStatus.COMPLETED, progress_tracker, result)

            # è®°å½• token ä½¿ç”¨
            try:
                # è·å–ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
                quick_model = getattr(task.parameters, 'quick_analysis_model', None)
                deep_model = getattr(task.parameters, 'deep_analysis_model', None)

                # ä¼˜å…ˆä½¿ç”¨æ·±åº¦åˆ†ææ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å‹
                model_name = deep_model or quick_model or "qwen-plus"

                # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šä¾›åº”å•†
                from app.services.simple_analysis_service import get_provider_by_model_name
                provider = get_provider_by_model_name(model_name)

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                await self._record_token_usage(task, result, provider, model_name)
            except Exception as e:
                logger.error(f"âš ï¸  è®°å½• token ä½¿ç”¨å¤±è´¥: {e}")

            logger.info(f"âœ… åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id}")

        except Exception as e:
            logger.error(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")

            # æ ‡è®°å¤±è´¥
            if progress_tracker:
                progress_tracker.mark_failed(str(e))
                await self._update_task_status_with_tracker(task.task_id, AnalysisStatus.FAILED, progress_tracker)
            else:
                await self._update_task_status(task.task_id, AnalysisStatus.FAILED, 0, str(e))
        finally:
            # æ¸…ç†è¿›åº¦è·Ÿè¸ªå™¨ç¼“å­˜
            if task.task_id in self._progress_trackers:
                del self._progress_trackers[task.task_id]

    async def submit_single_analysis(
        self,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤å•è‚¡åˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹æäº¤å•è‚¡åˆ†æä»»åŠ¡")
            logger.info(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # è·å–è‚¡ç¥¨ä»£ç  (å…¼å®¹æ—§å­—æ®µ)
            stock_symbol = request.get_symbol()
            logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç : {stock_symbol}")
            logger.info(f"âš™ï¸ åˆ†æå‚æ•°: {request.parameters}")

            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())
            logger.info(f"ğŸ†” ç”Ÿæˆä»»åŠ¡ID: {task_id}")

            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)
            logger.info(f"ğŸ”„ è½¬æ¢åçš„ç”¨æˆ·ID: {converted_user_id} (ç±»å‹: {type(converted_user_id)})")

            # åˆ›å»ºåˆ†æä»»åŠ¡
            logger.info(f"ğŸ—ï¸ å¼€å§‹åˆ›å»ºAnalysisTaskå¯¹è±¡...")

            # è¯»å–åˆå¹¶åçš„ç³»ç»Ÿè®¾ç½®ï¼ˆENV ä¼˜å…ˆ â†’ DBï¼‰ï¼Œç”¨äºå¡«å……æ¨¡å‹ä¸å¹¶å‘/è¶…æ—¶é…ç½®
            try:
                effective_settings = await config_provider.get_effective_system_settings()
            except Exception:
                effective_settings = {}

            # å¡«å……åˆ†æå‚æ•°ä¸­çš„æ¨¡å‹ï¼ˆè‹¥è¯·æ±‚æœªæ˜¾å¼æä¾›ï¼‰
            params = request.parameters or AnalysisParameters()
            if not getattr(params, 'quick_analysis_model', None):
                params.quick_analysis_model = effective_settings.get("quick_analysis_model", "qwen-turbo")
            if not getattr(params, 'deep_analysis_model', None):
                params.deep_analysis_model = effective_settings.get("deep_analysis_model", "qwen-max")

            # åº”ç”¨ç³»ç»Ÿçº§å¹¶å‘ä¸å¯è§æ€§è¶…æ—¶ï¼ˆè‹¥æä¾›ï¼‰
            try:
                self.queue_service.user_concurrent_limit = int(effective_settings.get("max_concurrent_tasks", DEFAULT_USER_CONCURRENT_LIMIT))
                self.queue_service.global_concurrent_limit = int(effective_settings.get("max_concurrent_tasks", GLOBAL_CONCURRENT_LIMIT))
                self.queue_service.visibility_timeout = int(effective_settings.get("default_analysis_timeout", VISIBILITY_TIMEOUT_SECONDS))
            except Exception:
                # ä½¿ç”¨é»˜è®¤å€¼å³å¯
                pass

            task = AnalysisTask(
                task_id=task_id,
                user_id=converted_user_id,
                symbol=stock_symbol,
                stock_code=stock_symbol,  # å…¼å®¹å­—æ®µ
                parameters=params,
                status=AnalysisStatus.PENDING
            )
            logger.info(f"âœ… AnalysisTaskå¯¹è±¡åˆ›å»ºæˆåŠŸ")

            # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“...")
            db = get_mongo_db()
            task_dict = task.model_dump(by_alias=True)
            logger.info(f"ğŸ“„ ä»»åŠ¡å­—å…¸: {task_dict}")
            await db.analysis_tasks.insert_one(task_dict)
            logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“")

            # å•è‚¡åˆ†æï¼šç›´æ¥åœ¨åå°æ‰§è¡Œï¼ˆä¸é˜»å¡APIå“åº”ï¼‰
            logger.info(f"ğŸš€ å¼€å§‹åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡...")

            # åˆ›å»ºåå°ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
            import asyncio
            background_task = asyncio.create_task(
                self._execute_single_analysis_async(task)
            )

            # ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
            logger.info(f"âœ… åå°ä»»åŠ¡å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}")

            logger.info(f"ğŸ‰ å•è‚¡åˆ†æä»»åŠ¡æäº¤å®Œæˆ: {task_id} - {stock_symbol}")

            return {
                "task_id": task_id,
                "symbol": stock_symbol,
                "stock_code": stock_symbol,  # å…¼å®¹å­—æ®µ
                "status": AnalysisStatus.PENDING,
                "message": "ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
            }
            
        except Exception as e:
            logger.error(f"æäº¤å•è‚¡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def submit_batch_analysis(
        self, 
        user_id: str, 
        request: BatchAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤æ‰¹é‡åˆ†æä»»åŠ¡"""
        try:
            # ç”Ÿæˆæ‰¹æ¬¡ID
            batch_id = str(uuid.uuid4())
            
            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)

            # è¯»å–ç³»ç»Ÿè®¾ç½®ï¼Œå¡«å……æ¨¡å‹å‚æ•°å¹¶åº”ç”¨å¹¶å‘/è¶…æ—¶é…ç½®
            try:
                effective_settings = await config_provider.get_effective_system_settings()
            except Exception:
                effective_settings = {}

            params = request.parameters or AnalysisParameters()
            if not getattr(params, 'quick_analysis_model', None):
                params.quick_analysis_model = effective_settings.get("quick_analysis_model", "qwen-turbo")
            if not getattr(params, 'deep_analysis_model', None):
                params.deep_analysis_model = effective_settings.get("deep_analysis_model", "qwen-max")

            try:
                self.queue_service.user_concurrent_limit = int(effective_settings.get("max_concurrent_tasks", DEFAULT_USER_CONCURRENT_LIMIT))
                self.queue_service.global_concurrent_limit = int(effective_settings.get("max_concurrent_tasks", GLOBAL_CONCURRENT_LIMIT))
                self.queue_service.visibility_timeout = int(effective_settings.get("default_analysis_timeout", VISIBILITY_TIMEOUT_SECONDS))
            except Exception:
                pass

            # åˆ›å»ºæ‰¹æ¬¡è®°å½•
            # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ (å…¼å®¹æ—§å­—æ®µ)
            stock_symbols = request.get_symbols()

            batch = AnalysisBatch(
                batch_id=batch_id,
                user_id=converted_user_id,
                title=request.title,
                description=request.description,
                total_tasks=len(stock_symbols),
                parameters=params,
                status=BatchStatus.PENDING
            )

            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            for symbol in stock_symbols:
                task_id = str(uuid.uuid4())
                task = AnalysisTask(
                    task_id=task_id,
                    batch_id=batch_id,
                    user_id=converted_user_id,
                    symbol=symbol,
                    stock_code=symbol,  # å…¼å®¹å­—æ®µ
                    parameters=batch.parameters,
                    status=AnalysisStatus.PENDING
                )
                tasks.append(task)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            db = get_mongo_db()
            await db.analysis_batches.insert_one(batch.dict(by_alias=True))
            await db.analysis_tasks.insert_many([task.dict(by_alias=True) for task in tasks])
            
            # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
            for task in tasks:
                # å‡†å¤‡é˜Ÿåˆ—å‚æ•°ï¼ˆç›´æ¥ä¼ é€’åˆ†æå‚æ•°ï¼Œä¸åµŒå¥—ï¼‰
                queue_params = task.parameters.dict() if task.parameters else {}

                # æ·»åŠ ä»»åŠ¡å…ƒæ•°æ®
                queue_params.update({
                    "task_id": task.task_id,
                    "symbol": task.symbol,
                    "stock_code": task.symbol,  # å…¼å®¹å­—æ®µ
                    "user_id": str(task.user_id),
                    "batch_id": task.batch_id,
                    "created_at": task.created_at.isoformat() if task.created_at else None
                })

                # è°ƒç”¨é˜Ÿåˆ—æœåŠ¡
                await self.queue_service.enqueue_task(
                    user_id=str(converted_user_id),
                    symbol=task.symbol,
                    params=queue_params,
                    batch_id=task.batch_id
                )
            
            logger.info(f"æ‰¹é‡åˆ†æä»»åŠ¡å·²æäº¤: {batch_id} - {len(tasks)}ä¸ªè‚¡ç¥¨")
            
            return {
                "batch_id": batch_id,
                "total_tasks": len(tasks),
                "status": BatchStatus.PENDING,
                "message": f"å·²æäº¤{len(tasks)}ä¸ªåˆ†æä»»åŠ¡åˆ°é˜Ÿåˆ—"
            }
            
        except Exception as e:
            logger.error(f"æäº¤æ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def execute_analysis_task(
        self, 
        task: AnalysisTask,
        progress_callback: Optional[Callable[[int, str], None]] = None
    ) -> AnalysisResult:
        """æ‰§è¡Œå•ä¸ªåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 0)
            
            if progress_callback:
                progress_callback(10, "åˆå§‹åŒ–åˆ†æå¼•æ“...")
            
            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½® - ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´
            from app.core.unified_config import unified_config

            quick_model = getattr(task.parameters, 'quick_analysis_model', None) or unified_config.get_quick_analysis_model()
            deep_model = getattr(task.parameters, 'deep_analysis_model', None) or unified_config.get_deep_analysis_model()

            # ğŸ”§ ä»æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°
            quick_model_config = None
            deep_model_config = None
            llm_configs = unified_config.get_llm_configs()

            for llm_config in llm_configs:
                if llm_config.model_name == quick_model:
                    quick_model_config = {
                        "max_tokens": llm_config.max_tokens,
                        "temperature": llm_config.temperature,
                        "timeout": llm_config.timeout,
                        "retry_times": llm_config.retry_times,
                        "api_base": llm_config.api_base
                    }

                if llm_config.model_name == deep_model:
                    deep_model_config = {
                        "max_tokens": llm_config.max_tokens,
                        "temperature": llm_config.temperature,
                        "timeout": llm_config.timeout,
                        "retry_times": llm_config.retry_times,
                        "api_base": llm_config.api_base
                    }

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†
            llm_provider = await get_provider_by_model_name(quick_model)

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, 'market_type', "Aè‚¡"),
                quick_model_config=quick_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
                deep_model_config=deep_model_config     # ä¼ é€’æ¨¡å‹é…ç½®
            )
            
            if progress_callback:
                progress_callback(30, "åˆ›å»ºåˆ†æå›¾...")
            
            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)
            
            if progress_callback:
                progress_callback(50, "æ‰§è¡Œè‚¡ç¥¨åˆ†æ...")
            
            # æ‰§è¡Œåˆ†æ
            start_time = datetime.utcnow()
            analysis_date = task.parameters.analysis_date or datetime.now().strftime("%Y-%m-%d")
            
            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•
            _, decision = trading_graph.propagate(task.symbol, analysis_date)
            
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            
            if progress_callback:
                progress_callback(80, "å¤„ç†åˆ†æç»“æœ...")

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = decision.get('model_info', 'Unknown') if isinstance(decision, dict) else 'Unknown'

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0),
                model_info=model_info  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
            )

            if progress_callback:
                progress_callback(100, "åˆ†æå®Œæˆ")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task.task_id, AnalysisStatus.COMPLETED, 100, result)

            # è®°å½• token ä½¿ç”¨
            try:
                # è®°å½•ä½¿ç”¨æƒ…å†µ
                await self._record_token_usage(task, result, llm_provider, deep_model or quick_model)
            except Exception as e:
                logger.error(f"âš ï¸  è®°å½• token ä½¿ç”¨å¤±è´¥: {e}")

            logger.info(f"åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’")

            return result
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            error_result = AnalysisResult(error_message=str(e))
            await self._update_task_status(task.task_id, AnalysisStatus.FAILED, 0, error_result)
            
            raise
    
    async def _update_task_status(
        self,
        task_id: str,
        status: AnalysisStatus,
        progress: int,
        result: Optional[AnalysisResult] = None,
    ) -> None:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆå§”æ‰˜è‡³æ‹†åˆ†çš„å·¥å…·å‡½æ•°ï¼‰"""
        try:
            from app.services.analysis.status_update_utils import perform_update_task_status
            await perform_update_task_status(task_id, status, progress, result)
        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")

    async def _update_task_status_with_tracker(
        self,
        task_id: str,
        status: AnalysisStatus,
        progress_tracker: RedisProgressTracker,
        result: Optional[AnalysisResult] = None,
    ) -> None:
        """ä½¿ç”¨è¿›åº¦è·Ÿè¸ªå™¨æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆå§”æ‰˜è‡³æ‹†åˆ†çš„å·¥å…·å‡½æ•°ï¼‰"""
        try:
            from app.services.analysis.status_update_utils import perform_update_task_status_with_tracker
            await perform_update_task_status_with_tracker(task_id, status, progress_tracker, result)
        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            # å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„è¿›åº¦è·Ÿè¸ªå™¨
            if task_id in self._progress_trackers:
                progress_tracker = self._progress_trackers[task_id]
                progress_data = progress_tracker.to_dict()

                # ä»æ•°æ®åº“è·å–ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
                db = get_mongo_db()
                task = await db.analysis_tasks.find_one({"task_id": task_id})

                if task:
                    # åˆå¹¶æ•°æ®åº“ä¿¡æ¯å’Œè¿›åº¦è·Ÿè¸ªå™¨ä¿¡æ¯
                    return {
                        "task_id": task_id,
                        "user_id": task.get("user_id"),
                        "symbol": task.get("stock_symbol") or task.get("symbol"),
                        "stock_code": task.get("stock_symbol") or task.get("symbol"),  # å…¼å®¹å­—æ®µ
                        "status": progress_data["status"],
                        "progress": progress_data["progress"],
                        "current_step": progress_data["current_step"],
                        "message": progress_data["message"],
                        "elapsed_time": progress_data["elapsed_time"],
                        "remaining_time": progress_data["remaining_time"],
                        "estimated_total_time": progress_data.get("estimated_total_time", 0),
                        "steps": progress_data["steps"],
                        "start_time": progress_data["start_time"],
                        "end_time": None,
                        "last_update": progress_data["last_update"],
                        "parameters": task.get("parameters", {}),
                        "execution_time": None,
                        "tokens_used": None,
                        "result_data": task.get("result"),
                        "error_message": None
                    }

            # ä»Redisç¼“å­˜è·å–
            redis_service = get_redis_service()
            progress_key = RedisKeys.TASK_PROGRESS.format(task_id=task_id)
            cached_status = await redis_service.get_json(progress_key)

            if cached_status:
                return cached_status

            # ä»æ•°æ®åº“è·å–
            db = get_mongo_db()
            task = await db.analysis_tasks.find_one({"task_id": task_id})

            if task:
                # è®¡ç®—å·²ç”¨æ—¶é—´
                elapsed_time = 0
                remaining_time = 0
                estimated_total_time = 0

                if task.get("started_at"):
                    from datetime import datetime
                    start_time = task.get("started_at")
                    if task.get("completed_at"):
                        # ä»»åŠ¡å·²å®Œæˆ
                        elapsed_time = (task.get("completed_at") - start_time).total_seconds()
                        estimated_total_time = elapsed_time  # å·²å®Œæˆä»»åŠ¡çš„æ€»æ—¶é•¿å°±æ˜¯å·²ç”¨æ—¶é—´
                        remaining_time = 0
                    else:
                        # ä»»åŠ¡è¿›è¡Œä¸­
                        elapsed_time = (datetime.utcnow() - start_time).total_seconds()

                        # ä½¿ç”¨ä»»åŠ¡çš„é¢„ä¼°æ—¶é•¿ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼ˆ5åˆ†é’Ÿï¼‰
                        estimated_total_time = task.get("estimated_duration", 300)

                        # é¢„è®¡å‰©ä½™ = é¢„ä¼°æ€»æ—¶é•¿ - å·²ç”¨æ—¶é—´
                        remaining_time = max(0, estimated_total_time - elapsed_time)

                return {
                    "task_id": task_id,
                    "status": task.get("status"),
                    "progress": task.get("progress", 0),
                    "current_step": task.get("current_step", ""),
                    "message": task.get("message", ""),
                    "elapsed_time": elapsed_time,
                    "remaining_time": remaining_time,
                    "estimated_total_time": estimated_total_time,
                    "start_time": task.get("started_at").isoformat() if task.get("started_at") else None,
                    "updated_at": task.get("updated_at", "").isoformat() if task.get("updated_at") else None,
                    "result_data": task.get("result")
                }

            return None

        except Exception as e:
            logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")
            return None
    
    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task_id, AnalysisStatus.CANCELLED, 0)
            
            # ä»é˜Ÿåˆ—ä¸­ç§»é™¤ï¼ˆå¦‚æœè¿˜åœ¨é˜Ÿåˆ—ä¸­ï¼‰
            await self.queue_service.remove_task(task_id)
            
            logger.info(f"ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            return False

    async def _record_token_usage(
        self,
        task: AnalysisTask,
        result: AnalysisResult,
        provider: str,
        model_name: str
    ):
        """è®°å½• token ä½¿ç”¨æƒ…å†µ"""
        try:
            # ä»ç»“æœä¸­æå– token ä½¿ç”¨ä¿¡æ¯
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä» LLM å“åº”ä¸­è·å–å®é™…çš„ token ä½¿ç”¨é‡
            # ç›®å‰ä½¿ç”¨ä¼°ç®—å€¼
            input_tokens = result.tokens_used // 2 if result.tokens_used > 0 else 0
            output_tokens = result.tokens_used - input_tokens if result.tokens_used > 0 else 0

            # å¦‚æœæ²¡æœ‰ token ä½¿ç”¨ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
            if result.tokens_used == 0:
                # æ ¹æ®åˆ†æç±»å‹ä¼°ç®—
                input_tokens = 2000  # é»˜è®¤è¾“å…¥ token
                output_tokens = 1000  # é»˜è®¤è¾“å‡º token

            # è·å–æ¨¡å‹ä»·æ ¼é…ç½®
            from app.services.config_service import config_service
            config = await config_service.get_system_config()

            # æŸ¥æ‰¾å¯¹åº”çš„ LLM é…ç½®
            llm_config = None
            if config and config.llm_configs:
                for cfg in config.llm_configs:
                    if cfg.provider == provider and cfg.model_name == model_name:
                        llm_config = cfg
                        break

            # è®¡ç®—æˆæœ¬
            cost = 0.0
            currency = "CNY"  # é»˜è®¤è´§å¸å•ä½
            if llm_config:
                input_price = llm_config.input_price_per_1k or 0.0
                output_price = llm_config.output_price_per_1k or 0.0
                cost = (input_tokens / 1000 * input_price) + (output_tokens / 1000 * output_price)
                currency = llm_config.currency or "CNY"

            # åˆ›å»ºä½¿ç”¨è®°å½•
            usage_record = UsageRecord(
                timestamp=datetime.now().isoformat(),
                provider=provider,
                model_name=model_name,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                cost=cost,
                currency=currency,
                session_id=task.task_id,
                analysis_type="stock_analysis",
                stock_code=task.symbol
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            success = await self.usage_service.add_usage_record(usage_record)

            if success:
                logger.info(f"ğŸ’° è®°å½•ä½¿ç”¨æˆæœ¬: {provider}/{model_name} - Â¥{cost:.4f}")
            else:
                logger.warning(f"âš ï¸  è®°å½•ä½¿ç”¨æˆæœ¬å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ è®°å½• token ä½¿ç”¨å¤±è´¥: {e}")


# å…¨å±€åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
analysis_service: Optional[AnalysisService] = None


def get_analysis_service() -> AnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global analysis_service
    if analysis_service is None:
        analysis_service = AnalysisService()
    return analysis_service
