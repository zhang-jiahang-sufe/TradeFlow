"""
Multi-source stock basics synchronization service
- Supports multiple data sources with fallback mechanism
- Priority: Tushare > AKShare > BaoStock 
- Fetches A-share stock basic info with extended financial metrics
- Upserts into MongoDB collection `stock_basic_info`
- Provides unified interface for different data sources
"""
from __future__ import annotations

import asyncio
import logging
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum

from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne

from app.core.database import get_mongo_db
from app.services.basics_sync import add_financial_metrics as _add_financial_metrics_util


logger = logging.getLogger(__name__)

# Collection names
COLLECTION_NAME = "stock_basic_info"
STATUS_COLLECTION = "sync_status"
JOB_KEY = "stock_basics_multi_source"


class DataSourcePriority(Enum):
    """æ•°æ®æºä¼˜å…ˆçº§æšä¸¾"""
    TUSHARE = 1
    AKSHARE = 2
    BAOSTOCK = 3


@dataclass
class SyncStats:
    """åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
    job: str = JOB_KEY
    data_type: str = "stock_basics"  # æ·»åŠ data_typeå­—æ®µä»¥ç¬¦åˆæ•°æ®åº“ç´¢å¼•è¦æ±‚
    status: str = "idle"
    started_at: Optional[str] = None
    finished_at: Optional[str] = None
    total: int = 0
    inserted: int = 0
    updated: int = 0
    errors: int = 0
    last_trade_date: Optional[str] = None
    data_sources_used: List[str] = field(default_factory=list)
    source_stats: Dict[str, Dict[str, int]] = field(default_factory=dict)
    message: Optional[str] = None


class MultiSourceBasicsSyncService:
    """å¤šæ•°æ®æºè‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥æœåŠ¡"""

    def __init__(self):
        self._lock = asyncio.Lock()
        self._running = False
        self._last_status: Optional[Dict[str, Any]] = None

    async def get_status(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥çŠ¶æ€"""
        if self._last_status:
            return self._last_status

        db = get_mongo_db()
        doc = await db[STATUS_COLLECTION].find_one({"job": JOB_KEY})
        if doc:
            # ç§»é™¤MongoDBçš„_idå­—æ®µä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
            doc.pop("_id", None)
            return doc
        return {"job": JOB_KEY, "status": "never_run"}

    async def _persist_status(self, db: AsyncIOMotorDatabase, stats: Dict[str, Any]) -> None:
        """æŒä¹…åŒ–åŒæ­¥çŠ¶æ€"""
        stats["job"] = JOB_KEY

        # ä½¿ç”¨ upsert æ¥é¿å…é‡å¤é”®é”™è¯¯
        # åŸºäº data_type å’Œ job è¿›è¡Œæ›´æ–°æˆ–æ’å…¥
        filter_query = {
            "data_type": stats.get("data_type", "stock_basics"),
            "job": JOB_KEY
        }

        await db[STATUS_COLLECTION].update_one(
            filter_query,
            {"$set": stats},
            upsert=True
        )

        self._last_status = {k: v for k, v in stats.items() if k != "_id"}

    async def _execute_bulk_write_with_retry(
        self,
        db: AsyncIOMotorDatabase,
        operations: List,
        max_retries: int = 3
    ) -> Tuple[int, int]:
        """
        æ‰§è¡Œæ‰¹é‡å†™å…¥ï¼Œå¸¦é‡è¯•æœºåˆ¶

        Args:
            db: MongoDBæ•°æ®åº“å®ä¾‹
            operations: æ‰¹é‡æ“ä½œåˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            (æ–°å¢æ•°é‡, æ›´æ–°æ•°é‡)
        """
        inserted = 0
        updated = 0
        retry_count = 0

        while retry_count < max_retries:
            try:
                result = await db[COLLECTION_NAME].bulk_write(operations, ordered=False)
                inserted = result.upserted_count
                updated = result.modified_count
                logger.debug(f"âœ… æ‰¹é‡å†™å…¥æˆåŠŸ: æ–°å¢ {inserted}, æ›´æ–° {updated}")
                return inserted, updated

            except asyncio.TimeoutError as e:
                retry_count += 1
                if retry_count < max_retries:
                    wait_time = 2 ** retry_count  # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€8ç§’
                    logger.warning(f"âš ï¸ æ‰¹é‡å†™å…¥è¶…æ—¶ (ç¬¬{retry_count}æ¬¡é‡è¯•)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    return 0, 0

            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                return 0, 0

        return inserted, updated

    async def run_full_sync(self, force: bool = False, preferred_sources: List[str] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åŒæ­¥

        Args:
            force: æ˜¯å¦å¼ºåˆ¶è¿è¡Œï¼ˆå³ä½¿å·²åœ¨è¿è¡Œä¸­ï¼‰
            preferred_sources: ä¼˜å…ˆä½¿ç”¨çš„æ•°æ®æºåˆ—è¡¨
        """
        async with self._lock:
            if self._running and not force:
                logger.info("Multi-source stock basics sync already running; skip start")
                return await self.get_status()
            self._running = True

        db = get_mongo_db()
        stats = SyncStats()
        stats.started_at = datetime.now().isoformat()
        stats.status = "running"
        await self._persist_status(db, stats.__dict__.copy())

        try:
            # Step 1: è·å–æ•°æ®æºç®¡ç†å™¨
            from app.services.data_sources.manager import DataSourceManager
            manager = DataSourceManager()
            available_adapters = manager.get_available_adapters()

            if not available_adapters:
                raise RuntimeError("No available data sources found")

            logger.info(f"Available data sources: {[adapter.name for adapter in available_adapters]}")

            # å¦‚æœæŒ‡å®šäº†ä¼˜å…ˆæ•°æ®æºï¼Œè®°å½•æ—¥å¿—
            if preferred_sources:
                logger.info(f"Using preferred data sources: {preferred_sources}")

            # Step 2: å°è¯•ä»æ•°æ®æºè·å–è‚¡ç¥¨åˆ—è¡¨
            stock_df, source_used = await asyncio.to_thread(
                manager.get_stock_list_with_fallback, preferred_sources
            )
            if stock_df is None or getattr(stock_df, "empty", True):
                raise RuntimeError("All data sources failed to provide stock list")

            stats.data_sources_used.append(f"stock_list:{source_used}")
            logger.info(f"Successfully fetched {len(stock_df)} stocks from {source_used}")

            # Step 3: è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸå’Œè´¢åŠ¡æ•°æ®
            latest_trade_date = await asyncio.to_thread(
                manager.find_latest_trade_date_with_fallback, preferred_sources
            )
            stats.last_trade_date = latest_trade_date

            daily_data_map = {}
            daily_source = ""
            if latest_trade_date:
                daily_df, daily_source = await asyncio.to_thread(
                    manager.get_daily_basic_with_fallback, latest_trade_date, preferred_sources
                )
                if daily_df is not None and not daily_df.empty:
                    for _, row in daily_df.iterrows():
                        ts_code = row.get("ts_code")
                        if ts_code:
                            daily_data_map[ts_code] = row.to_dict()
                    stats.data_sources_used.append(f"daily_data:{daily_source}")

            # Step 5: å¤„ç†å’Œæ›´æ–°æ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
            ops = []
            inserted = updated = errors = 0
            batch_size = 500  # ğŸ”¥ æ¯æ‰¹å¤„ç† 500 åªè‚¡ç¥¨ï¼Œé¿å…è¶…æ—¶
            total_stocks = len(stock_df)

            logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {total_stocks} åªè‚¡ç¥¨ï¼Œæ•°æ®æº: {source_used}")

            for idx, (_, row) in enumerate(stock_df.iterrows(), 1):
                try:
                    # æå–åŸºç¡€ä¿¡æ¯
                    name = row.get("name") or ""
                    area = row.get("area") or ""
                    industry = row.get("industry") or ""
                    market = row.get("market") or ""
                    list_date = row.get("list_date") or ""
                    ts_code = row.get("ts_code") or ""

                    # æå–6ä½è‚¡ç¥¨ä»£ç 
                    if isinstance(ts_code, str) and "." in ts_code:
                        code = ts_code.split(".")[0]
                    else:
                        symbol = row.get("symbol") or ""
                        code = str(symbol).zfill(6) if symbol else ""

                    # æ ¹æ® ts_code åˆ¤æ–­äº¤æ˜“æ‰€
                    if isinstance(ts_code, str):
                        if ts_code.endswith(".SH"):
                            sse = "ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€"
                        elif ts_code.endswith(".SZ"):
                            sse = "æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€"
                        elif ts_code.endswith(".BJ"):
                            sse = "åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€"
                        else:
                            sse = "æœªçŸ¥"
                    else:
                        sse = "æœªçŸ¥"

                    category = "stock_cn"

                    # è·å–è´¢åŠ¡æ•°æ®
                    daily_metrics = {}
                    if isinstance(ts_code, str) and ts_code in daily_data_map:
                        daily_metrics = daily_data_map[ts_code]

                    # ç”Ÿæˆ full_symbolï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
                    full_symbol = ts_code if ts_code else self._generate_full_symbol(code)

                    # ğŸ”¥ ç¡®å®šæ•°æ®æºæ ‡è¯†
                    # æ ¹æ®å®é™…ä½¿ç”¨çš„æ•°æ®æºè®¾ç½® source å­—æ®µ
                    # æ³¨æ„ï¼šä¸å†ä½¿ç”¨ "multi_source" ä½œä¸ºé»˜è®¤å€¼ï¼Œå¿…é¡»æœ‰æ˜ç¡®çš„æ•°æ®æº
                    if not source_used:
                        logger.warning(f"âš ï¸ è‚¡ç¥¨ {code} æ²¡æœ‰æ˜ç¡®çš„æ•°æ®æºï¼Œè·³è¿‡")
                        errors += 1
                        continue
                    data_source = source_used

                    # æ„å»ºæ–‡æ¡£
                    doc = {
                        "code": code,
                        "symbol": code,  # æ·»åŠ  symbol å­—æ®µï¼ˆæ ‡å‡†åŒ–å­—æ®µï¼‰
                        "name": name,
                        "area": area,
                        "industry": industry,
                        "market": market,
                        "list_date": list_date,
                        "sse": sse,
                        "full_symbol": full_symbol,  # æ·»åŠ  full_symbol å­—æ®µ
                        "category": category,
                        "source": data_source,  # ğŸ”¥ ä½¿ç”¨å®é™…æ•°æ®æº
                        "updated_at": datetime.now(),
                    }

                    # æ·»åŠ è´¢åŠ¡æŒ‡æ ‡
                    self._add_financial_metrics(doc, daily_metrics)

                    # ğŸ”¥ ä½¿ç”¨ (code, source) è”åˆæŸ¥è¯¢æ¡ä»¶
                    ops.append(UpdateOne({"code": code, "source": data_source}, {"$set": doc}, upsert=True))

                except Exception as e:
                    logger.error(f"Error processing stock {row.get('ts_code', 'unknown')}: {e}")
                    errors += 1

                # ğŸ”¥ åˆ†æ‰¹æ‰§è¡Œæ•°æ®åº“æ“ä½œ
                if len(ops) >= batch_size or idx == total_stocks:
                    if ops:
                        progress_pct = (idx / total_stocks) * 100
                        logger.info(f"ğŸ“ æ‰§è¡Œæ‰¹é‡å†™å…¥: {len(ops)} æ¡è®°å½• ({idx}/{total_stocks}, {progress_pct:.1f}%)")

                        batch_inserted, batch_updated = await self._execute_bulk_write_with_retry(db, ops)

                        if batch_inserted > 0 or batch_updated > 0:
                            inserted += batch_inserted
                            updated += batch_updated
                            logger.info(f"âœ… æ‰¹é‡å†™å…¥å®Œæˆ: æ–°å¢ {batch_inserted}, æ›´æ–° {batch_updated} | ç´¯è®¡: æ–°å¢ {inserted}, æ›´æ–° {updated}, é”™è¯¯ {errors}")
                        else:
                            errors += len(ops)
                            logger.warning(f"âš ï¸ æ‰¹é‡å†™å…¥å¤±è´¥ï¼Œæ ‡è®° {len(ops)} æ¡è®°å½•ä¸ºé”™è¯¯")

                        ops = []  # æ¸…ç©ºæ“ä½œåˆ—è¡¨

            # Step 7: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats.total = total_stocks  # ğŸ”¥ ä½¿ç”¨æ€»è‚¡ç¥¨æ•°
            stats.inserted = inserted
            stats.updated = updated
            stats.errors = errors
            stats.status = "success" if errors == 0 else "success_with_errors"
            stats.finished_at = datetime.now().isoformat()

            await self._persist_status(db, stats.__dict__.copy())
            logger.info(
                f"âœ… Multi-source sync finished: total={stats.total} inserted={inserted} "
                f"updated={updated} errors={errors} sources={stats.data_sources_used}"
            )
            return stats.__dict__

        except Exception as e:
            stats.status = "failed"
            stats.message = str(e)
            stats.finished_at = datetime.now().isoformat()
            await self._persist_status(db, stats.__dict__.copy())
            logger.exception(f"Multi-source sync failed: {e}")
            return stats.__dict__
        finally:
            async with self._lock:
                self._running = False



    def _add_financial_metrics(self, doc: Dict, daily_metrics: Dict) -> None:
        """å§”æ‰˜åˆ° basics_sync.processing.add_financial_metrics"""
        return _add_financial_metrics_util(doc, daily_metrics)

    def _generate_full_symbol(self, code: str) -> str:
        """
        æ ¹æ®è‚¡ç¥¨ä»£ç ç”Ÿæˆå®Œæ•´æ ‡å‡†åŒ–ä»£ç 

        Args:
            code: 6ä½è‚¡ç¥¨ä»£ç 

        Returns:
            å®Œæ•´æ ‡å‡†åŒ–ä»£ç ï¼Œå¦‚æœæ— æ³•è¯†åˆ«åˆ™è¿”å›åŸå§‹ä»£ç ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
        """
        # ç¡®ä¿ code ä¸ä¸ºç©º
        if not code:
            return ""

        # æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºæ ¼
        code = str(code).strip()

        # å¦‚æœé•¿åº¦ä¸æ˜¯ 6ï¼Œè¿”å›åŸå§‹ä»£ç 
        if len(code) != 6:
            return code

        # æ ¹æ®ä»£ç å‰ç¼€åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith(('60', '68', '90')):  # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
            return f"{code}.SS"
        elif code.startswith(('00', '30', '20')):  # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
            return f"{code}.SZ"
        elif code.startswith(('8', '4')):  # åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€
            return f"{code}.BJ"
        else:
            # æ— æ³•è¯†åˆ«çš„ä»£ç ï¼Œè¿”å›åŸå§‹ä»£ç ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
            return code if code else ""


# å…¨å±€æœåŠ¡å®ä¾‹
_multi_source_sync_service = None

def get_multi_source_sync_service() -> MultiSourceBasicsSyncService:
    """è·å–å¤šæ•°æ®æºåŒæ­¥æœåŠ¡å®ä¾‹"""
    global _multi_source_sync_service
    if _multi_source_sync_service is None:
        _multi_source_sync_service = MultiSourceBasicsSyncService()
    return _multi_source_sync_service
