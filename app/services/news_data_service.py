"""
æ–°é—»æ•°æ®æœåŠ¡
æä¾›ç»Ÿä¸€çš„æ–°é—»æ•°æ®å­˜å‚¨ã€æŸ¥è¯¢å’Œç®¡ç†åŠŸèƒ½
"""
from typing import Optional, List, Dict, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
import logging
from pymongo import ReplaceOne
from pymongo.errors import BulkWriteError
from bson import ObjectId

from app.core.database import get_database

logger = logging.getLogger(__name__)


def convert_objectid_to_str(data: Union[Dict, List[Dict]]) -> Union[Dict, List[Dict]]:
    """
    è½¬æ¢ MongoDB ObjectId ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… JSON åºåˆ—åŒ–é”™è¯¯

    Args:
        data: å•ä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£åˆ—è¡¨

    Returns:
        è½¬æ¢åçš„æ•°æ®
    """
    if isinstance(data, list):
        for item in data:
            if isinstance(item, dict) and '_id' in item:
                item['_id'] = str(item['_id'])
        return data
    elif isinstance(data, dict):
        if '_id' in data:
            data['_id'] = str(data['_id'])
        return data
    return data


@dataclass
class NewsQueryParams:
    """æ–°é—»æŸ¥è¯¢å‚æ•°"""
    symbol: Optional[str] = None
    symbols: Optional[List[str]] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    category: Optional[str] = None
    sentiment: Optional[str] = None
    importance: Optional[str] = None
    data_source: Optional[str] = None
    keywords: Optional[List[str]] = None
    limit: int = 50
    skip: int = 0
    sort_by: str = "publish_time"
    sort_order: int = -1  # -1 for desc, 1 for asc


@dataclass
class NewsStats:
    """æ–°é—»ç»Ÿè®¡ä¿¡æ¯"""
    total_count: int = 0
    positive_count: int = 0
    negative_count: int = 0
    neutral_count: int = 0
    high_importance_count: int = 0
    medium_importance_count: int = 0
    low_importance_count: int = 0
    categories: Dict[str, int] = None
    sources: Dict[str, int] = None
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = {}
        if self.sources is None:
            self.sources = {}


class NewsDataService:
    """æ–°é—»æ•°æ®æœåŠ¡"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._db = None
        self._collection = None
        self._indexes_ensured = False

    async def _ensure_indexes(self):
        """ç¡®ä¿å¿…è¦çš„ç´¢å¼•å­˜åœ¨"""
        if self._indexes_ensured:
            return

        try:
            collection = self._get_collection()
            self.logger.info("ğŸ“Š æ£€æŸ¥å¹¶åˆ›å»ºæ–°é—»æ•°æ®ç´¢å¼•...")

            # 1. å”¯ä¸€ç´¢å¼•ï¼šé˜²æ­¢é‡å¤æ–°é—»ï¼ˆURL+æ ‡é¢˜+å‘å¸ƒæ—¶é—´ï¼‰
            await collection.create_index([
                ("url", 1),
                ("title", 1),
                ("publish_time", 1)
            ], unique=True, name="url_title_time_unique", background=True)

            # 2. è‚¡ç¥¨ä»£ç ç´¢å¼•ï¼ˆæŸ¥è¯¢å•åªè‚¡ç¥¨çš„æ–°é—»ï¼‰
            await collection.create_index([("symbol", 1)], name="symbol_index", background=True)

            # 3. å¤šè‚¡ç¥¨ä»£ç ç´¢å¼•ï¼ˆæŸ¥è¯¢æ¶‰åŠå¤šåªè‚¡ç¥¨çš„æ–°é—»ï¼‰
            await collection.create_index([("symbols", 1)], name="symbols_index", background=True)

            # 4. å‘å¸ƒæ—¶é—´ç´¢å¼•ï¼ˆæŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
            await collection.create_index([("publish_time", -1)], name="publish_time_desc", background=True)

            # 5. å¤åˆç´¢å¼•ï¼šè‚¡ç¥¨ä»£ç +å‘å¸ƒæ—¶é—´ï¼ˆå¸¸ç”¨æŸ¥è¯¢ï¼‰
            await collection.create_index([
                ("symbol", 1),
                ("publish_time", -1)
            ], name="symbol_time_index", background=True)

            # 6. æ•°æ®æºç´¢å¼•ï¼ˆæŒ‰æ•°æ®æºç­›é€‰ï¼‰
            await collection.create_index([("data_source", 1)], name="data_source_index", background=True)

            # 7. åˆ†ç±»ç´¢å¼•ï¼ˆæŒ‰æ–°é—»ç±»åˆ«ç­›é€‰ï¼‰
            await collection.create_index([("category", 1)], name="category_index", background=True)

            # 8. æƒ…æ„Ÿç´¢å¼•ï¼ˆæŒ‰æƒ…æ„Ÿç­›é€‰ï¼‰
            await collection.create_index([("sentiment", 1)], name="sentiment_index", background=True)

            # 9. é‡è¦æ€§ç´¢å¼•ï¼ˆæŒ‰é‡è¦æ€§ç­›é€‰ï¼‰
            await collection.create_index([("importance", 1)], name="importance_index", background=True)

            # 10. æ›´æ–°æ—¶é—´ç´¢å¼•ï¼ˆæ•°æ®ç»´æŠ¤ï¼‰
            await collection.create_index([("updated_at", -1)], name="updated_at_index", background=True)

            self._indexes_ensured = True
            self.logger.info("âœ… æ–°é—»æ•°æ®ç´¢å¼•æ£€æŸ¥å®Œæˆ")
        except Exception as e:
            # ç´¢å¼•åˆ›å»ºå¤±è´¥ä¸åº”è¯¥é˜»æ­¢æœåŠ¡å¯åŠ¨
            self.logger.warning(f"âš ï¸ åˆ›å»ºç´¢å¼•æ—¶å‡ºç°è­¦å‘Šï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")

    def _get_collection(self):
        """è·å–æ–°é—»æ•°æ®é›†åˆ"""
        if self._collection is None:
            self._db = get_database()
            self._collection = self._db.stock_news
        return self._collection
    
    async def save_news_data(
        self,
        news_data: Union[Dict[str, Any], List[Dict[str, Any]]],
        data_source: str,
        market: str = "CN"
    ) -> int:
        """
        ä¿å­˜æ–°é—»æ•°æ®

        Args:
            news_data: æ–°é—»æ•°æ®ï¼ˆå•æ¡æˆ–å¤šæ¡ï¼‰
            data_source: æ•°æ®æºæ ‡è¯†
            market: å¸‚åœºæ ‡è¯†

        Returns:
            ä¿å­˜çš„è®°å½•æ•°é‡
        """
        try:
            # ğŸ”¥ ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºï¼‰
            await self._ensure_indexes()

            collection = self._get_collection()
            now = datetime.utcnow()
            
            # æ ‡å‡†åŒ–æ•°æ®
            if isinstance(news_data, dict):
                news_list = [news_data]
            else:
                news_list = news_data
            
            if not news_list:
                return 0
            
            # å‡†å¤‡æ‰¹é‡æ“ä½œ
            operations = []

            for i, news in enumerate(news_list):
                # æ ‡å‡†åŒ–æ–°é—»æ•°æ®
                standardized_news = self._standardize_news_data(
                    news, data_source, market, now
                )

                # ğŸ” è®°å½•å‰3æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
                if i < 3:
                    self.logger.info(f"   ğŸ“ æ ‡å‡†åŒ–åçš„æ–°é—» {i+1}:")
                    self.logger.info(f"      symbol: {standardized_news.get('symbol')}")
                    self.logger.info(f"      title: {standardized_news.get('title', '')[:50]}...")
                    self.logger.info(f"      publish_time: {standardized_news.get('publish_time')} (type: {type(standardized_news.get('publish_time'))})")
                    self.logger.info(f"      url: {standardized_news.get('url', '')[:80]}...")

                # ä½¿ç”¨URLã€æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´ä½œä¸ºå”¯ä¸€æ ‡è¯†
                filter_query = {
                    "url": standardized_news["url"],
                    "title": standardized_news["title"],
                    "publish_time": standardized_news["publish_time"]
                }

                operations.append(
                    ReplaceOne(
                        filter_query,
                        standardized_news,
                        upsert=True
                    )
                )
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œ
            if operations:
                result = await collection.bulk_write(operations)
                saved_count = result.upserted_count + result.modified_count
                
                self.logger.info(f"ğŸ’¾ æ–°é—»æ•°æ®ä¿å­˜å®Œæˆ: {saved_count}æ¡è®°å½• (æ•°æ®æº: {data_source})")
                return saved_count
            
            return 0
            
        except BulkWriteError as e:
            # å¤„ç†æ‰¹é‡å†™å…¥é”™è¯¯ï¼Œä½†ä¸å®Œå…¨å¤±è´¥
            write_errors = e.details.get('writeErrors', [])
            error_count = len(write_errors)
            self.logger.warning(f"âš ï¸ éƒ¨åˆ†æ–°é—»æ•°æ®ä¿å­˜å¤±è´¥: {error_count}æ¡é”™è¯¯")

            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            for i, error in enumerate(write_errors[:3], 1):  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                error_msg = error.get('errmsg', 'Unknown error')
                error_code = error.get('code', 'N/A')
                self.logger.warning(f"   é”™è¯¯ {i}: [Code {error_code}] {error_msg}")

            # è®¡ç®—æˆåŠŸä¿å­˜çš„æ•°é‡
            success_count = len(operations) - error_count
            if success_count > 0:
                self.logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {success_count} æ¡æ–°é—»æ•°æ®")

            return success_count
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return 0

    def save_news_data_sync(
        self,
        news_data: Union[Dict[str, Any], List[Dict[str, Any]]],
        data_source: str,
        market: str = "CN"
    ) -> int:
        """
        ä¿å­˜æ–°é—»æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        ç”¨äºéå¼‚æ­¥ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŒæ­¥çš„ PyMongo å®¢æˆ·ç«¯

        Args:
            news_data: æ–°é—»æ•°æ®ï¼ˆå•æ¡æˆ–å¤šæ¡ï¼‰
            data_source: æ•°æ®æºæ ‡è¯†
            market: å¸‚åœºæ ‡è¯†

        Returns:
            ä¿å­˜çš„è®°å½•æ•°é‡
        """
        try:
            from app.core.database import get_mongo_db_sync

            # è·å–åŒæ­¥æ•°æ®åº“è¿æ¥
            db = get_mongo_db_sync()
            collection = db.stock_news
            now = datetime.utcnow()

            # æ ‡å‡†åŒ–æ•°æ®
            if isinstance(news_data, dict):
                news_list = [news_data]
            else:
                news_list = news_data

            if not news_list:
                return 0

            # å‡†å¤‡æ‰¹é‡æ“ä½œ
            operations = []

            self.logger.info(f"ğŸ“ å¼€å§‹æ ‡å‡†åŒ– {len(news_list)} æ¡æ–°é—»æ•°æ®...")

            for i, news in enumerate(news_list, 1):
                # æ ‡å‡†åŒ–æ–°é—»æ•°æ®
                standardized_news = self._standardize_news_data(news, data_source, market, now)

                # è®°å½•å‰3æ¡æ–°é—»çš„è¯¦ç»†ä¿¡æ¯
                if i <= 3:
                    self.logger.info(f"   ğŸ“ æ ‡å‡†åŒ–åçš„æ–°é—» {i}:")
                    self.logger.info(f"      symbol: {standardized_news.get('symbol')}")
                    self.logger.info(f"      title: {standardized_news.get('title', '')[:50]}...")
                    publish_time = standardized_news.get('publish_time')
                    self.logger.info(f"      publish_time: {publish_time} (type: {type(publish_time)})")
                    self.logger.info(f"      url: {standardized_news.get('url', '')[:60]}...")

                # ä½¿ç”¨URL+æ ‡é¢˜+å‘å¸ƒæ—¶é—´ä½œä¸ºå”¯ä¸€æ ‡è¯†
                filter_query = {
                    "url": standardized_news.get("url"),
                    "title": standardized_news.get("title"),
                    "publish_time": standardized_news.get("publish_time")
                }

                operations.append(
                    ReplaceOne(
                        filter_query,
                        standardized_news,
                        upsert=True
                    )
                )

            # æ‰§è¡Œæ‰¹é‡æ“ä½œï¼ˆåŒæ­¥æ–¹å¼ï¼‰
            if operations:
                result = collection.bulk_write(operations)
                saved_count = result.upserted_count + result.modified_count

                self.logger.info(f"ğŸ’¾ æ–°é—»æ•°æ®ä¿å­˜å®Œæˆ: {saved_count}æ¡è®°å½• (æ•°æ®æº: {data_source})")
                return saved_count

            return 0

        except BulkWriteError as e:
            # å¤„ç†æ‰¹é‡å†™å…¥é”™è¯¯ï¼Œä½†ä¸å®Œå…¨å¤±è´¥
            write_errors = e.details.get('writeErrors', [])
            error_count = len(write_errors)
            self.logger.warning(f"âš ï¸ éƒ¨åˆ†æ–°é—»æ•°æ®ä¿å­˜å¤±è´¥: {error_count}æ¡é”™è¯¯")

            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            for i, error in enumerate(write_errors[:3], 1):  # åªè®°å½•å‰3ä¸ªé”™è¯¯
                error_msg = error.get('errmsg', 'Unknown error')
                error_code = error.get('code', 'N/A')
                self.logger.warning(f"   é”™è¯¯ {i}: [Code {error_code}] {error_msg}")

            # è®¡ç®—æˆåŠŸä¿å­˜çš„æ•°é‡
            success_count = len(operations) - error_count
            if success_count > 0:
                self.logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {success_count} æ¡æ–°é—»æ•°æ®")

            return success_count

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ–°é—»æ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return 0

    def _standardize_news_data(
        self,
        news_data: Dict[str, Any],
        data_source: str,
        market: str,
        now: datetime
    ) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æ–°é—»æ•°æ®"""
        
        # æå–åŸºç¡€ä¿¡æ¯
        symbol = news_data.get("symbol")
        symbols = news_data.get("symbols", [])
        
        # å¦‚æœæœ‰ä¸»è¦è‚¡ç¥¨ä»£ç ä½†symbolsä¸ºç©ºï¼Œæ·»åŠ åˆ°symbolsä¸­
        if symbol and symbol not in symbols:
            symbols = [symbol] + symbols
        
        # æ ‡å‡†åŒ–æ•°æ®ç»“æ„
        standardized = {
            # åŸºç¡€ä¿¡æ¯
            "symbol": symbol,
            "full_symbol": self._get_full_symbol(symbol, market) if symbol else None,
            "market": market,
            "symbols": symbols,
            
            # æ–°é—»å†…å®¹
            "title": news_data.get("title", ""),
            "content": news_data.get("content", ""),
            "summary": news_data.get("summary", ""),
            "url": news_data.get("url", ""),
            "source": news_data.get("source", ""),
            "author": news_data.get("author", ""),
            
            # æ—¶é—´ä¿¡æ¯
            "publish_time": self._parse_datetime(news_data.get("publish_time")),
            
            # åˆ†ç±»å’Œæ ‡ç­¾
            "category": news_data.get("category", "general"),
            "sentiment": news_data.get("sentiment", "neutral"),
            "sentiment_score": self._safe_float(news_data.get("sentiment_score")),
            "keywords": news_data.get("keywords", []),
            "importance": news_data.get("importance", "medium"),
            # æ³¨æ„ï¼šä¸åŒ…å« language å­—æ®µï¼Œé¿å…ä¸ MongoDB æ–‡æœ¬ç´¢å¼•å†²çª

            # å…ƒæ•°æ®
            "data_source": data_source,
            "created_at": now,
            "updated_at": now,
            "version": 1
        }
        
        return standardized
    
    def _get_full_symbol(self, symbol: str, market: str) -> str:
        """è·å–å®Œæ•´è‚¡ç¥¨ä»£ç """
        if not symbol:
            return None
        
        if market == "CN":
            if len(symbol) == 6:
                if symbol.startswith(('60', '68')):
                    return f"{symbol}.SH"
                elif symbol.startswith(('00', '30')):
                    return f"{symbol}.SZ"
        
        return symbol
    
    def _parse_datetime(self, dt_value) -> Optional[datetime]:
        """è§£ææ—¥æœŸæ—¶é—´"""
        if dt_value is None:
            return None
        
        if isinstance(dt_value, datetime):
            return dt_value
        
        if isinstance(dt_value, str):
            try:
                # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                formats = [
                    "%Y-%m-%d %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S",
                    "%Y-%m-%dT%H:%M:%SZ",
                    "%Y-%m-%d",
                ]
                
                for fmt in formats:
                    try:
                        return datetime.strptime(dt_value, fmt)
                    except ValueError:
                        continue
                
                # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›å½“å‰æ—¶é—´
                self.logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸæ—¶é—´: {dt_value}")
                return datetime.utcnow()
                
            except Exception:
                return datetime.utcnow()
        
        return datetime.utcnow()
    
    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if value is None:
            return None
        
        try:
            return float(value)
        except (ValueError, TypeError):
            return None
    
    async def query_news(self, params: NewsQueryParams) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢æ–°é—»æ•°æ®
        
        Args:
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æ–°é—»æ•°æ®åˆ—è¡¨
        """
        try:
            collection = self._get_collection()

            self.logger.info(f"ğŸ” [query_news] å¼€å§‹æŸ¥è¯¢æ–°é—»æ•°æ®")
            self.logger.info(f"   å‚æ•°: symbol={params.symbol}, start_time={params.start_time}, end_time={params.end_time}, limit={params.limit}")

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {}

            if params.symbol:
                query["symbol"] = params.symbol
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: symbol={params.symbol}")

            if params.symbols:
                query["symbols"] = {"$in": params.symbols}
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: symbols in {params.symbols}")

            if params.start_time or params.end_time:
                time_query = {}
                if params.start_time:
                    time_query["$gte"] = params.start_time
                if params.end_time:
                    time_query["$lte"] = params.end_time
                query["publish_time"] = time_query
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: publish_time between {params.start_time} and {params.end_time}")

            if params.category:
                query["category"] = params.category
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: category={params.category}")

            if params.sentiment:
                query["sentiment"] = params.sentiment
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: sentiment={params.sentiment}")

            if params.importance:
                query["importance"] = params.importance
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: importance={params.importance}")

            if params.data_source:
                query["data_source"] = params.data_source
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: data_source={params.data_source}")

            if params.keywords:
                # æ–‡æœ¬æœç´¢
                query["$text"] = {"$search": " ".join(params.keywords)}
                self.logger.info(f"   æ·»åŠ æŸ¥è¯¢æ¡ä»¶: text search={params.keywords}")

            self.logger.info(f"   æœ€ç»ˆæŸ¥è¯¢æ¡ä»¶: {query}")

            # å…ˆç»Ÿè®¡æ€»æ•°
            total_count = await collection.count_documents(query)
            self.logger.info(f"   æ•°æ®åº“ä¸­ç¬¦åˆæ¡ä»¶çš„æ€»è®°å½•æ•°: {total_count}")

            # æ‰§è¡ŒæŸ¥è¯¢
            cursor = collection.find(query)

            # æ’åº
            cursor = cursor.sort(params.sort_by, params.sort_order)
            self.logger.info(f"   æ’åº: {params.sort_by} ({params.sort_order})")

            # åˆ†é¡µ
            cursor = cursor.skip(params.skip).limit(params.limit)
            self.logger.info(f"   åˆ†é¡µ: skip={params.skip}, limit={params.limit}")

            # è·å–ç»“æœ
            results = await cursor.to_list(length=None)
            self.logger.info(f"   æŸ¥è¯¢è¿”å›: {len(results)} æ¡è®°å½•")

            # ğŸ”§ è½¬æ¢ ObjectId ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… JSON åºåˆ—åŒ–é”™è¯¯
            results = convert_objectid_to_str(results)

            if results:
                self.logger.info(f"   å‰3æ¡é¢„è§ˆ:")
                for i, r in enumerate(results[:3], 1):
                    self.logger.info(f"      {i}. symbol={r.get('symbol')}, title={r.get('title', 'N/A')[:50]}..., publish_time={r.get('publish_time')}")
            else:
                self.logger.warning(f"   âš ï¸ æŸ¥è¯¢ç»“æœä¸ºç©º")

            self.logger.info(f"âœ… [query_news] æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(results)} æ¡è®°å½•")
            return results

        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢æ–°é—»æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return []
    
    async def get_latest_news(
        self,
        symbol: str = None,
        limit: int = 10,
        hours_back: int = 24
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€æ–°æ–°é—»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œä¸ºç©ºåˆ™è·å–æ‰€æœ‰æ–°é—»
            limit: è¿”å›æ•°é‡é™åˆ¶
            hours_back: å›æº¯å°æ—¶æ•°
            
        Returns:
            æœ€æ–°æ–°é—»åˆ—è¡¨
        """
        start_time = datetime.utcnow() - timedelta(hours=hours_back)
        
        params = NewsQueryParams(
            symbol=symbol,
            start_time=start_time,
            limit=limit,
            sort_by="publish_time",
            sort_order=-1
        )
        
        return await self.query_news(params)
    
    async def get_news_statistics(
        self,
        symbol: str = None,
        start_time: datetime = None,
        end_time: datetime = None
    ) -> NewsStats:
        """
        è·å–æ–°é—»ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            
        Returns:
            æ–°é—»ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            collection = self._get_collection()
            
            # æ„å»ºåŒ¹é…æ¡ä»¶
            match_stage = {}
            
            if symbol:
                match_stage["symbol"] = symbol
            
            if start_time or end_time:
                time_query = {}
                if start_time:
                    time_query["$gte"] = start_time
                if end_time:
                    time_query["$lte"] = end_time
                match_stage["publish_time"] = time_query
            
            # èšåˆç®¡é“
            pipeline = []
            
            if match_stage:
                pipeline.append({"$match": match_stage})
            
            pipeline.extend([
                {
                    "$group": {
                        "_id": None,
                        "total_count": {"$sum": 1},
                        "positive_count": {
                            "$sum": {"$cond": [{"$eq": ["$sentiment", "positive"]}, 1, 0]}
                        },
                        "negative_count": {
                            "$sum": {"$cond": [{"$eq": ["$sentiment", "negative"]}, 1, 0]}
                        },
                        "neutral_count": {
                            "$sum": {"$cond": [{"$eq": ["$sentiment", "neutral"]}, 1, 0]}
                        },
                        "high_importance_count": {
                            "$sum": {"$cond": [{"$eq": ["$importance", "high"]}, 1, 0]}
                        },
                        "medium_importance_count": {
                            "$sum": {"$cond": [{"$eq": ["$importance", "medium"]}, 1, 0]}
                        },
                        "low_importance_count": {
                            "$sum": {"$cond": [{"$eq": ["$importance", "low"]}, 1, 0]}
                        },
                        "categories": {"$push": "$category"},
                        "sources": {"$push": "$data_source"}
                    }
                }
            ])
            
            # æ‰§è¡Œèšåˆ
            result = await collection.aggregate(pipeline).to_list(length=1)
            
            if result:
                data = result[0]
                
                # ç»Ÿè®¡åˆ†ç±»å’Œæ¥æº
                categories = {}
                for cat in data.get("categories", []):
                    categories[cat] = categories.get(cat, 0) + 1
                
                sources = {}
                for src in data.get("sources", []):
                    sources[src] = sources.get(src, 0) + 1
                
                return NewsStats(
                    total_count=data.get("total_count", 0),
                    positive_count=data.get("positive_count", 0),
                    negative_count=data.get("negative_count", 0),
                    neutral_count=data.get("neutral_count", 0),
                    high_importance_count=data.get("high_importance_count", 0),
                    medium_importance_count=data.get("medium_importance_count", 0),
                    low_importance_count=data.get("low_importance_count", 0),
                    categories=categories,
                    sources=sources
                )
            
            return NewsStats()
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ–°é—»ç»Ÿè®¡å¤±è´¥: {e}")
            return NewsStats()
    
    async def delete_old_news(self, days_to_keep: int = 90) -> int:
        """
        åˆ é™¤è¿‡æœŸæ–°é—»
        
        Args:
            days_to_keep: ä¿ç•™å¤©æ•°
            
        Returns:
            åˆ é™¤çš„è®°å½•æ•°é‡
        """
        try:
            collection = self._get_collection()
            
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            result = await collection.delete_many({
                "publish_time": {"$lt": cutoff_date}
            })
            
            deleted_count = result.deleted_count
            self.logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ–°é—»: {deleted_count}æ¡è®°å½•")
            
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤è¿‡æœŸæ–°é—»å¤±è´¥: {e}")
            return 0

    async def search_news(
        self,
        query_text: str,
        symbol: str = None,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        å…¨æ–‡æœç´¢æ–°é—»

        Args:
            query_text: æœç´¢æ–‡æœ¬
            symbol: è‚¡ç¥¨ä»£ç è¿‡æ»¤
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            collection = self._get_collection()

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {"$text": {"$search": query_text}}

            if symbol:
                query["symbol"] = symbol

            # æ‰§è¡Œæœç´¢ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
            cursor = collection.find(
                query,
                {"score": {"$meta": "textScore"}}
            ).sort([("score", {"$meta": "textScore"})])

            cursor = cursor.limit(limit)
            results = await cursor.to_list(length=None)

            # ğŸ”§ è½¬æ¢ ObjectId ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… JSON åºåˆ—åŒ–é”™è¯¯
            results = convert_objectid_to_str(results)

            self.logger.info(f"ğŸ” å…¨æ–‡æœç´¢è¿”å› {len(results)} æ¡ç»“æœ")
            return results

        except Exception as e:
            self.logger.error(f"âŒ å…¨æ–‡æœç´¢å¤±è´¥: {e}")
            return []


# å…¨å±€æœåŠ¡å®ä¾‹
_service_instance = None

async def get_news_data_service() -> NewsDataService:
    """è·å–æ–°é—»æ•°æ®æœåŠ¡å®ä¾‹"""
    global _service_instance
    if _service_instance is None:
        _service_instance = NewsDataService()
        logger.info("âœ… æ–°é—»æ•°æ®æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    return _service_instance
