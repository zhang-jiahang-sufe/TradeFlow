"""
Backup, import, and export routines extracted from DatabaseService.
"""
from __future__ import annotations

import json
import os
import gzip
import asyncio
import subprocess
import shutil
from datetime import datetime
from typing import Any, Dict, List, Optional
import logging

from bson import ObjectId

from app.core.database import get_mongo_db
from app.core.config import settings
from .serialization import serialize_document

logger = logging.getLogger(__name__)


def _check_mongodump_available() -> bool:
    """æ£€æŸ¥ mongodump å‘½ä»¤æ˜¯å¦å¯ç”¨"""
    return shutil.which("mongodump") is not None


async def create_backup_native(name: str, backup_dir: str, collections: Optional[List[str]] = None, user_id: str | None = None) -> Dict[str, Any]:
    """
    ä½¿ç”¨ MongoDB åŸç”Ÿ mongodump å‘½ä»¤åˆ›å»ºå¤‡ä»½ï¼ˆæ¨èï¼Œé€Ÿåº¦å¿«ï¼‰

    ä¼˜åŠ¿ï¼š
    - é€Ÿåº¦å¿«ï¼ˆç›´æ¥æ“ä½œ BSONï¼Œä¸éœ€è¦ JSON è½¬æ¢ï¼‰
    - å‹ç¼©æ•ˆç‡é«˜
    - æ”¯æŒå¤§æ•°æ®é‡
    - å¹¶è¡Œå¤„ç†å¤šä¸ªé›†åˆ

    è¦æ±‚ï¼š
    - ç³»ç»Ÿä¸­éœ€è¦å®‰è£… MongoDB Database Tools
    - mongodump å‘½ä»¤åœ¨ PATH ä¸­å¯ç”¨
    """
    if not _check_mongodump_available():
        raise Exception("mongodump å‘½ä»¤ä¸å¯ç”¨ï¼Œè¯·å®‰è£… MongoDB Database Tools æˆ–ä½¿ç”¨ create_backup() æ–¹æ³•")

    db = get_mongo_db()

    backup_id = str(ObjectId())
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    backup_dirname = f"backup_{name}_{timestamp}"
    backup_path = os.path.join(backup_dir, backup_dirname)

    os.makedirs(backup_dir, exist_ok=True)

    # æ„å»º mongodump å‘½ä»¤
    cmd = [
        "mongodump",
        "--uri", settings.MONGO_URI,
        "--out", backup_path,
        "--gzip"  # å¯ç”¨å‹ç¼©
    ]

    # å¦‚æœæŒ‡å®šäº†é›†åˆï¼Œåªå¤‡ä»½è¿™äº›é›†åˆ
    if collections:
        for collection_name in collections:
            cmd.extend(["--collection", collection_name])

    logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ mongodump å¤‡ä»½: {name}")

    # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡çš„ subprocess è°ƒç”¨
    def _run_mongodump():
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        if result.returncode != 0:
            raise Exception(f"mongodump æ‰§è¡Œå¤±è´¥: {result.stderr}")
        return result

    try:
        await asyncio.to_thread(_run_mongodump)
        logger.info(f"âœ… mongodump å¤‡ä»½å®Œæˆ: {name}")
    except subprocess.TimeoutExpired:
        raise Exception("å¤‡ä»½è¶…æ—¶ï¼ˆè¶…è¿‡1å°æ—¶ï¼‰")
    except Exception as e:
        logger.error(f"âŒ mongodump å¤‡ä»½å¤±è´¥: {e}")
        # æ¸…ç†å¤±è´¥çš„å¤‡ä»½ç›®å½•
        if os.path.exists(backup_path):
            await asyncio.to_thread(shutil.rmtree, backup_path)
        raise

    # è®¡ç®—å¤‡ä»½å¤§å°
    def _get_dir_size(path):
        total = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                total += os.path.getsize(filepath)
        return total

    file_size = await asyncio.to_thread(_get_dir_size, backup_path)

    # è·å–å®é™…å¤‡ä»½çš„é›†åˆåˆ—è¡¨
    if not collections:
        collections = await db.list_collection_names()
        collections = [c for c in collections if not c.startswith("system.")]

    backup_meta = {
        "_id": ObjectId(backup_id),
        "name": name,
        "filename": backup_dirname,
        "file_path": backup_path,
        "size": file_size,
        "collections": collections,
        "created_at": datetime.utcnow(),
        "created_by": user_id,
        "backup_type": "mongodump",  # æ ‡è®°å¤‡ä»½ç±»å‹
    }

    await db.database_backups.insert_one(backup_meta)

    return {
        "id": backup_id,
        "name": name,
        "filename": backup_dirname,
        "file_path": backup_path,
        "size": file_size,
        "collections": collections,
        "created_at": backup_meta["created_at"].isoformat(),
        "backup_type": "mongodump",
    }


async def create_backup(name: str, backup_dir: str, collections: Optional[List[str]] = None, user_id: str | None = None) -> Dict[str, Any]:
    """
    åˆ›å»ºæ•°æ®åº“å¤‡ä»½ï¼ˆPython å®ç°ï¼Œå…¼å®¹æ€§å¥½ä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰

    å¯¹äºå¤§æ•°æ®é‡ï¼ˆ>100MBï¼‰ï¼Œå»ºè®®ä½¿ç”¨ create_backup_native() æ–¹æ³•
    """
    db = get_mongo_db()

    backup_id = str(ObjectId())
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"backup_{name}_{timestamp}.json.gz"
    backup_path = os.path.join(backup_dir, backup_filename)

    if not collections:
        collections = await db.list_collection_names()

    backup_data: Dict[str, Any] = {
        "backup_id": backup_id,
        "name": name,
        "created_at": datetime.utcnow().isoformat(),
        "created_by": user_id,
        "collections": collections,
        "data": {},
    }

    for collection_name in collections:
        collection = db[collection_name]
        documents: List[dict] = []
        async for doc in collection.find():
            documents.append(serialize_document(doc))
        backup_data["data"][collection_name] = documents

    os.makedirs(backup_dir, exist_ok=True)

    # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„æ–‡ä»¶ I/O æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
    def _write_backup():
        with gzip.open(backup_path, "wt", encoding="utf-8") as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        return os.path.getsize(backup_path)

    file_size = await asyncio.to_thread(_write_backup)

    backup_meta = {
        "_id": ObjectId(backup_id),
        "name": name,
        "filename": backup_filename,
        "file_path": backup_path,
        "size": file_size,
        "collections": collections,
        "created_at": datetime.utcnow(),
        "created_by": user_id,
    }

    await db.database_backups.insert_one(backup_meta)

    return {
        "id": backup_id,
        "name": name,
        "filename": backup_filename,
        "file_path": backup_path,
        "size": file_size,
        "collections": collections,
        "created_at": backup_meta["created_at"].isoformat(),
    }


async def list_backups() -> List[Dict[str, Any]]:
    db = get_mongo_db()
    backups: List[Dict[str, Any]] = []
    async for backup in db.database_backups.find().sort("created_at", -1):
        backups.append({
            "id": str(backup["_id"]),
            "name": backup["name"],
            "filename": backup["filename"],
            "size": backup["size"],
            "collections": backup["collections"],
            "created_at": backup["created_at"].isoformat(),
            "created_by": backup.get("created_by"),
        })
    return backups


async def delete_backup(backup_id: str) -> None:
    db = get_mongo_db()
    backup = await db.database_backups.find_one({"_id": ObjectId(backup_id)})
    if not backup:
        raise Exception("å¤‡ä»½ä¸å­˜åœ¨")
    if os.path.exists(backup["file_path"]):
        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„æ–‡ä»¶åˆ é™¤æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        backup_type = backup.get("backup_type", "python")
        if backup_type == "mongodump":
            # mongodump å¤‡ä»½æ˜¯ç›®å½•ï¼Œéœ€è¦é€’å½’åˆ é™¤
            await asyncio.to_thread(shutil.rmtree, backup["file_path"])
        else:
            # Python å¤‡ä»½æ˜¯å•ä¸ªæ–‡ä»¶
            await asyncio.to_thread(os.remove, backup["file_path"])
    await db.database_backups.delete_one({"_id": ObjectId(backup_id)})


def _convert_date_fields(doc: dict) -> dict:
    """
    è½¬æ¢æ–‡æ¡£ä¸­çš„æ—¥æœŸå­—æ®µï¼ˆå­—ç¬¦ä¸² -> datetimeï¼‰

    å¸¸è§çš„æ—¥æœŸå­—æ®µï¼š
    - created_at, updated_at, completed_at
    - started_at, finished_at
    - analysis_date (ä¿æŒå­—ç¬¦ä¸²æ ¼å¼ï¼Œå› ä¸ºæ˜¯æ—¥æœŸè€Œéæ—¶é—´æˆ³)
    """
    from dateutil import parser

    date_fields = [
        "created_at", "updated_at", "completed_at",
        "started_at", "finished_at", "deleted_at",
        "last_login", "last_modified", "timestamp"
    ]

    for field in date_fields:
        if field in doc and isinstance(doc[field], str):
            try:
                # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                doc[field] = parser.parse(doc[field])
                logger.debug(f"âœ… è½¬æ¢æ—¥æœŸå­—æ®µ {field}: {doc[field]}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸå­—æ®µ {field}: {doc[field]}, é”™è¯¯: {e}")

    return doc


async def import_data(content: bytes, collection: str, *, format: str = "json", overwrite: bool = False, filename: str | None = None) -> Dict[str, Any]:
    """
    å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“

    æ”¯æŒä¸¤ç§å¯¼å…¥æ¨¡å¼ï¼š
    1. å•é›†åˆæ¨¡å¼ï¼šå¯¼å…¥æ•°æ®åˆ°æŒ‡å®šé›†åˆ
    2. å¤šé›†åˆæ¨¡å¼ï¼šå¯¼å…¥åŒ…å«å¤šä¸ªé›†åˆçš„å¯¼å‡ºæ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
    """
    db = get_mongo_db()

    if format.lower() == "json":
        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„ JSON è§£ææ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        def _parse_json():
            return json.loads(content.decode("utf-8"))

        data = await asyncio.to_thread(_parse_json)
    else:
        raise Exception(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")

    # æ£€æµ‹æ˜¯å¦ä¸ºå¤šé›†åˆå¯¼å‡ºæ ¼å¼
    logger.info(f"ğŸ” [å¯¼å…¥æ£€æµ‹] æ•°æ®ç±»å‹: {type(data)}")

    # ğŸ”¥ æ–°æ ¼å¼ï¼šåŒ…å« export_info å’Œ data çš„å­—å…¸
    if isinstance(data, dict) and "export_info" in data and "data" in data:
        logger.info(f"ğŸ“¦ æ£€æµ‹åˆ°æ–°ç‰ˆå¤šé›†åˆå¯¼å‡ºæ–‡ä»¶ï¼ˆåŒ…å« export_infoï¼‰")
        export_info = data.get("export_info", {})
        logger.info(f"ğŸ“‹ å¯¼å‡ºä¿¡æ¯: åˆ›å»ºæ—¶é—´={export_info.get('created_at')}, é›†åˆæ•°={len(export_info.get('collections', []))}")

        # æå–å®é™…æ•°æ®
        data = data["data"]
        logger.info(f"ğŸ“¦ åŒ…å« {len(data)} ä¸ªé›†åˆ: {list(data.keys())}")

    # ğŸ”¥ æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯é›†åˆååˆ°æ–‡æ¡£åˆ—è¡¨çš„æ˜ å°„
    if isinstance(data, dict):
        logger.info(f"ğŸ” [å¯¼å…¥æ£€æµ‹] å­—å…¸åŒ…å« {len(data)} ä¸ªé”®")
        logger.info(f"ğŸ” [å¯¼å…¥æ£€æµ‹] é”®åˆ—è¡¨: {list(data.keys())[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª

        # æ£€æŸ¥æ¯ä¸ªé”®å€¼å¯¹çš„ç±»å‹
        for k, v in list(data.items())[:5]:  # åªæ£€æŸ¥å‰5ä¸ª
            logger.info(f"ğŸ” [å¯¼å…¥æ£€æµ‹] é”® '{k}': å€¼ç±»å‹={type(v)}, æ˜¯å¦ä¸ºåˆ—è¡¨={isinstance(v, list)}")
            if isinstance(v, list):
                logger.info(f"ğŸ” [å¯¼å…¥æ£€æµ‹] é”® '{k}': åˆ—è¡¨é•¿åº¦={len(v)}")

    if isinstance(data, dict) and all(isinstance(k, str) and isinstance(v, list) for k, v in data.items()):
        # å¤šé›†åˆæ¨¡å¼
        logger.info(f"ğŸ“¦ ç¡®è®¤ä¸ºå¤šé›†åˆå¯¼å…¥æ¨¡å¼ï¼ŒåŒ…å« {len(data)} ä¸ªé›†åˆ")

        total_inserted = 0
        imported_collections = []

        for coll_name, documents in data.items():
            if not documents:  # è·³è¿‡ç©ºé›†åˆ
                logger.info(f"â­ï¸ è·³è¿‡ç©ºé›†åˆ: {coll_name}")
                continue

            collection_obj = db[coll_name]

            if overwrite:
                deleted_count = await collection_obj.delete_many({})
                logger.info(f"ğŸ—‘ï¸ æ¸…ç©ºé›†åˆ {coll_name}ï¼šåˆ é™¤ {deleted_count.deleted_count} æ¡æ–‡æ¡£")

            # å¤„ç† _id å­—æ®µå’Œæ—¥æœŸå­—æ®µ
            for doc in documents:
                # è½¬æ¢ _id
                if "_id" in doc and isinstance(doc["_id"], str):
                    try:
                        doc["_id"] = ObjectId(doc["_id"])
                    except Exception:
                        del doc["_id"]

                # ğŸ”¥ è½¬æ¢æ—¥æœŸå­—æ®µï¼ˆå­—ç¬¦ä¸² -> datetimeï¼‰
                _convert_date_fields(doc)

            # æ’å…¥æ•°æ®
            if documents:
                res = await collection_obj.insert_many(documents)
                inserted_count = len(res.inserted_ids)
                total_inserted += inserted_count
                imported_collections.append(coll_name)
                logger.info(f"âœ… å¯¼å…¥é›†åˆ {coll_name}ï¼š{inserted_count} æ¡æ–‡æ¡£")

        return {
            "mode": "multi_collection",
            "collections": imported_collections,
            "total_collections": len(imported_collections),
            "total_inserted": total_inserted,
            "filename": filename,
            "format": format,
            "overwrite": overwrite,
        }
    else:
        # å•é›†åˆæ¨¡å¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        logger.info(f"ğŸ“„ å•é›†åˆå¯¼å…¥æ¨¡å¼ï¼Œç›®æ ‡é›†åˆ: {collection}")
        logger.info(f"ğŸ” [å•é›†åˆæ¨¡å¼] æ•°æ®ç±»å‹: {type(data)}")

        if isinstance(data, dict):
            logger.info(f"ğŸ” [å•é›†åˆæ¨¡å¼] å­—å…¸åŒ…å« {len(data)} ä¸ªé”®")
            logger.info(f"ğŸ” [å•é›†åˆæ¨¡å¼] é”®åˆ—è¡¨: {list(data.keys())[:10]}")

        collection_obj = db[collection]

        if not isinstance(data, list):
            logger.info(f"ğŸ” [å•é›†åˆæ¨¡å¼] æ•°æ®ä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨")
            data = [data]

        logger.info(f"ğŸ” [å•é›†åˆæ¨¡å¼] å‡†å¤‡æ’å…¥ {len(data)} æ¡æ–‡æ¡£")

        if overwrite:
            deleted_count = await collection_obj.delete_many({})
            logger.info(f"ğŸ—‘ï¸ æ¸…ç©ºé›†åˆ {collection}ï¼šåˆ é™¤ {deleted_count.deleted_count} æ¡æ–‡æ¡£")

        for doc in data:
            # è½¬æ¢ _id
            if "_id" in doc and isinstance(doc["_id"], str):
                try:
                    doc["_id"] = ObjectId(doc["_id"])
                except Exception:
                    del doc["_id"]

            # ğŸ”¥ è½¬æ¢æ—¥æœŸå­—æ®µï¼ˆå­—ç¬¦ä¸² -> datetimeï¼‰
            _convert_date_fields(doc)

        inserted_count = 0
        if data:
            res = await collection_obj.insert_many(data)
            inserted_count = len(res.inserted_ids)

        return {
            "mode": "single_collection",
            "collection": collection,
            "inserted_count": inserted_count,
            "filename": filename,
            "format": format,
            "overwrite": overwrite,
        }


def _sanitize_document(doc: Any) -> Any:
    """
    é€’å½’æ¸…ç©ºæ–‡æ¡£ä¸­çš„æ•æ„Ÿå­—æ®µ

    æ•æ„Ÿå­—æ®µå…³é”®è¯ï¼šapi_key, api_secret, secret, token, password,
                    client_secret, webhook_secret, private_key

    æ’é™¤å­—æ®µï¼šmax_tokens, timeout, retry_times ç­‰é…ç½®å­—æ®µï¼ˆä¸æ˜¯æ•æ„Ÿä¿¡æ¯ï¼‰
    """
    SENSITIVE_KEYWORDS = [
        "api_key", "api_secret", "secret", "token", "password",
        "client_secret", "webhook_secret", "private_key"
    ]

    # æ’é™¤çš„å­—æ®µï¼ˆè™½ç„¶åŒ…å«æ•æ„Ÿå…³é”®è¯ï¼Œä½†ä¸æ˜¯æ•æ„Ÿä¿¡æ¯ï¼‰
    EXCLUDED_FIELDS = [
        "max_tokens",      # LLM é…ç½®ï¼šæœ€å¤§ token æ•°
        "timeout",         # è¶…æ—¶æ—¶é—´
        "retry_times",     # é‡è¯•æ¬¡æ•°
        "context_length",  # ä¸Šä¸‹æ–‡é•¿åº¦
    ]

    if isinstance(doc, dict):
        sanitized = {}
        for k, v in doc.items():
            # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
            if k.lower() in [f.lower() for f in EXCLUDED_FIELDS]:
                # ä¿ç•™è¯¥å­—æ®µ
                if isinstance(v, (dict, list)):
                    sanitized[k] = _sanitize_document(v)
                else:
                    sanitized[k] = v
            # æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ…å«æ•æ„Ÿå…³é”®è¯ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            elif any(keyword in k.lower() for keyword in SENSITIVE_KEYWORDS):
                sanitized[k] = ""  # æ¸…ç©ºæ•æ„Ÿå­—æ®µ
            elif isinstance(v, (dict, list)):
                sanitized[k] = _sanitize_document(v)  # é€’å½’å¤„ç†
            else:
                sanitized[k] = v
        return sanitized
    elif isinstance(doc, list):
        return [_sanitize_document(item) for item in doc]
    else:
        return doc


async def export_data(collections: Optional[List[str]] = None, *, export_dir: str, format: str = "json", sanitize: bool = False) -> str:
    import pandas as pd

    # ğŸ”¥ ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“è¿æ¥
    db = get_mongo_db()
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")

    if not collections:
        # ğŸ”¥ å¼‚æ­¥è°ƒç”¨ list_collection_names()
        collections = await db.list_collection_names()
        collections = [c for c in collections if not c.startswith("system.")]

    os.makedirs(export_dir, exist_ok=True)

    all_data: Dict[str, List[dict]] = {}
    for collection_name in collections:
        collection = db[collection_name]
        docs: List[dict] = []

        # users é›†åˆåœ¨è„±æ•æ¨¡å¼ä¸‹åªå¯¼å‡ºç©ºæ•°ç»„ï¼ˆä¿ç•™ç»“æ„ï¼Œä¸å¯¼å‡ºå®é™…ç”¨æˆ·æ•°æ®ï¼‰
        if sanitize and collection_name == "users":
            all_data[collection_name] = []
            continue

        # ğŸ”¥ å¼‚æ­¥è¿­ä»£æŸ¥è¯¢ç»“æœ
        async for doc in collection.find():
            docs.append(serialize_document(doc))
        all_data[collection_name] = docs

    # å¦‚æœå¯ç”¨è„±æ•ï¼Œé€’å½’æ¸…ç©ºæ‰€æœ‰æ•æ„Ÿå­—æ®µ
    if sanitize:
        all_data = _sanitize_document(all_data)

    if format.lower() == "json":
        filename = f"export_{timestamp}.json"
        file_path = os.path.join(export_dir, filename)
        export_data_dict = {
            "export_info": {
                "created_at": datetime.utcnow().isoformat(),
                "collections": collections,
                "format": format,
            },
            "data": all_data,
        }

        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„æ–‡ä»¶ I/O æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        def _write_json():
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(export_data_dict, f, ensure_ascii=False, indent=2)

        await asyncio.to_thread(_write_json)
        return file_path

    if format.lower() == "csv":
        filename = f"export_{timestamp}.csv"
        file_path = os.path.join(export_dir, filename)
        rows: List[dict] = []
        for collection_name, documents in all_data.items():
            for doc in documents:
                row = {**doc}
                row["_collection"] = collection_name
                rows.append(row)

        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„æ–‡ä»¶ I/O æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        def _write_csv():
            if rows:
                pd.DataFrame(rows).to_csv(file_path, index=False, encoding="utf-8-sig")
            else:
                pd.DataFrame().to_csv(file_path, index=False, encoding="utf-8-sig")

        await asyncio.to_thread(_write_csv)
        return file_path

    if format.lower() in ["xlsx", "excel"]:
        filename = f"export_{timestamp}.xlsx"
        file_path = os.path.join(export_dir, filename)

        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread å°†é˜»å¡çš„æ–‡ä»¶ I/O æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
        def _write_excel():
            with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
                for collection_name, documents in all_data.items():
                    df = pd.DataFrame(documents) if documents else pd.DataFrame()
                    sheet = collection_name[:31]
                    df.to_excel(writer, sheet_name=sheet, index=False)

        await asyncio.to_thread(_write_excel)
        return file_path

    raise Exception(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")

