"""
ç®€åŒ–çš„è‚¡ç¥¨åˆ†ææœåŠ¡
ç›´æ¥è°ƒç”¨ç°æœ‰çš„ TradingAgents åˆ†æåŠŸèƒ½
"""

import asyncio
import uuid
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆå§‹åŒ–TradingAgentsæ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import init_logging
init_logging()

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from app.models.analysis import (
    AnalysisTask, AnalysisStatus, SingleAnalysisRequest, AnalysisParameters
)
from app.models.user import PyObjectId
from app.models.notification import NotificationCreate
from bson import ObjectId
from app.core.database import get_mongo_db
from app.services.config_service import ConfigService
from app.services.memory_state_manager import get_memory_state_manager, TaskStatus
from app.services.redis_progress_tracker import RedisProgressTracker, get_progress_by_id
from app.services.progress_log_handler import register_analysis_tracker, unregister_analysis_tracker

# è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è·å–ï¼ˆç”¨äºè¡¥å……æ˜¾ç¤ºåç§°ï¼‰
try:
    from tradingagents.dataflows.data_source_manager import get_data_source_manager
    _data_source_manager = get_data_source_manager()
    def _get_stock_info_safe(stock_code: str):
        """è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯çš„å®‰å…¨å°è£…"""
        return _data_source_manager.get_stock_basic_info(stock_code)
except Exception:
    _get_stock_info_safe = None

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("app.services.simple_analysis_service")

# é…ç½®æœåŠ¡å®ä¾‹
config_service = ConfigService()


async def get_provider_by_model_name(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ 'qwen-turbo', 'gpt-4' ç­‰

    Returns:
        str: ä¾›åº”å•†åç§°ï¼Œå¦‚ 'dashscope', 'openai' ç­‰
    """
    try:
        # ä»é…ç½®æœåŠ¡è·å–ç³»ç»Ÿé…ç½®
        system_config = await config_service.get_system_config()
        if not system_config or not system_config.llm_configs:
            logger.warning(f"âš ï¸ ç³»ç»Ÿé…ç½®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ä¾›åº”å•†æ˜ å°„")
            return _get_default_provider_by_model(model_name)

        # åœ¨LLMé…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹
        for llm_config in system_config.llm_configs:
            if llm_config.model_name == model_name:
                provider = llm_config.provider.value if hasattr(llm_config.provider, 'value') else str(llm_config.provider)
                logger.info(f"âœ… ä»æ•°æ®åº“æ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¾›åº”å•†: {provider}")
                return provider

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
        logger.warning(f"âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        return _get_default_provider_by_model(model_name)

    except Exception as e:
        logger.error(f"âŒ æŸ¥æ‰¾æ¨¡å‹ä¾›åº”å•†å¤±è´¥: {e}")
        return _get_default_provider_by_model(model_name)


def get_provider_by_model_name_sync(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ 'qwen-turbo', 'gpt-4' ç­‰

    Returns:
        str: ä¾›åº”å•†åç§°ï¼Œå¦‚ 'dashscope', 'openai' ç­‰
    """
    provider_info = get_provider_and_url_by_model_sync(model_name)
    return provider_info["provider"]


def get_provider_and_url_by_model_sync(model_name: str) -> dict:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†å’Œ API URLï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ 'qwen-turbo', 'gpt-4' ç­‰

    Returns:
        dict: {"provider": "google", "backend_url": "https://...", "api_key": "xxx"}
    """
    try:
        # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯ç›´æ¥æŸ¥è¯¢
        from pymongo import MongoClient
        from app.core.config import settings
        import os

        client = MongoClient(settings.MONGO_URI)
        db = client[settings.MONGO_DB]

        # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
        configs_collection = db.system_configs
        doc = configs_collection.find_one({"is_active": True}, sort=[("version", -1)])

        if doc and "llm_configs" in doc:
            llm_configs = doc["llm_configs"]

            for config_dict in llm_configs:
                if config_dict.get("model_name") == model_name:
                    provider = config_dict.get("provider")
                    api_base = config_dict.get("api_base")
                    model_api_key = config_dict.get("api_key")  # ğŸ”¥ è·å–æ¨¡å‹é…ç½®çš„ API Key

                    # ä» llm_providers é›†åˆä¸­æŸ¥æ‰¾å‚å®¶é…ç½®
                    providers_collection = db.llm_providers
                    provider_doc = providers_collection.find_one({"name": provider})

                    # ğŸ”¥ ç¡®å®š API Keyï¼ˆä¼˜å…ˆçº§ï¼šæ¨¡å‹é…ç½® > å‚å®¶é…ç½® > ç¯å¢ƒå˜é‡ï¼‰
                    api_key = None
                    if model_api_key and model_api_key.strip() and model_api_key != "your-api-key":
                        api_key = model_api_key
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨æ¨¡å‹é…ç½®çš„ API Key")
                    elif provider_doc and provider_doc.get("api_key"):
                        provider_api_key = provider_doc["api_key"]
                        if provider_api_key and provider_api_key.strip() and provider_api_key != "your-api-key":
                            api_key = provider_api_key
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶é…ç½®çš„ API Key")

                    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
                    if not api_key:
                        api_key = _get_env_api_key_for_provider(provider)
                        if api_key:
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨ç¯å¢ƒå˜é‡çš„ API Key")
                        else:
                            logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æœªæ‰¾åˆ° {provider} çš„ API Key")

                    # ç¡®å®š backend_url
                    backend_url = None
                    if api_base:
                        backend_url = api_base
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] æ¨¡å‹ {model_name} ä½¿ç”¨è‡ªå®šä¹‰ API: {api_base}")
                    elif provider_doc and provider_doc.get("default_base_url"):
                        backend_url = provider_doc["default_base_url"]
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] æ¨¡å‹ {model_name} ä½¿ç”¨å‚å®¶é»˜è®¤ API: {backend_url}")
                    else:
                        backend_url = _get_default_backend_url(provider)
                        logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] å‚å®¶ {provider} æ²¡æœ‰é…ç½® default_base_urlï¼Œä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼")

                    client.close()
                    return {
                        "provider": provider,
                        "backend_url": backend_url,
                        "api_key": api_key
                    }

        client.close()

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
        logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        provider = _get_default_provider_by_model(model_name)

        # å°è¯•ä»å‚å®¶é…ç½®ä¸­è·å– default_base_url å’Œ API Key
        try:
            client = MongoClient(settings.MONGO_URI)
            db = client[settings.MONGO_DB]
            providers_collection = db.llm_providers
            provider_doc = providers_collection.find_one({"name": provider})

            backend_url = _get_default_backend_url(provider)
            api_key = None

            if provider_doc:
                if provider_doc.get("default_base_url"):
                    backend_url = provider_doc["default_base_url"]
                    logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ default_base_url: {backend_url}")

                if provider_doc.get("api_key"):
                    provider_api_key = provider_doc["api_key"]
                    if provider_api_key and provider_api_key.strip() and provider_api_key != "your-api-key":
                        api_key = provider_api_key
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ API Key")

            # å¦‚æœå‚å®¶é…ç½®ä¸­æ²¡æœ‰ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            if not api_key:
                api_key = _get_env_api_key_for_provider(provider)
                if api_key:
                    logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨ç¯å¢ƒå˜é‡çš„ API Key")

            client.close()
            return {
                "provider": provider,
                "backend_url": backend_url,
                "api_key": api_key
            }
        except Exception as e:
            logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æ— æ³•æŸ¥è¯¢å‚å®¶é…ç½®: {e}")

        # æœ€åå›é€€åˆ°ç¡¬ç¼–ç çš„é»˜è®¤ URL å’Œç¯å¢ƒå˜é‡ API Key
        return {
            "provider": provider,
            "backend_url": _get_default_backend_url(provider),
            "api_key": _get_env_api_key_for_provider(provider)
        }

    except Exception as e:
        logger.error(f"âŒ [åŒæ­¥æŸ¥è¯¢] æŸ¥æ‰¾æ¨¡å‹ä¾›åº”å•†å¤±è´¥: {e}")
        provider = _get_default_provider_by_model(model_name)

        # å°è¯•ä»å‚å®¶é…ç½®ä¸­è·å– default_base_url å’Œ API Key
        try:
            from pymongo import MongoClient
            from app.core.config import settings

            client = MongoClient(settings.MONGO_URI)
            db = client[settings.MONGO_DB]
            providers_collection = db.llm_providers
            provider_doc = providers_collection.find_one({"name": provider})

            backend_url = _get_default_backend_url(provider)
            api_key = None

            if provider_doc:
                if provider_doc.get("default_base_url"):
                    backend_url = provider_doc["default_base_url"]
                    logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ default_base_url: {backend_url}")

                if provider_doc.get("api_key"):
                    provider_api_key = provider_doc["api_key"]
                    if provider_api_key and provider_api_key.strip() and provider_api_key != "your-api-key":
                        api_key = provider_api_key
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ API Key")

            # å¦‚æœå‚å®¶é…ç½®ä¸­æ²¡æœ‰ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            if not api_key:
                api_key = _get_env_api_key_for_provider(provider)

            client.close()
            return {
                "provider": provider,
                "backend_url": backend_url,
                "api_key": api_key
            }
        except Exception as e2:
            logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æ— æ³•æŸ¥è¯¢å‚å®¶é…ç½®: {e2}")

        # æœ€åå›é€€åˆ°ç¡¬ç¼–ç çš„é»˜è®¤ URL å’Œç¯å¢ƒå˜é‡ API Key
        return {
            "provider": provider,
            "backend_url": _get_default_backend_url(provider),
            "api_key": _get_env_api_key_for_provider(provider)
        }


def _get_env_api_key_for_provider(provider: str) -> str:
    """
    ä»ç¯å¢ƒå˜é‡è·å–æŒ‡å®šä¾›åº”å•†çš„ API Key

    Args:
        provider: ä¾›åº”å•†åç§°ï¼Œå¦‚ 'google', 'dashscope' ç­‰

    Returns:
        str: API Keyï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    import os

    env_key_map = {
        "google": "GOOGLE_API_KEY",
        "dashscope": "DASHSCOPE_API_KEY",
        "openai": "OPENAI_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
        "anthropic": "ANTHROPIC_API_KEY",
        "openrouter": "OPENROUTER_API_KEY",
        "siliconflow": "SILICONFLOW_API_KEY",
        "qianfan": "QIANFAN_API_KEY",
        "302ai": "AI302_API_KEY",
    }

    env_key_name = env_key_map.get(provider.lower())
    if env_key_name:
        api_key = os.getenv(env_key_name)
        if api_key and api_key.strip() and api_key != "your-api-key":
            return api_key

    return None


def _get_default_backend_url(provider: str) -> str:
    """
    æ ¹æ®ä¾›åº”å•†åç§°è¿”å›é»˜è®¤çš„ backend_url

    Args:
        provider: ä¾›åº”å•†åç§°ï¼Œå¦‚ 'google', 'dashscope' ç­‰

    Returns:
        str: é»˜è®¤çš„ backend_url
    """
    default_urls = {
        "google": "https://generativelanguage.googleapis.com/v1beta",
        "dashscope": "https://dashscope.aliyuncs.com/api/v1",
        "openai": "https://api.openai.com/v1",
        "deepseek": "https://api.deepseek.com",
        "anthropic": "https://api.anthropic.com",
        "openrouter": "https://openrouter.ai/api/v1",
        "qianfan": "https://qianfan.baidubce.com/v2",
        "302ai": "https://api.302.ai/v1",
    }

    url = default_urls.get(provider, "https://dashscope.aliyuncs.com/compatible-mode/v1")
    logger.info(f"ğŸ”§ [é»˜è®¤URL] {provider} -> {url}")
    return url


def _get_default_provider_by_model(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°è¿”å›é»˜è®¤çš„ä¾›åº”å•†æ˜ å°„
    è¿™æ˜¯ä¸€ä¸ªåå¤‡æ–¹æ¡ˆï¼Œå½“æ•°æ®åº“æŸ¥è¯¢å¤±è´¥æ—¶ä½¿ç”¨
    """
    # æ¨¡å‹åç§°åˆ°ä¾›åº”å•†çš„é»˜è®¤æ˜ å°„
    model_provider_map = {
        # é˜¿é‡Œç™¾ç‚¼ (DashScope)
        'qwen-turbo': 'dashscope',
        'qwen-plus': 'dashscope',
        'qwen-max': 'dashscope',
        'qwen-plus-latest': 'dashscope',
        'qwen-max-longcontext': 'dashscope',

        # OpenAI
        'gpt-3.5-turbo': 'openai',
        'gpt-4': 'openai',
        'gpt-4-turbo': 'openai',
        'gpt-4o': 'openai',
        'gpt-4o-mini': 'openai',

        # Google
        'gemini-pro': 'google',
        'gemini-2.0-flash': 'google',
        'gemini-2.0-flash-thinking-exp': 'google',

        # DeepSeek
        'deepseek-chat': 'deepseek',
        'deepseek-coder': 'deepseek',

        # æ™ºè°±AI
        'glm-4': 'zhipu',
        'glm-3-turbo': 'zhipu',
        'chatglm3-6b': 'zhipu'
    }

    provider = model_provider_map.get(model_name, 'dashscope')  # é»˜è®¤ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼
    logger.info(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æ˜ å°„: {model_name} -> {provider}")
    return provider


def create_analysis_config(
    research_depth,  # æ”¯æŒæ•°å­—(1-5)æˆ–å­—ç¬¦ä¸²("å¿«é€Ÿ", "æ ‡å‡†", "æ·±åº¦")
    selected_analysts: list,
    quick_model: str,
    deep_model: str,
    llm_provider: str,
    market_type: str = "Aè‚¡",
    quick_model_config: dict = None,  # æ–°å¢ï¼šå¿«é€Ÿæ¨¡å‹çš„å®Œæ•´é…ç½®
    deep_model_config: dict = None    # æ–°å¢ï¼šæ·±åº¦æ¨¡å‹çš„å®Œæ•´é…ç½®
) -> dict:
    """
    åˆ›å»ºåˆ†æé…ç½® - æ”¯æŒæ•°å­—ç­‰çº§å’Œä¸­æ–‡ç­‰çº§

    Args:
        research_depth: ç ”ç©¶æ·±åº¦ï¼Œæ”¯æŒæ•°å­—(1-5)æˆ–ä¸­æ–‡("å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†", "æ·±åº¦", "å…¨é¢")
        selected_analysts: é€‰ä¸­çš„åˆ†æå¸ˆåˆ—è¡¨
        quick_model: å¿«é€Ÿåˆ†ææ¨¡å‹
        deep_model: æ·±åº¦åˆ†ææ¨¡å‹
        llm_provider: LLMä¾›åº”å•†
        market_type: å¸‚åœºç±»å‹
        quick_model_config: å¿«é€Ÿæ¨¡å‹çš„å®Œæ•´é…ç½®ï¼ˆåŒ…å« max_tokensã€temperatureã€timeout ç­‰ï¼‰
        deep_model_config: æ·±åº¦æ¨¡å‹çš„å®Œæ•´é…ç½®ï¼ˆåŒ…å« max_tokensã€temperatureã€timeout ç­‰ï¼‰

    Returns:
        dict: å®Œæ•´çš„åˆ†æé…ç½®
    """
    # ğŸ” [è°ƒè¯•] è®°å½•æ¥æ”¶åˆ°çš„åŸå§‹å‚æ•°
    logger.info(f"ğŸ” [é…ç½®åˆ›å»º] æ¥æ”¶åˆ°çš„research_depthå‚æ•°: {research_depth} (ç±»å‹: {type(research_depth).__name__})")

    # æ•°å­—ç­‰çº§åˆ°ä¸­æ–‡ç­‰çº§çš„æ˜ å°„
    numeric_to_chinese = {
        1: "å¿«é€Ÿ",
        2: "åŸºç¡€",
        3: "æ ‡å‡†",
        4: "æ·±åº¦",
        5: "å…¨é¢"
    }

    # æ ‡å‡†åŒ–ç ”ç©¶æ·±åº¦ï¼šæ”¯æŒæ•°å­—è¾“å…¥
    if isinstance(research_depth, (int, float)):
        research_depth = int(research_depth)
        if research_depth in numeric_to_chinese:
            chinese_depth = numeric_to_chinese[research_depth]
            logger.info(f"ğŸ”¢ [ç­‰çº§è½¬æ¢] æ•°å­—ç­‰çº§ {research_depth} â†’ ä¸­æ–‡ç­‰çº§ '{chinese_depth}'")
            research_depth = chinese_depth
        else:
            logger.warning(f"âš ï¸ æ— æ•ˆçš„æ•°å­—ç­‰çº§: {research_depth}ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åˆ†æ")
            research_depth = "æ ‡å‡†"
    elif isinstance(research_depth, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—ï¼Œè½¬æ¢ä¸ºæ•´æ•°
        if research_depth.isdigit():
            numeric_level = int(research_depth)
            if numeric_level in numeric_to_chinese:
                chinese_depth = numeric_to_chinese[numeric_level]
                logger.info(f"ğŸ”¢ [ç­‰çº§è½¬æ¢] å­—ç¬¦ä¸²æ•°å­— '{research_depth}' â†’ ä¸­æ–‡ç­‰çº§ '{chinese_depth}'")
                research_depth = chinese_depth
            else:
                logger.warning(f"âš ï¸ æ— æ•ˆçš„å­—ç¬¦ä¸²æ•°å­—ç­‰çº§: {research_depth}ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åˆ†æ")
                research_depth = "æ ‡å‡†"
        # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ç­‰çº§ï¼Œç›´æ¥ä½¿ç”¨
        elif research_depth in ["å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†", "æ·±åº¦", "å…¨é¢"]:
            logger.info(f"ğŸ“ [ç­‰çº§ç¡®è®¤] ä½¿ç”¨ä¸­æ–‡ç­‰çº§: '{research_depth}'")
        else:
            logger.warning(f"âš ï¸ æœªçŸ¥çš„ç ”ç©¶æ·±åº¦: {research_depth}ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åˆ†æ")
            research_depth = "æ ‡å‡†"
    else:
        logger.warning(f"âš ï¸ æ— æ•ˆçš„ç ”ç©¶æ·±åº¦ç±»å‹: {type(research_depth)}ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åˆ†æ")
        research_depth = "æ ‡å‡†"

    # ä»DEFAULT_CONFIGå¼€å§‹ï¼Œå®Œå…¨å¤åˆ¶webç›®å½•çš„é€»è¾‘
    config = DEFAULT_CONFIG.copy()
    config["llm_provider"] = llm_provider
    config["deep_think_llm"] = deep_model
    config["quick_think_llm"] = quick_model

    # æ ¹æ®ç ”ç©¶æ·±åº¦è°ƒæ•´é…ç½® - æ”¯æŒ5ä¸ªçº§åˆ«ï¼ˆä¸Webç•Œé¢ä¿æŒä¸€è‡´ï¼‰
    if research_depth == "å¿«é€Ÿ":
        # 1çº§ - å¿«é€Ÿåˆ†æ
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 1
        config["memory_enabled"] = False  # ç¦ç”¨è®°å¿†ä»¥åŠ é€Ÿ
        config["online_tools"] = True  # ç»Ÿä¸€ä½¿ç”¨åœ¨çº¿å·¥å…·ï¼Œé¿å…ç¦»çº¿å·¥å…·çš„å„ç§é—®é¢˜
        logger.info(f"ğŸ”§ [1çº§-å¿«é€Ÿåˆ†æ] {market_type}ä½¿ç”¨ç»Ÿä¸€å·¥å…·ï¼Œç¡®ä¿æ•°æ®æºæ­£ç¡®å’Œç¨³å®šæ€§")
        logger.info(f"ğŸ”§ [1çº§-å¿«é€Ÿåˆ†æ] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: quick={quick_model}, deep={deep_model}")

    elif research_depth == "åŸºç¡€":
        # 2çº§ - åŸºç¡€åˆ†æ
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 1
        config["memory_enabled"] = True
        config["online_tools"] = True
        logger.info(f"ğŸ”§ [2çº§-åŸºç¡€åˆ†æ] {market_type}ä½¿ç”¨åœ¨çº¿å·¥å…·ï¼Œè·å–æœ€æ–°æ•°æ®")
        logger.info(f"ğŸ”§ [2çº§-åŸºç¡€åˆ†æ] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: quick={quick_model}, deep={deep_model}")

    elif research_depth == "æ ‡å‡†":
        # 3çº§ - æ ‡å‡†åˆ†æï¼ˆæ¨èï¼‰
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True
        logger.info(f"ğŸ”§ [3çº§-æ ‡å‡†åˆ†æ] {market_type}å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼ˆæ¨èï¼‰")
        logger.info(f"ğŸ”§ [3çº§-æ ‡å‡†åˆ†æ] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: quick={quick_model}, deep={deep_model}")

    elif research_depth == "æ·±åº¦":
        # 4çº§ - æ·±åº¦åˆ†æ
        config["max_debate_rounds"] = 2
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True
        logger.info(f"ğŸ”§ [4çº§-æ·±åº¦åˆ†æ] {market_type}å¤šè½®è¾©è®ºï¼Œæ·±åº¦ç ”ç©¶")
        logger.info(f"ğŸ”§ [4çº§-æ·±åº¦åˆ†æ] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: quick={quick_model}, deep={deep_model}")

    elif research_depth == "å…¨é¢":
        # 5çº§ - å…¨é¢åˆ†æ
        config["max_debate_rounds"] = 3
        config["max_risk_discuss_rounds"] = 3
        config["memory_enabled"] = True
        config["online_tools"] = True
        logger.info(f"ğŸ”§ [5çº§-å…¨é¢åˆ†æ] {market_type}æœ€å…¨é¢çš„åˆ†æï¼Œæœ€é«˜è´¨é‡")
        logger.info(f"ğŸ”§ [5çº§-å…¨é¢åˆ†æ] ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹: quick={quick_model}, deep={deep_model}")

    else:
        # é»˜è®¤ä½¿ç”¨æ ‡å‡†åˆ†æ
        logger.warning(f"âš ï¸ æœªçŸ¥çš„ç ”ç©¶æ·±åº¦: {research_depth}ï¼Œä½¿ç”¨æ ‡å‡†åˆ†æ")
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True

    # ğŸ”§ è·å– backend_url å’Œ API Keyï¼ˆä¼˜å…ˆçº§ï¼šæ¨¡å‹é…ç½® > å‚å®¶é…ç½® > ç¯å¢ƒå˜é‡ï¼‰
    try:
        # 1ï¸âƒ£ ä¼˜å…ˆä»æ•°æ®åº“è·å–ï¼ˆåŒ…å«æ¨¡å‹é…ç½®çš„ api_baseã€API Key å’Œå‚å®¶çš„ default_base_urlã€API Keyï¼‰
        quick_provider_info = get_provider_and_url_by_model_sync(quick_model)
        deep_provider_info = get_provider_and_url_by_model_sync(deep_model)

        config["backend_url"] = quick_provider_info["backend_url"]
        config["quick_api_key"] = quick_provider_info.get("api_key")  # ğŸ”¥ ä¿å­˜å¿«é€Ÿæ¨¡å‹çš„ API Key
        config["deep_api_key"] = deep_provider_info.get("api_key")    # ğŸ”¥ ä¿å­˜æ·±åº¦æ¨¡å‹çš„ API Key

        logger.info(f"âœ… ä½¿ç”¨æ•°æ®åº“é…ç½®çš„ backend_url: {quick_provider_info['backend_url']}")
        logger.info(f"   æ¥æº: æ¨¡å‹ {quick_model} çš„é…ç½®æˆ–å‚å®¶ {quick_provider_info['provider']} çš„é»˜è®¤åœ°å€")
        logger.info(f"ğŸ”‘ å¿«é€Ÿæ¨¡å‹ API Key: {'å·²é…ç½®' if config['quick_api_key'] else 'æœªé…ç½®ï¼ˆå°†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰'}")
        logger.info(f"ğŸ”‘ æ·±åº¦æ¨¡å‹ API Key: {'å·²é…ç½®' if config['deep_api_key'] else 'æœªé…ç½®ï¼ˆå°†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰'}")
    except Exception as e:
        logger.warning(f"âš ï¸  æ— æ³•ä»æ•°æ®åº“è·å– backend_url å’Œ API Key: {e}")
        # 2ï¸âƒ£ å›é€€åˆ°ç¡¬ç¼–ç çš„é»˜è®¤ URLï¼ŒAPI Key å°†ä»ç¯å¢ƒå˜é‡è¯»å–
        if llm_provider == "dashscope":
            config["backend_url"] = "https://dashscope.aliyuncs.com/api/v1"
        elif llm_provider == "deepseek":
            config["backend_url"] = "https://api.deepseek.com"
        elif llm_provider == "openai":
            config["backend_url"] = "https://api.openai.com/v1"
        elif llm_provider == "google":
            config["backend_url"] = "https://generativelanguage.googleapis.com/v1beta"
        elif llm_provider == "qianfan":
            config["backend_url"] = "https://aip.baidubce.com"
        else:
            # ğŸ”§ æœªçŸ¥å‚å®¶ï¼Œå°è¯•ä»æ•°æ®åº“è·å–å‚å®¶çš„ default_base_url
            logger.warning(f"âš ï¸  æœªçŸ¥å‚å®¶ {llm_provider}ï¼Œå°è¯•ä»æ•°æ®åº“è·å–é…ç½®")
            try:
                from pymongo import MongoClient
                from app.core.config import settings

                client = MongoClient(settings.MONGO_URI)
                db = client[settings.MONGO_DB]
                providers_collection = db.llm_providers
                provider_doc = providers_collection.find_one({"name": llm_provider})

                if provider_doc and provider_doc.get("default_base_url"):
                    config["backend_url"] = provider_doc["default_base_url"]
                    logger.info(f"âœ… ä»æ•°æ®åº“è·å–è‡ªå®šä¹‰å‚å®¶ {llm_provider} çš„ backend_url: {config['backend_url']}")
                else:
                    # å¦‚æœæ•°æ®åº“ä¸­ä¹Ÿæ²¡æœ‰ï¼Œä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ä½œä¸ºæœ€åçš„å›é€€
                    config["backend_url"] = "https://api.openai.com/v1"
                    logger.warning(f"âš ï¸  æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å‚å®¶ {llm_provider} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤ OpenAI ç«¯ç‚¹")

                client.close()
            except Exception as e2:
                logger.error(f"âŒ æŸ¥è¯¢æ•°æ®åº“å¤±è´¥: {e2}ï¼Œä½¿ç”¨é»˜è®¤ OpenAI ç«¯ç‚¹")
                config["backend_url"] = "https://api.openai.com/v1"

        logger.info(f"âš ï¸  ä½¿ç”¨å›é€€çš„ backend_url: {config['backend_url']}")

    # æ·»åŠ åˆ†æå¸ˆé…ç½®
    config["selected_analysts"] = selected_analysts
    config["debug"] = False

    # ğŸ”§ æ·»åŠ research_depthåˆ°é…ç½®ä¸­ï¼Œä½¿å·¥å…·å‡½æ•°èƒ½å¤Ÿè®¿é—®åˆ†æçº§åˆ«ä¿¡æ¯
    config["research_depth"] = research_depth

    # ğŸ”§ æ·»åŠ æ¨¡å‹é…ç½®å‚æ•°ï¼ˆmax_tokensã€temperatureã€timeoutã€retry_timesï¼‰
    if quick_model_config:
        config["quick_model_config"] = quick_model_config
        logger.info(f"ğŸ”§ [å¿«é€Ÿæ¨¡å‹é…ç½®] max_tokens={quick_model_config.get('max_tokens')}, "
                   f"temperature={quick_model_config.get('temperature')}, "
                   f"timeout={quick_model_config.get('timeout')}, "
                   f"retry_times={quick_model_config.get('retry_times')}")

    if deep_model_config:
        config["deep_model_config"] = deep_model_config
        logger.info(f"ğŸ”§ [æ·±åº¦æ¨¡å‹é…ç½®] max_tokens={deep_model_config.get('max_tokens')}, "
                   f"temperature={deep_model_config.get('temperature')}, "
                   f"timeout={deep_model_config.get('timeout')}, "
                   f"retry_times={deep_model_config.get('retry_times')}")

    logger.info(f"ğŸ“‹ ========== åˆ›å»ºåˆ†æé…ç½®å®Œæˆ ==========")
    logger.info(f"   ğŸ¯ ç ”ç©¶æ·±åº¦: {research_depth}")
    logger.info(f"   ğŸ”¥ è¾©è®ºè½®æ¬¡: {config['max_debate_rounds']}")
    logger.info(f"   âš–ï¸ é£é™©è®¨è®ºè½®æ¬¡: {config['max_risk_discuss_rounds']}")
    logger.info(f"   ğŸ’¾ è®°å¿†åŠŸèƒ½: {config['memory_enabled']}")
    logger.info(f"   ğŸŒ åœ¨çº¿å·¥å…·: {config['online_tools']}")
    logger.info(f"   ğŸ¤– LLMä¾›åº”å•†: {llm_provider}")
    logger.info(f"   âš¡ å¿«é€Ÿæ¨¡å‹: {config['quick_think_llm']}")
    logger.info(f"   ğŸ§  æ·±åº¦æ¨¡å‹: {config['deep_think_llm']}")
    logger.info(f"ğŸ“‹ ========================================")

    return config


class SimpleAnalysisService:
    """ç®€åŒ–çš„è‚¡ç¥¨åˆ†ææœåŠ¡ç±»"""

    def __init__(self):
        self._trading_graph_cache = {}
        self.memory_manager = get_memory_state_manager()

        # è¿›åº¦è·Ÿè¸ªå™¨ç¼“å­˜
        self._progress_trackers: Dict[str, RedisProgressTracker] = {}

        # ğŸ”§ åˆ›å»ºå…±äº«çš„çº¿ç¨‹æ± ï¼Œæ”¯æŒå¹¶å‘æ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡
        # é»˜è®¤æœ€å¤šåŒæ—¶æ‰§è¡Œ3ä¸ªåˆ†æä»»åŠ¡ï¼ˆå¯æ ¹æ®æœåŠ¡å™¨èµ„æºè°ƒæ•´ï¼‰
        import concurrent.futures
        self._thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=3)

        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] SimpleAnalysisService å®ä¾‹ID: {id(self)}")
        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")
        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] çº¿ç¨‹æ± æœ€å¤§å¹¶å‘æ•°: 3")

        # è®¾ç½® WebSocket ç®¡ç†å™¨
        # ç®€å•çš„è‚¡ç¥¨åç§°ç¼“å­˜ï¼Œå‡å°‘é‡å¤æŸ¥è¯¢
        self._stock_name_cache: Dict[str, str] = {}

        # è®¾ç½® WebSocket ç®¡ç†å™¨
        try:
            from app.services.websocket_manager import get_websocket_manager
            self.memory_manager.set_websocket_manager(get_websocket_manager())
        except ImportError:
            logger.warning("âš ï¸ WebSocket ç®¡ç†å™¨ä¸å¯ç”¨")

    async def _update_progress_async(self, task_id: str, progress: int, message: str):
        """å¼‚æ­¥æ›´æ–°è¿›åº¦ï¼ˆå†…å­˜å’ŒMongoDBï¼‰"""
        try:
            # æ›´æ–°å†…å­˜
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=progress,
                message=message,
                current_step=message
            )

            # æ›´æ–° MongoDB
            from app.core.database import get_mongo_db
            from datetime import datetime
            db = get_mongo_db()
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "progress": progress,
                        "current_step": message,
                        "message": message,
                        "updated_at": datetime.utcnow()
                    }
                }
            )
            logger.debug(f"âœ… [å¼‚æ­¥æ›´æ–°] å·²æ›´æ–°å†…å­˜å’ŒMongoDB: {progress}%")
        except Exception as e:
            logger.warning(f"âš ï¸ [å¼‚æ­¥æ›´æ–°] å¤±è´¥: {e}")

    def _resolve_stock_name(self, code: Optional[str]) -> str:
        """è§£æè‚¡ç¥¨åç§°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if not code:
            return ""
        # å‘½ä¸­ç¼“å­˜
        if code in self._stock_name_cache:
            return self._stock_name_cache[code]
        name = None
        try:
            if _get_stock_info_safe:
                info = _get_stock_info_safe(code)
                if isinstance(info, dict):
                    name = info.get("name")
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨åç§°å¤±è´¥: {code} - {e}")
        if not name:
            name = f"è‚¡ç¥¨{code}"
        # å†™ç¼“å­˜
        self._stock_name_cache[code] = name
        return name

    def _enrich_stock_names(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºä»»åŠ¡åˆ—è¡¨è¡¥é½è‚¡ç¥¨åç§°(å°±åœ°æ›´æ–°)"""
        try:
            for t in tasks:
                code = t.get("stock_code") or t.get("stock_symbol")
                name = t.get("stock_name")
                if not name and code:
                    t["stock_name"] = self._resolve_stock_name(code)
        except Exception as e:
            logger.warning(f"âš ï¸ è¡¥é½è‚¡ç¥¨åç§°æ—¶å‡ºç°å¼‚å¸¸: {e}")
        return tasks

    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # å¦‚æœæ˜¯adminç”¨æˆ·ï¼Œä½¿ç”¨å›ºå®šçš„ObjectId
            if user_id == "admin":
                admin_object_id = ObjectId("507f1f77bcf86cd799439011")
                logger.info(f"ğŸ”„ è½¬æ¢adminç”¨æˆ·ID: {user_id} -> {admin_object_id}")
                return PyObjectId(admin_object_id)
            else:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºObjectId
                object_id = ObjectId(user_id)
                logger.info(f"ğŸ”„ è½¬æ¢ç”¨æˆ·ID: {user_id} -> {object_id}")
                return PyObjectId(object_id)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·IDè½¬æ¢å¤±è´¥: {user_id} -> {e}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ObjectId
            new_object_id = ObjectId()
            logger.warning(f"âš ï¸ ç”Ÿæˆæ–°çš„ç”¨æˆ·ID: {new_object_id}")
            return PyObjectId(new_object_id)

    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså®ä¾‹

        âš ï¸ æ³¨æ„ï¼šä¸ºäº†é¿å…å¹¶å‘æ‰§è¡Œæ—¶çš„æ•°æ®æ··æ·†ï¼Œæ¯æ¬¡éƒ½åˆ›å»ºæ–°å®ä¾‹
        è™½ç„¶è¿™ä¼šå¢åŠ ä¸€äº›åˆå§‹åŒ–å¼€é”€ï¼Œä½†å¯ä»¥ç¡®ä¿çº¿ç¨‹å®‰å…¨

        TradingAgentsGraph å®ä¾‹åŒ…å«å¯å˜çŠ¶æ€ï¼ˆself.ticker, self.curr_stateç­‰ï¼‰ï¼Œ
        å¦‚æœå¤šä¸ªçº¿ç¨‹å…±äº«åŒä¸€ä¸ªå®ä¾‹ï¼Œä¼šå¯¼è‡´æ•°æ®æ··æ·†ã€‚
        """
        # ğŸ”§ [å¹¶å‘å®‰å…¨] æ¯æ¬¡éƒ½åˆ›å»ºæ–°å®ä¾‹ï¼Œé¿å…å¤šçº¿ç¨‹å…±äº«çŠ¶æ€
        # ä¸å†ä½¿ç”¨ç¼“å­˜ï¼Œå› ä¸º TradingAgentsGraph æœ‰å¯å˜çš„å®ä¾‹å˜é‡
        logger.info(f"ğŸ”§ åˆ›å»ºæ–°çš„TradingAgentså®ä¾‹ï¼ˆå¹¶å‘å®‰å…¨æ¨¡å¼ï¼‰...")

        trading_graph = TradingAgentsGraph(
            selected_analysts=config.get("selected_analysts", ["market", "fundamentals"]),
            debug=config.get("debug", False),
            config=config
        )

        logger.info(f"âœ… TradingAgentså®ä¾‹åˆ›å»ºæˆåŠŸï¼ˆå®ä¾‹ID: {id(trading_graph)}ï¼‰")

        return trading_graph

    async def create_analysis_task(
        self,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†æä»»åŠ¡ï¼ˆç«‹å³è¿”å›ï¼Œä¸æ‰§è¡Œåˆ†æï¼‰"""
        try:
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())

            # ğŸ”§ ä½¿ç”¨ get_symbol() æ–¹æ³•è·å–è‚¡ç¥¨ä»£ç ï¼ˆå…¼å®¹ symbol å’Œ stock_code å­—æ®µï¼‰
            stock_code = request.get_symbol()
            if not stock_code:
                raise ValueError("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")

            logger.info(f"ğŸ“ åˆ›å»ºåˆ†æä»»åŠ¡: {task_id} - {stock_code}")
            logger.info(f"ğŸ” å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")

            # åœ¨å†…å­˜ä¸­åˆ›å»ºä»»åŠ¡çŠ¶æ€
            task_state = await self.memory_manager.create_task(
                task_id=task_id,
                user_id=user_id,
                stock_code=stock_code,
                parameters=request.parameters.model_dump() if request.parameters else {},
                stock_name=(self._resolve_stock_name(stock_code) if hasattr(self, '_resolve_stock_name') else None),
            )

            logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²åˆ›å»º: {task_state.task_id}")

            # ç«‹å³éªŒè¯ä»»åŠ¡æ˜¯å¦å¯ä»¥æŸ¥è¯¢åˆ°
            verify_task = await self.memory_manager.get_task(task_id)
            if verify_task:
                logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºéªŒè¯æˆåŠŸ: {verify_task.task_id}")
            else:
                logger.error(f"âŒ ä»»åŠ¡åˆ›å»ºéªŒè¯å¤±è´¥: æ— æ³•æŸ¥è¯¢åˆ°åˆšåˆ›å»ºçš„ä»»åŠ¡ {task_id}")

            # è¡¥é½è‚¡ç¥¨åç§°å¹¶å†™å…¥æ•°æ®åº“ä»»åŠ¡æ–‡æ¡£çš„åˆå§‹è®°å½•
            code = stock_code
            name = self._resolve_stock_name(code) if hasattr(self, '_resolve_stock_name') else f"è‚¡ç¥¨{code}"

            try:
                db = get_mongo_db()
                result = await db.analysis_tasks.update_one(
                    {"task_id": task_id},
                    {"$setOnInsert": {
                        "task_id": task_id,
                        "user_id": user_id,
                        "stock_code": code,
                        "stock_symbol": code,
                        "stock_name": name,
                        "status": "pending",
                        "progress": 0,
                        "created_at": datetime.utcnow(),
                    }},
                    upsert=True
                )

                if result.upserted_id or result.matched_count > 0:
                    logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°MongoDB: {task_id}")
                else:
                    logger.warning(f"âš ï¸ MongoDBä¿å­˜ç»“æœå¼‚å¸¸: matched={result.matched_count}, upserted={result.upserted_id}")

            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºä»»åŠ¡æ—¶å†™å…¥MongoDBå¤±è´¥: {e}")
                # è¿™é‡Œä¸åº”è¯¥å¿½ç•¥é”™è¯¯ï¼Œå› ä¸ºæ²¡æœ‰MongoDBè®°å½•ä¼šå¯¼è‡´çŠ¶æ€æŸ¥è¯¢å¤±è´¥
                # ä½†ä¸ºäº†ä¸å½±å“ä»»åŠ¡æ‰§è¡Œï¼Œæˆ‘ä»¬è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œ
                import traceback
                logger.error(f"âŒ MongoDBä¿å­˜è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

            return {
                "task_id": task_id,
                "status": "pending",
                "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ"
            }

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def execute_analysis_background(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest
    ):
        """åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡"""
        # ğŸ”§ ä½¿ç”¨ get_symbol() æ–¹æ³•è·å–è‚¡ç¥¨ä»£ç ï¼ˆå…¼å®¹ symbol å’Œ stock_code å­—æ®µï¼‰
        stock_code = request.get_symbol()

        # æ·»åŠ æœ€å¤–å±‚çš„å¼‚å¸¸æ•è·ï¼Œç¡®ä¿æ‰€æœ‰å¼‚å¸¸éƒ½è¢«è®°å½•
        try:
            logger.info(f"ğŸ¯ğŸ¯ğŸ¯ [ENTRY] execute_analysis_background æ–¹æ³•è¢«è°ƒç”¨: {task_id}")
            logger.info(f"ğŸ¯ğŸ¯ğŸ¯ [ENTRY] user_id={user_id}, stock_code={stock_code}")
        except Exception as entry_error:
            print(f"âŒâŒâŒ [CRITICAL] æ—¥å¿—è®°å½•å¤±è´¥: {entry_error}")
            import traceback
            traceback.print_exc()

        progress_tracker = None
        try:
            logger.info(f"ğŸš€ å¼€å§‹åå°æ‰§è¡Œåˆ†æä»»åŠ¡: {task_id}")

            # ğŸ” éªŒè¯è‚¡ç¥¨ä»£ç æ˜¯å¦å­˜åœ¨
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯è‚¡ç¥¨ä»£ç : {stock_code}")
            from tradingagents.utils.stock_validator import prepare_stock_data_async
            from datetime import datetime

            # è·å–å¸‚åœºç±»å‹
            market_type = request.parameters.market_type if request.parameters else "Aè‚¡"

            # è·å–åˆ†ææ—¥æœŸå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            analysis_date = request.parameters.analysis_date if request.parameters else None
            if analysis_date:
                # å¦‚æœæ˜¯ datetime å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if isinstance(analysis_date, datetime):
                    analysis_date = analysis_date.strftime('%Y-%m-%d')
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
                elif isinstance(analysis_date, str):
                    # å°è¯•è§£æå¹¶é‡æ–°æ ¼å¼åŒ–ï¼Œç¡®ä¿æ ¼å¼ç»Ÿä¸€
                    try:
                        parsed_date = datetime.strptime(analysis_date, '%Y-%m-%d')
                        analysis_date = parsed_date.strftime('%Y-%m-%d')
                    except ValueError:
                        # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œä½¿ç”¨ä»Šå¤©
                        analysis_date = datetime.now().strftime('%Y-%m-%d')
                        logger.warning(f"âš ï¸ åˆ†ææ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨ä»Šå¤©: {analysis_date}")

            # ğŸ”¥ ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼Œç›´æ¥ awaitï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
            validation_result = await prepare_stock_data_async(
                stock_code=stock_code,
                market_type=market_type,
                period_days=30,
                analysis_date=analysis_date
            )

            if not validation_result.is_valid:
                error_msg = f"âŒ è‚¡ç¥¨ä»£ç éªŒè¯å¤±è´¥: {validation_result.error_message}"
                logger.error(error_msg)
                logger.error(f"ğŸ’¡ å»ºè®®: {validation_result.suggestion}")

                # æ„å»ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
                user_friendly_error = (
                    f"âŒ è‚¡ç¥¨ä»£ç æ— æ•ˆ\n\n"
                    f"{validation_result.error_message}\n\n"
                    f"ğŸ’¡ {validation_result.suggestion}"
                )

                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                await self.memory_manager.update_task_status(
                    task_id=task_id,
                    status=AnalysisStatus.FAILED,
                    progress=0,
                    error_message=user_friendly_error
                )

                # æ›´æ–°MongoDBçŠ¶æ€
                await self._update_task_status(
                    task_id,
                    AnalysisStatus.FAILED,
                    0,
                    error_message=user_friendly_error
                )

                return

            logger.info(f"âœ… è‚¡ç¥¨ä»£ç éªŒè¯é€šè¿‡: {stock_code} - {validation_result.stock_name}")
            logger.info(f"ğŸ“Š å¸‚åœºç±»å‹: {validation_result.market_type}")
            logger.info(f"ğŸ“ˆ å†å²æ•°æ®: {'æœ‰' if validation_result.has_historical_data else 'æ— '}")
            logger.info(f"ğŸ“‹ åŸºæœ¬ä¿¡æ¯: {'æœ‰' if validation_result.has_basic_info else 'æ— '}")

            # åœ¨çº¿ç¨‹æ± ä¸­åˆ›å»ºRedisè¿›åº¦è·Ÿè¸ªå™¨ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            def create_progress_tracker():
                """åœ¨çº¿ç¨‹ä¸­åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨"""
                logger.info(f"ğŸ“Š [çº¿ç¨‹] åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨: {task_id}")
                tracker = RedisProgressTracker(
                    task_id=task_id,
                    analysts=request.parameters.selected_analysts or ["market", "fundamentals"],
                    research_depth=request.parameters.research_depth or "æ ‡å‡†",
                    llm_provider="dashscope"
                )
                logger.info(f"âœ… [çº¿ç¨‹] è¿›åº¦è·Ÿè¸ªå™¨åˆ›å»ºå®Œæˆ: {task_id}")
                return tracker

            progress_tracker = await asyncio.to_thread(create_progress_tracker)

            # ç¼“å­˜è¿›åº¦è·Ÿè¸ªå™¨
            self._progress_trackers[task_id] = progress_tracker

            # æ³¨å†Œåˆ°æ—¥å¿—ç›‘æ§
            register_analysis_tracker(task_id, progress_tracker)

            # åˆå§‹åŒ–è¿›åº¦ï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            await asyncio.to_thread(
                progress_tracker.update_progress,
                {
                    "progress_percentage": 10,
                    "last_message": "ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ"
                }
            )

            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=10,
                message="åˆ†æå¼€å§‹...",
                current_step="initialization"
            )

            # åŒæ­¥æ›´æ–°MongoDBçŠ¶æ€
            await self._update_task_status(task_id, AnalysisStatus.PROCESSING, 10)

            # æ•°æ®å‡†å¤‡é˜¶æ®µï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            await asyncio.to_thread(
                progress_tracker.update_progress,
                {
                    "progress_percentage": 20,
                    "last_message": "ğŸ”§ æ£€æŸ¥ç¯å¢ƒé…ç½®"
                }
            )
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=20,
                message="å‡†å¤‡åˆ†ææ•°æ®...",
                current_step="data_preparation"
            )

            # åŒæ­¥æ›´æ–°MongoDBçŠ¶æ€
            await self._update_task_status(task_id, AnalysisStatus.PROCESSING, 20)

            # æ‰§è¡Œå®é™…çš„åˆ†æ
            result = await self._execute_analysis_sync(task_id, user_id, request, progress_tracker)

            # æ ‡è®°è¿›åº¦è·Ÿè¸ªå™¨å®Œæˆï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            await asyncio.to_thread(progress_tracker.mark_completed)

            # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶å’Œæ•°æ®åº“
            try:
                logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜åˆ†æç»“æœ: {task_id}")
                await self._save_analysis_results_complete(task_id, result)
                logger.info(f"âœ… åˆ†æç»“æœä¿å­˜å®Œæˆ: {task_id}")
            except Exception as save_error:
                logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {task_id} - {save_error}")
                # ä¿å­˜å¤±è´¥ä¸å½±å“åˆ†æå®ŒæˆçŠ¶æ€

            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥å³å°†ä¿å­˜åˆ°å†…å­˜çš„result
            logger.info(f"ğŸ” [DEBUG] å³å°†ä¿å­˜åˆ°å†…å­˜çš„resulté”®: {list(result.keys())}")
            logger.info(f"ğŸ” [DEBUG] å³å°†ä¿å­˜åˆ°å†…å­˜çš„decision: {bool(result.get('decision'))}")
            if result.get('decision'):
                logger.info(f"ğŸ” [DEBUG] å³å°†ä¿å­˜çš„decisionå†…å®¹: {result['decision']}")

            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.COMPLETED,
                progress=100,
                message="åˆ†æå®Œæˆ",
                current_step="completed",
                result_data=result
            )

            # åŒæ­¥æ›´æ–°MongoDBçŠ¶æ€ä¸ºå®Œæˆ
            await self._update_task_status(task_id, AnalysisStatus.COMPLETED, 100)

            # åˆ›å»ºé€šçŸ¥ï¼šåˆ†æå®Œæˆï¼ˆæ–¹æ¡ˆBï¼šREST+SSEï¼‰
            try:
                from app.services.notifications_service import get_notifications_service
                svc = get_notifications_service()
                summary = str(result.get("summary", ""))[:120]
                await svc.create_and_publish(
                    payload=NotificationCreate(
                        user_id=str(user_id),
                        type='analysis',
                        title=f"{request.stock_code} åˆ†æå®Œæˆ",
                        content=summary,
                        link=f"/stocks/{request.stock_code}",
                        source='analysis'
                    )
                )
            except Exception as notif_err:
                logger.warning(f"âš ï¸ åˆ›å»ºé€šçŸ¥å¤±è´¥(å¿½ç•¥): {notif_err}")

            logger.info(f"âœ… åå°åˆ†æä»»åŠ¡å®Œæˆ: {task_id}")

        except Exception as e:
            logger.error(f"âŒ åå°åˆ†æä»»åŠ¡å¤±è´¥: {task_id} - {e}")

            # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ä¸ºç”¨æˆ·å‹å¥½çš„æç¤º
            from ..utils.error_formatter import ErrorFormatter

            # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
            error_context = {}
            if hasattr(request, 'parameters') and request.parameters:
                if hasattr(request.parameters, 'quick_model'):
                    error_context['model'] = request.parameters.quick_model
                if hasattr(request.parameters, 'deep_model'):
                    error_context['model'] = request.parameters.deep_model

            # æ ¼å¼åŒ–é”™è¯¯
            formatted_error = ErrorFormatter.format_error(str(e), error_context)

            # æ„å»ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            user_friendly_error = (
                f"{formatted_error['title']}\n\n"
                f"{formatted_error['message']}\n\n"
                f"ğŸ’¡ {formatted_error['suggestion']}"
            )

            # æ ‡è®°è¿›åº¦è·Ÿè¸ªå™¨å¤±è´¥
            if progress_tracker:
                progress_tracker.mark_failed(user_friendly_error)

            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.FAILED,
                progress=0,
                message="åˆ†æå¤±è´¥",
                current_step="failed",
                error_message=user_friendly_error
            )

            # åŒæ­¥æ›´æ–°MongoDBçŠ¶æ€ä¸ºå¤±è´¥
            await self._update_task_status(task_id, AnalysisStatus.FAILED, 0, user_friendly_error)
        finally:
            # æ¸…ç†è¿›åº¦è·Ÿè¸ªå™¨ç¼“å­˜
            if task_id in self._progress_trackers:
                del self._progress_trackers[task_id]

            # ä»æ—¥å¿—ç›‘æ§ä¸­æ³¨é”€
            unregister_analysis_tracker(task_id)

    async def _execute_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest,
        progress_tracker: Optional[RedisProgressTracker] = None
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æï¼ˆåœ¨å…±äº«çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        # ğŸ”§ ä½¿ç”¨å…±äº«çº¿ç¨‹æ± ï¼Œæ”¯æŒå¤šä¸ªä»»åŠ¡å¹¶å‘æ‰§è¡Œ
        # ä¸å†æ¯æ¬¡åˆ›å»ºæ–°çš„çº¿ç¨‹æ± ï¼Œé¿å…ä¸²è¡Œæ‰§è¡Œ
        loop = asyncio.get_event_loop()
        logger.info(f"ğŸš€ [çº¿ç¨‹æ± ] æäº¤åˆ†æä»»åŠ¡åˆ°å…±äº«çº¿ç¨‹æ± : {task_id} - {request.stock_code}")
        result = await loop.run_in_executor(
            self._thread_pool,  # ä½¿ç”¨å…±äº«çº¿ç¨‹æ± 
            self._run_analysis_sync,
            task_id,
            user_id,
            request,
            progress_tracker
        )
        logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task_id}")
        return result

    def _run_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest,
        progress_tracker: Optional[RedisProgressTracker] = None
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æçš„å…·ä½“å®ç°"""
        try:
            # åœ¨çº¿ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
            from tradingagents.utils.logging_init import init_logging, get_logger
            init_logging()
            thread_logger = get_logger('analysis_thread')

            thread_logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æ: {task_id} - {request.stock_code}")
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æ: {task_id} - {request.stock_code}")

            # ğŸ”§ æ ¹æ® RedisProgressTracker çš„æ­¥éª¤æƒé‡è®¡ç®—å‡†ç¡®çš„è¿›åº¦
            # åŸºç¡€å‡†å¤‡é˜¶æ®µ (10%): 0.03 + 0.02 + 0.01 + 0.02 + 0.02 = 0.10
            # æ­¥éª¤ç´¢å¼• 0-4 å¯¹åº” 0-10%

            # å¼‚æ­¥æ›´æ–°è¿›åº¦ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨ï¼‰
            def update_progress_sync(progress: int, message: str, step: str):
                """åœ¨çº¿ç¨‹æ± ä¸­åŒæ­¥æ›´æ–°è¿›åº¦"""
                try:
                    # åŒæ—¶æ›´æ–° Redis è¿›åº¦è·Ÿè¸ªå™¨
                    if progress_tracker:
                        progress_tracker.update_progress({
                            "progress_percentage": progress,
                            "last_message": message
                        })

                    # ğŸ”¥ ä½¿ç”¨åŒæ­¥æ–¹å¼æ›´æ–°å†…å­˜å’Œ MongoDBï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                    # 1. æ›´æ–°å†…å­˜ä¸­çš„ä»»åŠ¡çŠ¶æ€ï¼ˆä½¿ç”¨æ–°äº‹ä»¶å¾ªç¯ï¼‰
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(
                            self.memory_manager.update_task_status(
                                task_id=task_id,
                                status=TaskStatus.RUNNING,
                                progress=progress,
                                message=message,
                                current_step=step
                            )
                        )
                    finally:
                        loop.close()

                    # 2. æ›´æ–° MongoDBï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çªï¼‰
                    from pymongo import MongoClient
                    from app.core.config import settings
                    from datetime import datetime

                    sync_client = MongoClient(settings.MONGO_URI)
                    sync_db = sync_client[settings.MONGO_DB]

                    sync_db.analysis_tasks.update_one(
                        {"task_id": task_id},
                        {
                            "$set": {
                                "progress": progress,
                                "current_step": step,
                                "message": message,
                                "updated_at": datetime.utcnow()
                            }
                        }
                    )
                    sync_client.close()

                except Exception as e:
                    logger.warning(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

            # é…ç½®é˜¶æ®µ - å¯¹åº”æ­¥éª¤3 "âš™ï¸ å‚æ•°è®¾ç½®" (6-8%)
            update_progress_sync(7, "âš™ï¸ é…ç½®åˆ†æå‚æ•°", "configuration")

            # ğŸ†• æ™ºèƒ½æ¨¡å‹é€‰æ‹©é€»è¾‘
            from app.services.model_capability_service import get_model_capability_service
            capability_service = get_model_capability_service()

            research_depth = request.parameters.research_depth if request.parameters else "æ ‡å‡†"

            # 1. æ£€æŸ¥å‰ç«¯æ˜¯å¦æŒ‡å®šäº†æ¨¡å‹
            if (request.parameters and
                hasattr(request.parameters, 'quick_analysis_model') and
                hasattr(request.parameters, 'deep_analysis_model') and
                request.parameters.quick_analysis_model and
                request.parameters.deep_analysis_model):

                # ä½¿ç”¨å‰ç«¯æŒ‡å®šçš„æ¨¡å‹
                quick_model = request.parameters.quick_analysis_model
                deep_model = request.parameters.deep_analysis_model

                logger.info(f"ğŸ“ [åˆ†ææœåŠ¡] ç”¨æˆ·æŒ‡å®šæ¨¡å‹: quick={quick_model}, deep={deep_model}")

                # éªŒè¯æ¨¡å‹æ˜¯å¦åˆé€‚
                validation = capability_service.validate_model_pair(
                    quick_model, deep_model, research_depth
                )

                if not validation["valid"]:
                    # è®°å½•è­¦å‘Š
                    for warning in validation["warnings"]:
                        logger.warning(warning)

                    # å¦‚æœæ¨¡å‹ä¸åˆé€‚ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ¨èæ¨¡å‹
                    logger.info(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°æ¨èæ¨¡å‹...")
                    quick_model, deep_model = capability_service.recommend_models_for_depth(
                        research_depth
                    )
                    logger.info(f"âœ… å·²åˆ‡æ¢: quick={quick_model}, deep={deep_model}")
                else:
                    # å³ä½¿éªŒè¯é€šè¿‡ï¼Œä¹Ÿè®°å½•è­¦å‘Šä¿¡æ¯
                    for warning in validation["warnings"]:
                        logger.info(warning)
                    logger.info(f"âœ… ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹éªŒè¯é€šè¿‡: quick={quick_model}, deep={deep_model}")

            else:
                # 2. è‡ªåŠ¨æ¨èæ¨¡å‹
                quick_model, deep_model = capability_service.recommend_models_for_depth(
                    research_depth
                )
                logger.info(f"ğŸ¤– è‡ªåŠ¨æ¨èæ¨¡å‹: quick={quick_model}, deep={deep_model}")

            # ğŸ”§ æ ¹æ®å¿«é€Ÿæ¨¡å‹å’Œæ·±åº¦æ¨¡å‹åˆ†åˆ«æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†å’Œ API URL
            quick_provider_info = get_provider_and_url_by_model_sync(quick_model)
            deep_provider_info = get_provider_and_url_by_model_sync(deep_model)

            quick_provider = quick_provider_info["provider"]
            deep_provider = deep_provider_info["provider"]
            quick_backend_url = quick_provider_info["backend_url"]
            deep_backend_url = deep_provider_info["backend_url"]

            logger.info(f"ğŸ” [ä¾›åº”å•†æŸ¥æ‰¾] å¿«é€Ÿæ¨¡å‹ {quick_model} å¯¹åº”çš„ä¾›åº”å•†: {quick_provider}")
            logger.info(f"ğŸ” [APIåœ°å€] å¿«é€Ÿæ¨¡å‹ä½¿ç”¨ backend_url: {quick_backend_url}")
            logger.info(f"ğŸ” [ä¾›åº”å•†æŸ¥æ‰¾] æ·±åº¦æ¨¡å‹ {deep_model} å¯¹åº”çš„ä¾›åº”å•†: {deep_provider}")
            logger.info(f"ğŸ” [APIåœ°å€] æ·±åº¦æ¨¡å‹ä½¿ç”¨ backend_url: {deep_backend_url}")

            # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹æ˜¯å¦æ¥è‡ªåŒä¸€ä¸ªå‚å®¶
            if quick_provider == deep_provider:
                logger.info(f"âœ… [ä¾›åº”å•†éªŒè¯] ä¸¤ä¸ªæ¨¡å‹æ¥è‡ªåŒä¸€å‚å®¶: {quick_provider}")
            else:
                logger.info(f"âœ… [æ··åˆæ¨¡å¼] å¿«é€Ÿæ¨¡å‹({quick_provider}) å’Œ æ·±åº¦æ¨¡å‹({deep_provider}) æ¥è‡ªä¸åŒå‚å®¶")

            # è·å–å¸‚åœºç±»å‹
            market_type = request.parameters.market_type if request.parameters else "Aè‚¡"
            logger.info(f"ğŸ“Š [å¸‚åœºç±»å‹] ä½¿ç”¨å¸‚åœºç±»å‹: {market_type}")

            # åˆ›å»ºåˆ†æé…ç½®ï¼ˆæ”¯æŒæ··åˆæ¨¡å¼ï¼‰
            config = create_analysis_config(
                research_depth=research_depth,
                selected_analysts=request.parameters.selected_analysts if request.parameters else ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=quick_provider,  # ä¸»è¦ä½¿ç”¨å¿«é€Ÿæ¨¡å‹çš„ä¾›åº”å•†
                market_type=market_type  # ä½¿ç”¨å‰ç«¯ä¼ é€’çš„å¸‚åœºç±»å‹
            )

            # ğŸ”§ æ·»åŠ æ··åˆæ¨¡å¼é…ç½®
            config["quick_provider"] = quick_provider
            config["deep_provider"] = deep_provider
            config["quick_backend_url"] = quick_backend_url
            config["deep_backend_url"] = deep_backend_url
            config["backend_url"] = quick_backend_url  # ä¿æŒå‘åå…¼å®¹

            # ğŸ” éªŒè¯é…ç½®ä¸­çš„æ¨¡å‹
            logger.info(f"ğŸ” [æ¨¡å‹éªŒè¯] é…ç½®ä¸­çš„å¿«é€Ÿæ¨¡å‹: {config.get('quick_think_llm')}")
            logger.info(f"ğŸ” [æ¨¡å‹éªŒè¯] é…ç½®ä¸­çš„æ·±åº¦æ¨¡å‹: {config.get('deep_think_llm')}")
            logger.info(f"ğŸ” [æ¨¡å‹éªŒè¯] é…ç½®ä¸­çš„LLMä¾›åº”å•†: {config.get('llm_provider')}")

            # åˆå§‹åŒ–åˆ†æå¼•æ“ - å¯¹åº”æ­¥éª¤4 "ğŸš€ å¯åŠ¨å¼•æ“" (8-10%)
            update_progress_sync(9, "ğŸš€ åˆå§‹åŒ–AIåˆ†æå¼•æ“", "engine_initialization")
            trading_graph = self._get_trading_graph(config)

            # ğŸ” éªŒè¯TradingGraphå®ä¾‹ä¸­çš„é…ç½®
            logger.info(f"ğŸ” [å¼•æ“éªŒè¯] TradingGraphé…ç½®ä¸­çš„å¿«é€Ÿæ¨¡å‹: {trading_graph.config.get('quick_think_llm')}")
            logger.info(f"ğŸ” [å¼•æ“éªŒè¯] TradingGraphé…ç½®ä¸­çš„æ·±åº¦æ¨¡å‹: {trading_graph.config.get('deep_think_llm')}")

            # å‡†å¤‡åˆ†ææ•°æ®
            start_time = datetime.now()

            # ğŸ”§ ä½¿ç”¨å‰ç«¯ä¼ é€’çš„åˆ†ææ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
            if request.parameters and hasattr(request.parameters, 'analysis_date') and request.parameters.analysis_date:
                # å‰ç«¯ä¼ é€’çš„æ˜¯ datetime å¯¹è±¡æˆ–å­—ç¬¦ä¸²
                if isinstance(request.parameters.analysis_date, datetime):
                    analysis_date = request.parameters.analysis_date.strftime("%Y-%m-%d")
                elif isinstance(request.parameters.analysis_date, str):
                    analysis_date = request.parameters.analysis_date
                else:
                    analysis_date = datetime.now().strftime("%Y-%m-%d")
                logger.info(f"ğŸ“… ä½¿ç”¨å‰ç«¯æŒ‡å®šçš„åˆ†ææ—¥æœŸ: {analysis_date}")
            else:
                analysis_date = datetime.now().strftime("%Y-%m-%d")
                logger.info(f"ğŸ“… ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºåˆ†ææ—¥æœŸ: {analysis_date}")

            # ğŸ”§ æ™ºèƒ½æ—¥æœŸèŒƒå›´å¤„ç†ï¼šè·å–æœ€è¿‘10å¤©çš„æ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†å‘¨æœ«/èŠ‚å‡æ—¥
            # è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿æ˜¯å‘¨æœ«æˆ–èŠ‚å‡æ—¥ï¼Œä¹Ÿèƒ½è·å–åˆ°æœ€åä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
            from tradingagents.utils.dataflow_utils import get_trading_date_range
            data_start_date, data_end_date = get_trading_date_range(analysis_date, lookback_days=10)

            logger.info(f"ğŸ“… åˆ†æç›®æ ‡æ—¥æœŸ: {analysis_date}")
            logger.info(f"ğŸ“… æ•°æ®æŸ¥è¯¢èŒƒå›´: {data_start_date} è‡³ {data_end_date} (æœ€è¿‘10å¤©)")
            logger.info(f"ğŸ’¡ è¯´æ˜: è·å–10å¤©æ•°æ®å¯è‡ªåŠ¨å¤„ç†å‘¨æœ«ã€èŠ‚å‡æ—¥å’Œæ•°æ®å»¶è¿Ÿé—®é¢˜")

            # å¼€å§‹åˆ†æ - è¿›åº¦10%ï¼Œå³å°†è¿›å…¥åˆ†æå¸ˆé˜¶æ®µ
            # æ³¨æ„ï¼šä¸è¦æ‰‹åŠ¨è®¾ç½®è¿‡é«˜çš„è¿›åº¦ï¼Œè®© graph_progress_callback æ¥æ›´æ–°å®é™…çš„åˆ†æè¿›åº¦
            update_progress_sync(10, "ğŸ¤– å¼€å§‹å¤šæ™ºèƒ½ä½“åä½œåˆ†æ", "agent_analysis")

            # å¯åŠ¨ä¸€ä¸ªå¼‚æ­¥ä»»åŠ¡æ¥æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
            import threading
            import time

            def simulate_progress():
                """æ¨¡æ‹ŸTradingAgentså†…éƒ¨è¿›åº¦"""
                try:
                    if not progress_tracker:
                        return

                    # åˆ†æå¸ˆé˜¶æ®µ - æ ¹æ®é€‰æ‹©çš„åˆ†æå¸ˆæ•°é‡åŠ¨æ€è°ƒæ•´
                    analysts = request.parameters.selected_analysts if request.parameters else ["market", "fundamentals"]

                    # æ¨¡æ‹Ÿåˆ†æå¸ˆæ‰§è¡Œ
                    for i, analyst in enumerate(analysts):
                        time.sleep(15)  # æ¯ä¸ªåˆ†æå¸ˆå¤§çº¦15ç§’
                        if analyst == "market":
                            progress_tracker.update_progress("ğŸ“Š å¸‚åœºåˆ†æå¸ˆæ­£åœ¨åˆ†æ")
                        elif analyst == "fundamentals":
                            progress_tracker.update_progress("ğŸ’¼ åŸºæœ¬é¢åˆ†æå¸ˆæ­£åœ¨åˆ†æ")
                        elif analyst == "news":
                            progress_tracker.update_progress("ğŸ“° æ–°é—»åˆ†æå¸ˆæ­£åœ¨åˆ†æ")
                        elif analyst == "social":
                            progress_tracker.update_progress("ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆæ­£åœ¨åˆ†æ")

                    # ç ”ç©¶å›¢é˜Ÿé˜¶æ®µ
                    time.sleep(10)
                    progress_tracker.update_progress("ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜æ„å»ºè®ºæ®")

                    time.sleep(8)
                    progress_tracker.update_progress("ğŸ» çœ‹è·Œç ”ç©¶å‘˜è¯†åˆ«é£é™©")

                    # è¾©è®ºé˜¶æ®µ - æ ¹æ®5ä¸ªçº§åˆ«ç¡®å®šè¾©è®ºè½®æ¬¡
                    research_depth = request.parameters.research_depth if request.parameters else "æ ‡å‡†"
                    if research_depth == "å¿«é€Ÿ":
                        debate_rounds = 1
                    elif research_depth == "åŸºç¡€":
                        debate_rounds = 1
                    elif research_depth == "æ ‡å‡†":
                        debate_rounds = 1
                    elif research_depth == "æ·±åº¦":
                        debate_rounds = 2
                    elif research_depth == "å…¨é¢":
                        debate_rounds = 3
                    else:
                        debate_rounds = 1  # é»˜è®¤

                    for round_num in range(debate_rounds):
                        time.sleep(12)
                        progress_tracker.update_progress(f"ğŸ¯ ç ”ç©¶è¾©è®º ç¬¬{round_num+1}è½®")

                    time.sleep(8)
                    progress_tracker.update_progress("ğŸ‘” ç ”ç©¶ç»ç†å½¢æˆå…±è¯†")

                    # äº¤æ˜“å‘˜é˜¶æ®µ
                    time.sleep(10)
                    progress_tracker.update_progress("ğŸ’¼ äº¤æ˜“å‘˜åˆ¶å®šç­–ç•¥")

                    # é£é™©ç®¡ç†é˜¶æ®µ
                    time.sleep(8)
                    progress_tracker.update_progress("ğŸ”¥ æ¿€è¿›é£é™©è¯„ä¼°")

                    time.sleep(6)
                    progress_tracker.update_progress("ğŸ›¡ï¸ ä¿å®ˆé£é™©è¯„ä¼°")

                    time.sleep(6)
                    progress_tracker.update_progress("âš–ï¸ ä¸­æ€§é£é™©è¯„ä¼°")

                    time.sleep(8)
                    progress_tracker.update_progress("ğŸ¯ é£é™©ç»ç†åˆ¶å®šç­–ç•¥")

                    # æœ€ç»ˆé˜¶æ®µ
                    time.sleep(5)
                    progress_tracker.update_progress("ğŸ“¡ ä¿¡å·å¤„ç†")

                except Exception as e:
                    logger.warning(f"âš ï¸ è¿›åº¦æ¨¡æ‹Ÿå¤±è´¥: {e}")

            # å¯åŠ¨è¿›åº¦æ¨¡æ‹Ÿçº¿ç¨‹
            progress_thread = threading.Thread(target=simulate_progress, daemon=True)
            progress_thread.start()

            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äºæ¥æ”¶ LangGraph çš„å®æ—¶è¿›åº¦
            # èŠ‚ç‚¹è¿›åº¦æ˜ å°„è¡¨ï¼ˆä¸ RedisProgressTracker çš„æ­¥éª¤æƒé‡å¯¹åº”ï¼‰
            node_progress_map = {
                # åˆ†æå¸ˆé˜¶æ®µ (10% â†’ 45%)
                "ğŸ“Š å¸‚åœºåˆ†æå¸ˆ": 27.5,      # 10% + 17.5% (å‡è®¾2ä¸ªåˆ†æå¸ˆ)
                "ğŸ’¼ åŸºæœ¬é¢åˆ†æå¸ˆ": 45,       # 10% + 35%
                "ğŸ“° æ–°é—»åˆ†æå¸ˆ": 27.5,       # å¦‚æœæœ‰3ä¸ªåˆ†æå¸ˆ
                "ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ": 27.5,   # å¦‚æœæœ‰4ä¸ªåˆ†æå¸ˆ
                # ç ”ç©¶è¾©è®ºé˜¶æ®µ (45% â†’ 70%)
                "ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜": 51.25,      # 45% + 6.25%
                "ğŸ» çœ‹è·Œç ”ç©¶å‘˜": 57.5,       # 45% + 12.5%
                "ğŸ‘” ç ”ç©¶ç»ç†": 70,           # 45% + 25%
                # äº¤æ˜“å‘˜é˜¶æ®µ (70% â†’ 78%)
                "ğŸ’¼ äº¤æ˜“å‘˜å†³ç­–": 78,         # 70% + 8%
                # é£é™©è¯„ä¼°é˜¶æ®µ (78% â†’ 93%)
                "ğŸ”¥ æ¿€è¿›é£é™©è¯„ä¼°": 81.75,    # 78% + 3.75%
                "ğŸ›¡ï¸ ä¿å®ˆé£é™©è¯„ä¼°": 85.5,    # 78% + 7.5%
                "âš–ï¸ ä¸­æ€§é£é™©è¯„ä¼°": 89.25,   # 78% + 11.25%
                "ğŸ¯ é£é™©ç»ç†": 93,           # 78% + 15%
                # æœ€ç»ˆé˜¶æ®µ (93% â†’ 100%)
                "ğŸ“Š ç”ŸæˆæŠ¥å‘Š": 97,           # 93% + 4%
            }

            def graph_progress_callback(message: str):
                """æ¥æ”¶ LangGraph çš„è¿›åº¦æ›´æ–°

                æ ¹æ®èŠ‚ç‚¹åç§°ç›´æ¥æ˜ å°„åˆ°è¿›åº¦ç™¾åˆ†æ¯”ï¼Œç¡®ä¿ä¸ RedisProgressTracker çš„æ­¥éª¤æƒé‡ä¸€è‡´
                æ³¨æ„ï¼šåªåœ¨è¿›åº¦å¢åŠ æ—¶æ›´æ–°ï¼Œé¿å…è¦†ç›– RedisProgressTracker çš„è™šæ‹Ÿæ­¥éª¤è¿›åº¦
                """
                try:
                    logger.info(f"ğŸ¯ğŸ¯ğŸ¯ [Graphè¿›åº¦å›è°ƒè¢«è°ƒç”¨] message={message}")
                    if not progress_tracker:
                        logger.warning(f"âš ï¸ progress_tracker ä¸º Noneï¼Œæ— æ³•æ›´æ–°è¿›åº¦")
                        return

                    # æŸ¥æ‰¾èŠ‚ç‚¹å¯¹åº”çš„è¿›åº¦ç™¾åˆ†æ¯”
                    progress_pct = node_progress_map.get(message)

                    if progress_pct is not None:
                        # è·å–å½“å‰è¿›åº¦ï¼ˆä½¿ç”¨ progress_data å±æ€§ï¼‰
                        current_progress = progress_tracker.progress_data.get('progress_percentage', 0)

                        # åªåœ¨è¿›åº¦å¢åŠ æ—¶æ›´æ–°ï¼Œé¿å…è¦†ç›–è™šæ‹Ÿæ­¥éª¤çš„è¿›åº¦
                        if int(progress_pct) > current_progress:
                            # æ›´æ–° Redis è¿›åº¦è·Ÿè¸ªå™¨
                            progress_tracker.update_progress({
                                'progress_percentage': int(progress_pct),
                                'last_message': message
                            })
                            logger.info(f"ğŸ“Š [Graphè¿›åº¦] è¿›åº¦å·²æ›´æ–°: {current_progress}% â†’ {int(progress_pct)}% - {message}")

                            # ğŸ”¥ åŒæ—¶æ›´æ–°å†…å­˜å’Œ MongoDB
                            try:
                                import asyncio
                                from datetime import datetime

                                # å°è¯•è·å–å½“å‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯
                                try:
                                    loop = asyncio.get_running_loop()
                                    # å¦‚æœåœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨ create_task
                                    asyncio.create_task(
                                        self._update_progress_async(task_id, int(progress_pct), message)
                                    )
                                    logger.debug(f"âœ… [Graphè¿›åº¦] å·²æäº¤å¼‚æ­¥æ›´æ–°ä»»åŠ¡: {int(progress_pct)}%")
                                except RuntimeError:
                                    # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨åŒæ­¥æ–¹å¼æ›´æ–° MongoDB
                                    from pymongo import MongoClient
                                    from app.core.config import settings

                                    # åˆ›å»ºåŒæ­¥ MongoDB å®¢æˆ·ç«¯
                                    sync_client = MongoClient(settings.MONGO_URI)
                                    sync_db = sync_client[settings.MONGO_DB]

                                    # åŒæ­¥æ›´æ–° MongoDB
                                    sync_db.analysis_tasks.update_one(
                                        {"task_id": task_id},
                                        {
                                            "$set": {
                                                "progress": int(progress_pct),
                                                "current_step": message,
                                                "message": message,
                                                "updated_at": datetime.utcnow()
                                            }
                                        }
                                    )
                                    sync_client.close()

                                    # å¼‚æ­¥æ›´æ–°å†…å­˜ï¼ˆåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯ï¼‰
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                    try:
                                        loop.run_until_complete(
                                            self.memory_manager.update_task_status(
                                                task_id=task_id,
                                                status=TaskStatus.RUNNING,
                                                progress=int(progress_pct),
                                                message=message,
                                                current_step=message
                                            )
                                        )
                                    finally:
                                        loop.close()

                                    logger.debug(f"âœ… [Graphè¿›åº¦] å·²åŒæ­¥æ›´æ–°å†…å­˜å’ŒMongoDB: {int(progress_pct)}%")
                            except Exception as sync_err:
                                logger.warning(f"âš ï¸ [Graphè¿›åº¦] åŒæ­¥æ›´æ–°å¤±è´¥: {sync_err}")
                        else:
                            # è¿›åº¦æ²¡æœ‰å¢åŠ ï¼Œåªæ›´æ–°æ¶ˆæ¯
                            progress_tracker.update_progress({
                                'last_message': message
                            })
                            logger.info(f"ğŸ“Š [Graphè¿›åº¦] è¿›åº¦æœªå˜åŒ–({current_progress}% >= {int(progress_pct)}%)ï¼Œä»…æ›´æ–°æ¶ˆæ¯: {message}")
                    else:
                        # æœªçŸ¥èŠ‚ç‚¹ï¼Œåªæ›´æ–°æ¶ˆæ¯
                        logger.warning(f"âš ï¸ [Graphè¿›åº¦] æœªçŸ¥èŠ‚ç‚¹: {message}ï¼Œä»…æ›´æ–°æ¶ˆæ¯")
                        progress_tracker.update_progress({
                            'last_message': message
                        })

                except Exception as e:
                    logger.error(f"âŒ Graphè¿›åº¦å›è°ƒå¤±è´¥: {e}", exc_info=True)

            logger.info(f"ğŸš€ å‡†å¤‡è°ƒç”¨ trading_graph.propagateï¼Œprogress_callback={graph_progress_callback}")

            # æ‰§è¡Œå®é™…åˆ†æï¼Œä¼ é€’è¿›åº¦å›è°ƒå’Œtask_id
            state, decision = trading_graph.propagate(
                request.stock_code,
                analysis_date,
                progress_callback=graph_progress_callback,
                task_id=task_id
            )

            logger.info(f"âœ… trading_graph.propagate æ‰§è¡Œå®Œæˆ")

            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥decisionçš„ç»“æ„
            logger.info(f"ğŸ” [DEBUG] Decisionç±»å‹: {type(decision)}")
            logger.info(f"ğŸ” [DEBUG] Decisionå†…å®¹: {decision}")
            if isinstance(decision, dict):
                logger.info(f"ğŸ” [DEBUG] Decisioné”®: {list(decision.keys())}")
            elif hasattr(decision, '__dict__'):
                logger.info(f"ğŸ” [DEBUG] Decisionå±æ€§: {list(vars(decision).keys())}")

            # å¤„ç†ç»“æœ
            if progress_tracker:
                progress_tracker.update_progress("ğŸ“Š å¤„ç†åˆ†æç»“æœ")
            update_progress_sync(90, "å¤„ç†åˆ†æç»“æœ...", "result_processing")

            execution_time = (datetime.now() - start_time).total_seconds()

            # ä»stateä¸­æå–reportså­—æ®µ
            reports = {}
            try:
                # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æŠ¥å‘Šå­—æ®µ
                report_fields = [
                    'market_report',
                    'sentiment_report',
                    'news_report',
                    'fundamentals_report',
                    'investment_plan',
                    'trader_investment_plan',
                    'final_trade_decision'
                ]

                # ä»stateä¸­æå–æŠ¥å‘Šå†…å®¹
                for field in report_fields:
                    if hasattr(state, field):
                        value = getattr(state, field, "")
                    elif isinstance(state, dict) and field in state:
                        value = state[field]
                    else:
                        value = ""

                    if isinstance(value, str) and len(value.strip()) > 10:  # åªä¿å­˜æœ‰å®é™…å†…å®¹çš„æŠ¥å‘Š
                        reports[field] = value.strip()
                        logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: {field} - é•¿åº¦: {len(value.strip())}")
                    else:
                        logger.debug(f"âš ï¸ [REPORTS] è·³è¿‡æŠ¥å‘Š: {field} - å†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­")

                # å¤„ç†ç ”ç©¶å›¢é˜Ÿè¾©è®ºçŠ¶æ€æŠ¥å‘Š
                if hasattr(state, 'investment_debate_state') or (isinstance(state, dict) and 'investment_debate_state' in state):
                    debate_state = getattr(state, 'investment_debate_state', None) if hasattr(state, 'investment_debate_state') else state.get('investment_debate_state')
                    if debate_state:
                        # æå–å¤šå¤´ç ”ç©¶å‘˜å†å²
                        if hasattr(debate_state, 'bull_history'):
                            bull_content = getattr(debate_state, 'bull_history', "")
                        elif isinstance(debate_state, dict) and 'bull_history' in debate_state:
                            bull_content = debate_state['bull_history']
                        else:
                            bull_content = ""

                        if bull_content and len(bull_content.strip()) > 10:
                            reports['bull_researcher'] = bull_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: bull_researcher - é•¿åº¦: {len(bull_content.strip())}")

                        # æå–ç©ºå¤´ç ”ç©¶å‘˜å†å²
                        if hasattr(debate_state, 'bear_history'):
                            bear_content = getattr(debate_state, 'bear_history', "")
                        elif isinstance(debate_state, dict) and 'bear_history' in debate_state:
                            bear_content = debate_state['bear_history']
                        else:
                            bear_content = ""

                        if bear_content and len(bear_content.strip()) > 10:
                            reports['bear_researcher'] = bear_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: bear_researcher - é•¿åº¦: {len(bear_content.strip())}")

                        # æå–ç ”ç©¶ç»ç†å†³ç­–
                        if hasattr(debate_state, 'judge_decision'):
                            decision_content = getattr(debate_state, 'judge_decision', "")
                        elif isinstance(debate_state, dict) and 'judge_decision' in debate_state:
                            decision_content = debate_state['judge_decision']
                        else:
                            decision_content = str(debate_state)

                        if decision_content and len(decision_content.strip()) > 10:
                            reports['research_team_decision'] = decision_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: research_team_decision - é•¿åº¦: {len(decision_content.strip())}")

                # å¤„ç†é£é™©ç®¡ç†å›¢é˜Ÿè¾©è®ºçŠ¶æ€æŠ¥å‘Š
                if hasattr(state, 'risk_debate_state') or (isinstance(state, dict) and 'risk_debate_state' in state):
                    risk_state = getattr(state, 'risk_debate_state', None) if hasattr(state, 'risk_debate_state') else state.get('risk_debate_state')
                    if risk_state:
                        # æå–æ¿€è¿›åˆ†æå¸ˆå†å²
                        if hasattr(risk_state, 'risky_history'):
                            risky_content = getattr(risk_state, 'risky_history', "")
                        elif isinstance(risk_state, dict) and 'risky_history' in risk_state:
                            risky_content = risk_state['risky_history']
                        else:
                            risky_content = ""

                        if risky_content and len(risky_content.strip()) > 10:
                            reports['risky_analyst'] = risky_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: risky_analyst - é•¿åº¦: {len(risky_content.strip())}")

                        # æå–ä¿å®ˆåˆ†æå¸ˆå†å²
                        if hasattr(risk_state, 'safe_history'):
                            safe_content = getattr(risk_state, 'safe_history', "")
                        elif isinstance(risk_state, dict) and 'safe_history' in risk_state:
                            safe_content = risk_state['safe_history']
                        else:
                            safe_content = ""

                        if safe_content and len(safe_content.strip()) > 10:
                            reports['safe_analyst'] = safe_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: safe_analyst - é•¿åº¦: {len(safe_content.strip())}")

                        # æå–ä¸­æ€§åˆ†æå¸ˆå†å²
                        if hasattr(risk_state, 'neutral_history'):
                            neutral_content = getattr(risk_state, 'neutral_history', "")
                        elif isinstance(risk_state, dict) and 'neutral_history' in risk_state:
                            neutral_content = risk_state['neutral_history']
                        else:
                            neutral_content = ""

                        if neutral_content and len(neutral_content.strip()) > 10:
                            reports['neutral_analyst'] = neutral_content.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: neutral_analyst - é•¿åº¦: {len(neutral_content.strip())}")

                        # æå–æŠ•èµ„ç»„åˆç»ç†å†³ç­–
                        if hasattr(risk_state, 'judge_decision'):
                            risk_decision = getattr(risk_state, 'judge_decision', "")
                        elif isinstance(risk_state, dict) and 'judge_decision' in risk_state:
                            risk_decision = risk_state['judge_decision']
                        else:
                            risk_decision = str(risk_state)

                        if risk_decision and len(risk_decision.strip()) > 10:
                            reports['risk_management_decision'] = risk_decision.strip()
                            logger.info(f"ğŸ“Š [REPORTS] æå–æŠ¥å‘Š: risk_management_decision - é•¿åº¦: {len(risk_decision.strip())}")

                logger.info(f"ğŸ“Š [REPORTS] ä»stateä¸­æå–åˆ° {len(reports)} ä¸ªæŠ¥å‘Š: {list(reports.keys())}")

            except Exception as e:
                logger.warning(f"âš ï¸ æå–reportsæ—¶å‡ºé”™: {e}")
                # é™çº§åˆ°ä»detailed_analysisæå–
                try:
                    if isinstance(decision, dict):
                        for key, value in decision.items():
                            if isinstance(value, str) and len(value) > 50:
                                reports[key] = value
                        logger.info(f"ğŸ“Š é™çº§ï¼šä»decisionä¸­æå–åˆ° {len(reports)} ä¸ªæŠ¥å‘Š")
                except Exception as fallback_error:
                    logger.warning(f"âš ï¸ é™çº§æå–ä¹Ÿå¤±è´¥: {fallback_error}")

            # ğŸ”¥ æ ¼å¼åŒ–decisionæ•°æ®ï¼ˆå‚è€ƒwebç›®å½•çš„å®ç°ï¼‰
            formatted_decision = {}
            try:
                if isinstance(decision, dict):
                    # å¤„ç†ç›®æ ‡ä»·æ ¼
                    target_price = decision.get('target_price')
                    if target_price is not None and target_price != 'N/A':
                        try:
                            if isinstance(target_price, str):
                                # ç§»é™¤è´§å¸ç¬¦å·å’Œç©ºæ ¼
                                clean_price = target_price.replace('$', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                                target_price = float(clean_price) if clean_price and clean_price != 'None' else None
                            elif isinstance(target_price, (int, float)):
                                target_price = float(target_price)
                            else:
                                target_price = None
                        except (ValueError, TypeError):
                            target_price = None
                    else:
                        target_price = None

                    # å°†è‹±æ–‡æŠ•èµ„å»ºè®®è½¬æ¢ä¸ºä¸­æ–‡
                    action_translation = {
                        'BUY': 'ä¹°å…¥',
                        'SELL': 'å–å‡º',
                        'HOLD': 'æŒæœ‰',
                        'buy': 'ä¹°å…¥',
                        'sell': 'å–å‡º',
                        'hold': 'æŒæœ‰'
                    }
                    action = decision.get('action', 'æŒæœ‰')
                    chinese_action = action_translation.get(action, action)

                    formatted_decision = {
                        'action': chinese_action,
                        'confidence': decision.get('confidence', 0.5),
                        'risk_score': decision.get('risk_score', 0.3),
                        'target_price': target_price,
                        'reasoning': decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')
                    }

                    logger.info(f"ğŸ¯ [DEBUG] æ ¼å¼åŒ–åçš„decision: {formatted_decision}")
                else:
                    # å¤„ç†å…¶ä»–ç±»å‹
                    formatted_decision = {
                        'action': 'æŒæœ‰',
                        'confidence': 0.5,
                        'risk_score': 0.3,
                        'target_price': None,
                        'reasoning': 'æš‚æ— åˆ†ææ¨ç†'
                    }
                    logger.warning(f"âš ï¸ Decisionä¸æ˜¯å­—å…¸ç±»å‹: {type(decision)}")
            except Exception as e:
                logger.error(f"âŒ æ ¼å¼åŒ–decisionå¤±è´¥: {e}")
                formatted_decision = {
                    'action': 'æŒæœ‰',
                    'confidence': 0.5,
                    'risk_score': 0.3,
                    'target_price': None,
                    'reasoning': 'æš‚æ— åˆ†ææ¨ç†'
                }

            # ğŸ”¥ æŒ‰ç…§webç›®å½•çš„æ–¹å¼ç”Ÿæˆsummaryå’Œrecommendation
            summary = ""
            recommendation = ""

            # 1. ä¼˜å…ˆä»reportsä¸­çš„final_trade_decisionæå–summaryï¼ˆä¸webç›®å½•ä¿æŒä¸€è‡´ï¼‰
            if isinstance(reports, dict) and 'final_trade_decision' in reports:
                final_decision_content = reports['final_trade_decision']
                if isinstance(final_decision_content, str) and len(final_decision_content) > 50:
                    # æå–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼ˆä¸webç›®å½•å®Œå…¨ä¸€è‡´ï¼‰
                    summary = final_decision_content[:200].replace('#', '').replace('*', '').strip()
                    if len(final_decision_content) > 200:
                        summary += "..."
                    logger.info(f"ğŸ“ [SUMMARY] ä»final_trade_decisionæå–æ‘˜è¦: {len(summary)}å­—ç¬¦")

            # 2. å¦‚æœæ²¡æœ‰final_trade_decisionï¼Œä»stateä¸­æå–
            if not summary and isinstance(state, dict):
                final_decision = state.get('final_trade_decision', '')
                if isinstance(final_decision, str) and len(final_decision) > 50:
                    summary = final_decision[:200].replace('#', '').replace('*', '').strip()
                    if len(final_decision) > 200:
                        summary += "..."
                    logger.info(f"ğŸ“ [SUMMARY] ä»state.final_trade_decisionæå–æ‘˜è¦: {len(summary)}å­—ç¬¦")

            # 3. ç”Ÿæˆrecommendationï¼ˆä»decisionçš„reasoningï¼‰
            if isinstance(formatted_decision, dict):
                action = formatted_decision.get('action', 'æŒæœ‰')
                target_price = formatted_decision.get('target_price')
                reasoning = formatted_decision.get('reasoning', '')

                # ç”ŸæˆæŠ•èµ„å»ºè®®
                recommendation = f"æŠ•èµ„å»ºè®®ï¼š{action}ã€‚"
                if target_price:
                    recommendation += f"ç›®æ ‡ä»·æ ¼ï¼š{target_price}å…ƒã€‚"
                if reasoning:
                    recommendation += f"å†³ç­–ä¾æ®ï¼š{reasoning}"
                logger.info(f"ğŸ’¡ [RECOMMENDATION] ç”ŸæˆæŠ•èµ„å»ºè®®: {len(recommendation)}å­—ç¬¦")

            # 4. å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä»å…¶ä»–æŠ¥å‘Šä¸­æå–
            if not summary and isinstance(reports, dict):
                # å°è¯•ä»å…¶ä»–æŠ¥å‘Šä¸­æå–æ‘˜è¦
                for report_name, content in reports.items():
                    if isinstance(content, str) and len(content) > 100:
                        summary = content[:200].replace('#', '').replace('*', '').strip()
                        if len(content) > 200:
                            summary += "..."
                        logger.info(f"ğŸ“ [SUMMARY] ä»{report_name}æå–æ‘˜è¦: {len(summary)}å­—ç¬¦")
                        break

            # 5. æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            if not summary:
                summary = f"å¯¹{request.stock_code}çš„åˆ†æå·²å®Œæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šã€‚"
                logger.warning(f"âš ï¸ [SUMMARY] ä½¿ç”¨å¤‡ç”¨æ‘˜è¦")

            if not recommendation:
                recommendation = f"è¯·å‚è€ƒè¯¦ç»†åˆ†ææŠ¥å‘Šåšå‡ºæŠ•èµ„å†³ç­–ã€‚"
                logger.warning(f"âš ï¸ [RECOMMENDATION] ä½¿ç”¨å¤‡ç”¨å»ºè®®")

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = decision.get('model_info', 'Unknown') if isinstance(decision, dict) else 'Unknown'

            # æ„å»ºç»“æœ
            result = {
                "analysis_id": str(uuid.uuid4()),
                "stock_code": request.stock_code,
                "stock_symbol": request.stock_code,  # æ·»åŠ stock_symbolå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                "analysis_date": analysis_date,
                "summary": summary,
                "recommendation": recommendation,
                "confidence_score": formatted_decision.get("confidence", 0.0) if isinstance(formatted_decision, dict) else 0.0,
                "risk_level": "ä¸­ç­‰",  # å¯ä»¥æ ¹æ®risk_scoreè®¡ç®—
                "key_points": [],  # å¯ä»¥ä»reasoningä¸­æå–å…³é”®ç‚¹
                "detailed_analysis": decision,
                "execution_time": execution_time,
                "tokens_used": decision.get("tokens_used", 0) if isinstance(decision, dict) else 0,
                "state": state,
                # æ·»åŠ åˆ†æå¸ˆä¿¡æ¯
                "analysts": request.parameters.selected_analysts if request.parameters else [],
                "research_depth": request.parameters.research_depth if request.parameters else "å¿«é€Ÿ",
                # æ·»åŠ æå–çš„æŠ¥å‘Šå†…å®¹
                "reports": reports,
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ ¼å¼åŒ–åçš„decisionå­—æ®µï¼
                "decision": formatted_decision,
                # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
                "model_info": model_info,
                # ğŸ†• æ€§èƒ½æŒ‡æ ‡æ•°æ®
                "performance_metrics": state.get("performance_metrics", {}) if isinstance(state, dict) else {}
            }

            logger.info(f"âœ… [çº¿ç¨‹æ± ] åˆ†æå®Œæˆ: {task_id} - è€—æ—¶{execution_time:.2f}ç§’")

            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥è¿”å›çš„resultç»“æ„
            logger.info(f"ğŸ” [DEBUG] è¿”å›resultçš„é”®: {list(result.keys())}")
            logger.info(f"ğŸ” [DEBUG] è¿”å›resultä¸­æœ‰decision: {bool(result.get('decision'))}")
            if result.get('decision'):
                decision = result['decision']
                logger.info(f"ğŸ” [DEBUG] è¿”å›decisionå†…å®¹: {decision}")

            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] åˆ†ææ‰§è¡Œå¤±è´¥: {task_id} - {e}")

            # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ä¸ºç”¨æˆ·å‹å¥½çš„æç¤º
            from ..utils.error_formatter import ErrorFormatter

            # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
            error_context = {}
            if request and hasattr(request, 'parameters') and request.parameters:
                if hasattr(request.parameters, 'quick_model'):
                    error_context['model'] = request.parameters.quick_model
                if hasattr(request.parameters, 'deep_model'):
                    error_context['model'] = request.parameters.deep_model

            # æ ¼å¼åŒ–é”™è¯¯
            formatted_error = ErrorFormatter.format_error(str(e), error_context)

            # æ„å»ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            user_friendly_error = (
                f"{formatted_error['title']}\n\n"
                f"{formatted_error['message']}\n\n"
                f"ğŸ’¡ {formatted_error['suggestion']}"
            )

            # æŠ›å‡ºåŒ…å«å‹å¥½é”™è¯¯ä¿¡æ¯çš„å¼‚å¸¸
            raise Exception(user_friendly_error) from e

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        logger.info(f"ğŸ” æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€: {task_id}")
        logger.info(f"ğŸ” å½“å‰æœåŠ¡å®ä¾‹ID: {id(self)}")
        logger.info(f"ğŸ” å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(self.memory_manager)}")

        # å¼ºåˆ¶ä½¿ç”¨å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼‰
        global_memory_manager = get_memory_state_manager()
        logger.info(f"ğŸ” å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹ID: {id(global_memory_manager)}")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await global_memory_manager.get_statistics()
        logger.info(f"ğŸ“Š å†…å­˜ä¸­ä»»åŠ¡ç»Ÿè®¡: {stats}")

        result = await global_memory_manager.get_task_dict(task_id)
        if result:
            logger.info(f"âœ… æ‰¾åˆ°ä»»åŠ¡: {task_id} - çŠ¶æ€: {result.get('status')}")

            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ä»å†…å­˜è·å–çš„result_data
            result_data = result.get('result_data')
            logger.debug(f"ğŸ” [GET_STATUS] result_dataå­˜åœ¨: {bool(result_data)}")
            if result_data:
                logger.debug(f"ğŸ” [GET_STATUS] result_dataé”®: {list(result_data.keys())}")
                logger.debug(f"ğŸ” [GET_STATUS] result_dataä¸­æœ‰decision: {bool(result_data.get('decision'))}")
                if result_data.get('decision'):
                    logger.debug(f"ğŸ” [GET_STATUS] decisionå†…å®¹: {result_data['decision']}")
            else:
                logger.debug(f"ğŸ” [GET_STATUS] result_dataä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼ˆä»»åŠ¡è¿è¡Œä¸­ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼‰")

            # ä¼˜å…ˆä»Redisè·å–è¯¦ç»†è¿›åº¦ä¿¡æ¯
            redis_progress = get_progress_by_id(task_id)
            if redis_progress:
                logger.info(f"ğŸ“Š [Redisè¿›åº¦] è·å–åˆ°è¯¦ç»†è¿›åº¦: {task_id}")

                # ä» steps æ•°ç»„ä¸­æå–å½“å‰æ­¥éª¤çš„åç§°å’Œæè¿°
                current_step_index = redis_progress.get('current_step', 0)
                steps = redis_progress.get('steps', [])
                current_step_name = redis_progress.get('current_step_name', '')
                current_step_description = redis_progress.get('current_step_description', '')

                # å¦‚æœ Redis ä¸­çš„åç§°/æè¿°ä¸ºç©ºï¼Œä» steps æ•°ç»„ä¸­æå–
                if not current_step_name and steps and 0 <= current_step_index < len(steps):
                    current_step_info = steps[current_step_index]
                    current_step_name = current_step_info.get('name', '')
                    current_step_description = current_step_info.get('description', '')
                    logger.info(f"ğŸ“‹ ä»stepsæ•°ç»„æå–å½“å‰æ­¥éª¤ä¿¡æ¯: index={current_step_index}, name={current_step_name}")

                # åˆå¹¶Redisè¿›åº¦æ•°æ®
                result.update({
                    'progress': redis_progress.get('progress_percentage', result.get('progress', 0)),
                    'current_step': current_step_index,  # ä½¿ç”¨ç´¢å¼•è€Œä¸æ˜¯åç§°
                    'current_step_name': current_step_name,  # æ­¥éª¤åç§°
                    'current_step_description': current_step_description,  # æ­¥éª¤æè¿°
                    'message': redis_progress.get('last_message', result.get('message', '')),
                    'elapsed_time': redis_progress.get('elapsed_time', 0),
                    'remaining_time': redis_progress.get('remaining_time', 0),
                    'estimated_total_time': redis_progress.get('estimated_total_time', result.get('estimated_duration', 300)),  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨Redisä¸­çš„é¢„ä¼°æ€»æ—¶é•¿
                    'steps': steps,
                    'start_time': result.get('start_time'),  # ä¿æŒåŸæœ‰æ ¼å¼
                    'last_update': redis_progress.get('last_update', result.get('start_time'))
                })
            else:
                # å¦‚æœRedisä¸­æ²¡æœ‰ï¼Œå°è¯•ä»å†…å­˜ä¸­çš„è¿›åº¦è·Ÿè¸ªå™¨è·å–
                if task_id in self._progress_trackers:
                    progress_tracker = self._progress_trackers[task_id]
                    progress_data = progress_tracker.to_dict()

                    # åˆå¹¶è¿›åº¦è·Ÿè¸ªå™¨çš„è¯¦ç»†ä¿¡æ¯
                    result.update({
                        'progress': progress_data['progress'],
                        'current_step': progress_data['current_step'],
                        'message': progress_data['message'],
                        'elapsed_time': progress_data['elapsed_time'],
                        'remaining_time': progress_data['remaining_time'],
                        'estimated_total_time': progress_data.get('estimated_total_time', 0),
                        'steps': progress_data['steps'],
                        'start_time': progress_data['start_time'],
                        'last_update': progress_data['last_update']
                    })
                    logger.info(f"ğŸ“Š åˆå¹¶å†…å­˜è¿›åº¦è·Ÿè¸ªå™¨æ•°æ®: {task_id}")
                else:
                    logger.info(f"âš ï¸ æœªæ‰¾åˆ°è¿›åº¦ä¿¡æ¯: {task_id}")
        else:
            logger.warning(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")

        return result

    async def list_all_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ï¼ˆä¸é™ç”¨æˆ·ï¼‰
        - åˆå¹¶å†…å­˜å’Œ MongoDB æ•°æ®
        - æŒ‰å¼€å§‹æ—¶é—´å€’åºæ’åˆ—
        """
        try:
            task_status = None
            if status:
                try:
                    status_mapping = {
                        "processing": "running",
                        "pending": "pending",
                        "completed": "completed",
                        "failed": "failed",
                        "cancelled": "cancelled"
                    }
                    mapped_status = status_mapping.get(status, status)
                    task_status = TaskStatus(mapped_status)
                except ValueError:
                    logger.warning(f"âš ï¸ [Tasks] æ— æ•ˆçš„çŠ¶æ€å€¼: {status}")
                    task_status = None

            # 1) ä»å†…å­˜è¯»å–æ‰€æœ‰ä»»åŠ¡
            logger.info(f"ğŸ“‹ [Tasks] å‡†å¤‡ä»å†…å­˜è¯»å–æ‰€æœ‰ä»»åŠ¡: status={status}, limit={limit}, offset={offset}")
            tasks_in_mem = await self.memory_manager.list_all_tasks(
                status=task_status,
                limit=limit * 2,
                offset=0
            )
            logger.info(f"ğŸ“‹ [Tasks] å†…å­˜è¿”å›æ•°é‡: {len(tasks_in_mem)}")

            # 2) ä» MongoDB è¯»å–ä»»åŠ¡
            db = get_mongo_db()
            collection = db["analysis_tasks"]

            query = {}
            if task_status:
                query["status"] = task_status.value

            count = await collection.count_documents(query)
            logger.info(f"ğŸ“‹ [Tasks] MongoDB ä»»åŠ¡æ€»æ•°: {count}")

            cursor = collection.find(query).sort("start_time", -1).limit(limit * 2)
            tasks_from_db = []
            async for doc in cursor:
                doc.pop("_id", None)
                tasks_from_db.append(doc)

            logger.info(f"ğŸ“‹ [Tasks] MongoDB è¿”å›æ•°é‡: {len(tasks_from_db)}")

            # 3) åˆå¹¶ä»»åŠ¡ï¼ˆå†…å­˜ä¼˜å…ˆï¼‰
            task_dict = {}

            # å…ˆæ·»åŠ  MongoDB ä¸­çš„ä»»åŠ¡
            for task in tasks_from_db:
                task_id = task.get("task_id")
                if task_id:
                    task_dict[task_id] = task

            # å†æ·»åŠ å†…å­˜ä¸­çš„ä»»åŠ¡ï¼ˆè¦†ç›– MongoDB ä¸­çš„åŒåä»»åŠ¡ï¼‰
            for task in tasks_in_mem:
                task_id = task.get("task_id")
                if task_id:
                    task_dict[task_id] = task

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åº
            merged_tasks = list(task_dict.values())
            merged_tasks.sort(key=lambda x: x.get('start_time', ''), reverse=True)

            # åˆ†é¡µ
            results = merged_tasks[offset:offset + limit]

            # ä¸ºç»“æœè¡¥é½è‚¡ç¥¨åç§°
            results = self._enrich_stock_names(results)
            logger.info(f"ğŸ“‹ [Tasks] åˆå¹¶åè¿”å›æ•°é‡: {len(results)} (å†…å­˜: {len(tasks_in_mem)}, MongoDB: {count})")
            return results
        except Exception as outer_e:
            logger.error(f"âŒ list_all_tasks å¤–å±‚å¼‚å¸¸: {outer_e}", exc_info=True)
            return []

    async def list_user_tasks(
        self,
        user_id: str,
        status: Optional[str] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨
        - å¯¹äº processing çŠ¶æ€ï¼šä¼˜å…ˆä»å†…å­˜è¯»å–ï¼ˆå®æ—¶è¿›åº¦ï¼‰
        - å¯¹äº completed/failed/all çŠ¶æ€ï¼šåˆå¹¶å†…å­˜å’Œ MongoDB æ•°æ®
        """
        try:
            task_status = None
            if status:
                try:
                    # å‰ç«¯ä¼ é€’çš„æ˜¯ "processing"ï¼Œä½† TaskStatus ä½¿ç”¨çš„æ˜¯ "running"
                    # éœ€è¦åšæ˜ å°„è½¬æ¢
                    status_mapping = {
                        "processing": "running",  # å‰ç«¯ä½¿ç”¨ processingï¼Œå†…å­˜ä½¿ç”¨ running
                        "pending": "pending",
                        "completed": "completed",
                        "failed": "failed",
                        "cancelled": "cancelled"
                    }
                    mapped_status = status_mapping.get(status, status)
                    task_status = TaskStatus(mapped_status)
                except ValueError:
                    logger.warning(f"âš ï¸ [Tasks] æ— æ•ˆçš„çŠ¶æ€å€¼: {status}")
                    task_status = None

            # 1) ä»å†…å­˜è¯»å–ä»»åŠ¡
            logger.info(f"ğŸ“‹ [Tasks] å‡†å¤‡ä»å†…å­˜è¯»å–ä»»åŠ¡: user_id={user_id}, status={status} (mapped to {task_status}), limit={limit}, offset={offset}")
            tasks_in_mem = await self.memory_manager.list_user_tasks(
                user_id=user_id,
                status=task_status,
                limit=limit * 2,  # å¤šè¯»ä¸€äº›ï¼Œåé¢åˆå¹¶å»é‡
                offset=0  # å†…å­˜ä¸­çš„ä»»åŠ¡ä¸å¤šï¼Œå…¨éƒ¨è¯»å–
            )
            logger.info(f"ğŸ“‹ [Tasks] å†…å­˜è¿”å›æ•°é‡: {len(tasks_in_mem)}")

            # 2) ğŸ”§ å¯¹äº processing/running çŠ¶æ€ï¼Œéœ€è¦åˆå¹¶ MongoDB æ•°æ®ä»¥è·å–æœ€æ–°è¿›åº¦
            # å› ä¸º graph_progress_callback å¯èƒ½ç›´æ¥æ›´æ–°äº† MongoDBï¼Œè€Œå†…å­˜æ•°æ®å¯èƒ½æ˜¯æ—§çš„

            # 3) ä» MongoDB è¯»å–å†å²ä»»åŠ¡ï¼ˆç”¨äºåˆå¹¶æˆ–å…œåº•ï¼‰
            logger.info(f"ğŸ“‹ [Tasks] ä» MongoDB è¯»å–å†å²ä»»åŠ¡")
            mongo_tasks: List[Dict[str, Any]] = []
            count = 0
            try:
                db = get_mongo_db()

                # user_id å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ– ObjectIdï¼Œåšå…¼å®¹
                uid_candidates: List[Any] = [user_id]

                # ç‰¹æ®Šå¤„ç† admin ç”¨æˆ·
                if str(user_id) == 'admin':
                    # admin ç”¨æˆ·ï¼šæ·»åŠ å›ºå®šçš„ ObjectId å’Œå­—ç¬¦ä¸²å½¢å¼
                    try:
                        from bson import ObjectId
                        admin_oid_str = '507f1f77bcf86cd799439011'
                        uid_candidates.append(ObjectId(admin_oid_str))
                        uid_candidates.append(admin_oid_str)  # å…¼å®¹å­—ç¬¦ä¸²å­˜å‚¨
                        logger.info(f"ğŸ“‹ [Tasks] adminç”¨æˆ·æŸ¥è¯¢ï¼Œå€™é€‰ID: ['admin', ObjectId('{admin_oid_str}'), '{admin_oid_str}']")
                    except Exception as e:
                        logger.warning(f"âš ï¸ [Tasks] adminç”¨æˆ·ObjectIdåˆ›å»ºå¤±è´¥: {e}")
                else:
                    # æ™®é€šç”¨æˆ·ï¼šå°è¯•è½¬æ¢ä¸º ObjectId
                    try:
                        from bson import ObjectId
                        uid_candidates.append(ObjectId(user_id))
                        logger.debug(f"ğŸ“‹ [Tasks] ç”¨æˆ·IDå·²è½¬æ¢ä¸ºObjectId: {user_id}")
                    except Exception as conv_err:
                        logger.warning(f"âš ï¸ [Tasks] ç”¨æˆ·IDè½¬æ¢ObjectIdå¤±è´¥ï¼ŒæŒ‰å­—ç¬¦ä¸²åŒ¹é…: {conv_err}")

                # å…¼å®¹ user_id ä¸ user ä¸¤ç§å­—æ®µå
                base_condition = {"$in": uid_candidates}
                or_conditions: List[Dict[str, Any]] = [
                    {"user_id": base_condition},
                    {"user": base_condition}
                ]
                query = {"$or": or_conditions}

                if task_status:
                    # ä½¿ç”¨æ˜ å°„åçš„çŠ¶æ€å€¼ï¼ˆTaskStatusæšä¸¾çš„valueï¼‰
                    query["status"] = task_status.value
                    logger.info(f"ğŸ“‹ [Tasks] æ·»åŠ çŠ¶æ€è¿‡æ»¤: {task_status.value}")

                logger.info(f"ğŸ“‹ [Tasks] MongoDB æŸ¥è¯¢æ¡ä»¶: {query}")
                # è¯»å–æ›´å¤šæ•°æ®ç”¨äºåˆå¹¶
                cursor = db.analysis_tasks.find(query).sort("created_at", -1).limit(limit * 2)
                async for doc in cursor:
                    count += 1
                    # å…¼å®¹ user_id æˆ– user å­—æ®µ
                    user_field_val = doc.get("user_id", doc.get("user"))
                    # ğŸ”§ å…¼å®¹å¤šç§è‚¡ç¥¨ä»£ç å­—æ®µåï¼šsymbol, stock_code, stock_symbol
                    stock_code_value = doc.get("symbol") or doc.get("stock_code") or doc.get("stock_symbol")
                    item = {
                        "task_id": doc.get("task_id"),
                        "user_id": str(user_field_val) if user_field_val is not None else None,
                        "symbol": stock_code_value,  # ğŸ”§ æ·»åŠ  symbol å­—æ®µï¼ˆå‰ç«¯ä¼˜å…ˆä½¿ç”¨ï¼‰
                        "stock_code": stock_code_value,  # ğŸ”§ å…¼å®¹å­—æ®µ
                        "stock_symbol": stock_code_value,  # ğŸ”§ å…¼å®¹å­—æ®µ
                        "stock_name": doc.get("stock_name"),
                        "status": str(doc.get("status", "pending")),
                        "progress": int(doc.get("progress", 0) or 0),
                        "message": doc.get("message", ""),
                        "current_step": doc.get("current_step", ""),
                        "start_time": doc.get("started_at") or doc.get("created_at"),
                        "end_time": doc.get("completed_at"),
                        "parameters": doc.get("parameters", {}),
                        "execution_time": doc.get("execution_time"),
                        "tokens_used": doc.get("tokens_used"),
                        # ä¸ºå…¼å®¹å‰ç«¯ï¼Œè¿™é‡Œæ²¿ç”¨ memory_manager çš„å­—æ®µå
                        "result_data": doc.get("result"),
                    }
                    # æ—¶é—´æ ¼å¼è½¬ä¸º ISO å­—ç¬¦ä¸²ï¼ˆæ·»åŠ æ—¶åŒºä¿¡æ¯ï¼‰
                    for k in ("start_time", "end_time"):
                        if item.get(k) and hasattr(item[k], "isoformat"):
                            dt = item[k]
                            # å¦‚æœæ˜¯ naive datetimeï¼ˆæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼‰ï¼Œå‡å®šä¸º UTC+8
                            if dt.tzinfo is None:
                                from datetime import timezone, timedelta
                                china_tz = timezone(timedelta(hours=8))
                                dt = dt.replace(tzinfo=china_tz)
                            item[k] = dt.isoformat()
                    mongo_tasks.append(item)

                logger.info(f"ğŸ“‹ [Tasks] MongoDB è¿”å›æ•°é‡: {count}")
            except Exception as mongo_e:
                logger.error(f"âŒ MongoDB æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {mongo_e}", exc_info=True)
                # MongoDB æŸ¥è¯¢å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å†…å­˜æ•°æ®

            # 4) åˆå¹¶å†…å­˜å’Œ MongoDB æ•°æ®ï¼Œå»é‡
            # ğŸ”§ å¯¹äº processing/running çŠ¶æ€ï¼Œä¼˜å…ˆä½¿ç”¨ MongoDB ä¸­çš„è¿›åº¦æ•°æ®
            # å› ä¸º graph_progress_callback ç›´æ¥æ›´æ–° MongoDBï¼Œè€Œå†…å­˜æ•°æ®å¯èƒ½æ˜¯æ—§çš„
            task_dict = {}

            # å…ˆæ·»åŠ å†…å­˜ä¸­çš„ä»»åŠ¡
            for task in tasks_in_mem:
                task_id = task.get("task_id")
                if task_id:
                    task_dict[task_id] = task

            # å†æ·»åŠ  MongoDB ä¸­çš„ä»»åŠ¡
            # å¯¹äº processing/running çŠ¶æ€ï¼Œä½¿ç”¨ MongoDB ä¸­çš„è¿›åº¦æ•°æ®ï¼ˆæ›´æ–°ï¼‰
            # å¯¹äºå…¶ä»–çŠ¶æ€ï¼Œå¦‚æœå†…å­˜ä¸­å·²æœ‰ï¼Œåˆ™è·³è¿‡ï¼ˆå†…å­˜ä¼˜å…ˆï¼‰
            for task in mongo_tasks:
                task_id = task.get("task_id")
                if not task_id:
                    continue

                # å¦‚æœå†…å­˜ä¸­å·²æœ‰è¿™ä¸ªä»»åŠ¡
                if task_id in task_dict:
                    mem_task = task_dict[task_id]
                    mongo_task = task

                    # å¦‚æœæ˜¯ processing/running çŠ¶æ€ï¼Œä½¿ç”¨ MongoDB ä¸­çš„è¿›åº¦æ•°æ®
                    if mongo_task.get("status") in ["processing", "running"]:
                        # ä¿ç•™å†…å­˜ä¸­çš„åŸºæœ¬ä¿¡æ¯ï¼Œä½†æ›´æ–°è¿›åº¦ç›¸å…³å­—æ®µ
                        mem_task["progress"] = mongo_task.get("progress", mem_task.get("progress", 0))
                        mem_task["message"] = mongo_task.get("message", mem_task.get("message", ""))
                        mem_task["current_step"] = mongo_task.get("current_step", mem_task.get("current_step", ""))
                        logger.debug(f"ğŸ”„ [Tasks] æ›´æ–°ä»»åŠ¡è¿›åº¦: {task_id}, progress={mem_task['progress']}%")
                else:
                    # å†…å­˜ä¸­æ²¡æœ‰ï¼Œç›´æ¥æ·»åŠ  MongoDB ä¸­çš„ä»»åŠ¡
                    task_dict[task_id] = task

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åº
            merged_tasks = list(task_dict.values())
            merged_tasks.sort(key=lambda x: x.get('start_time', ''), reverse=True)

            # åˆ†é¡µ
            results = merged_tasks[offset:offset + limit]

            # ğŸ”¥ ç»Ÿä¸€å¤„ç†æ—¶åŒºä¿¡æ¯ï¼ˆç¡®ä¿æ‰€æœ‰æ—¶é—´å­—æ®µéƒ½æœ‰æ—¶åŒºæ ‡è¯†ï¼‰
            from datetime import timezone, timedelta
            china_tz = timezone(timedelta(hours=8))

            for task in results:
                for time_field in ("start_time", "end_time", "created_at", "started_at", "completed_at"):
                    value = task.get(time_field)
                    if value:
                        # å¦‚æœæ˜¯ datetime å¯¹è±¡
                        if hasattr(value, "isoformat"):
                            # å¦‚æœæ˜¯ naive datetimeï¼Œæ·»åŠ æ—¶åŒºä¿¡æ¯
                            if value.tzinfo is None:
                                value = value.replace(tzinfo=china_tz)
                            task[time_field] = value.isoformat()
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ä¸”æ²¡æœ‰æ—¶åŒºæ ‡è¯†ï¼Œæ·»åŠ æ—¶åŒºæ ‡è¯†
                        elif isinstance(value, str) and value and not value.endswith(('Z', '+08:00', '+00:00')):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ ISO æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²
                            if 'T' in value or ' ' in value:
                                task[time_field] = value.replace(' ', 'T') + '+08:00'

            # ä¸ºç»“æœè¡¥é½è‚¡ç¥¨åç§°
            results = self._enrich_stock_names(results)
            logger.info(f"ğŸ“‹ [Tasks] åˆå¹¶åè¿”å›æ•°é‡: {len(results)} (å†…å­˜: {len(tasks_in_mem)}, MongoDB: {count})")
            return results
        except Exception as outer_e:
            logger.error(f"âŒ list_user_tasks å¤–å±‚å¼‚å¸¸: {outer_e}", exc_info=True)
            return []

    async def cleanup_zombie_tasks(self, max_running_hours: int = 2) -> Dict[str, Any]:
        """æ¸…ç†åƒµå°¸ä»»åŠ¡ï¼ˆé•¿æ—¶é—´å¤„äº processing/running çŠ¶æ€çš„ä»»åŠ¡ï¼‰

        Args:
            max_running_hours: æœ€å¤§è¿è¡Œæ—¶é•¿ï¼ˆå°æ—¶ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é•¿çš„ä»»åŠ¡å°†è¢«æ ‡è®°ä¸ºå¤±è´¥

        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        try:
            # 1) æ¸…ç†å†…å­˜ä¸­çš„åƒµå°¸ä»»åŠ¡
            memory_cleaned = await self.memory_manager.cleanup_zombie_tasks(max_running_hours)

            # 2) æ¸…ç† MongoDB ä¸­çš„åƒµå°¸ä»»åŠ¡
            db = get_mongo_db()
            from datetime import timedelta
            cutoff_time = datetime.utcnow() - timedelta(hours=max_running_hours)

            # æŸ¥æ‰¾é•¿æ—¶é—´å¤„äº processing çŠ¶æ€çš„ä»»åŠ¡
            zombie_filter = {
                "status": {"$in": ["processing", "running", "pending"]},
                "$or": [
                    {"started_at": {"$lt": cutoff_time}},
                    {"created_at": {"$lt": cutoff_time, "started_at": None}}
                ]
            }

            # æ›´æ–°ä¸ºå¤±è´¥çŠ¶æ€
            update_result = await db.analysis_tasks.update_many(
                zombie_filter,
                {
                    "$set": {
                        "status": "failed",
                        "last_error": f"ä»»åŠ¡è¶…æ—¶ï¼ˆè¿è¡Œæ—¶é—´è¶…è¿‡ {max_running_hours} å°æ—¶ï¼‰",
                        "completed_at": datetime.utcnow(),
                        "updated_at": datetime.utcnow()
                    }
                }
            )

            mongo_cleaned = update_result.modified_count

            logger.info(f"ğŸ§¹ åƒµå°¸ä»»åŠ¡æ¸…ç†å®Œæˆ: å†…å­˜={memory_cleaned}, MongoDB={mongo_cleaned}")

            return {
                "success": True,
                "memory_cleaned": memory_cleaned,
                "mongo_cleaned": mongo_cleaned,
                "total_cleaned": memory_cleaned + mongo_cleaned,
                "max_running_hours": max_running_hours
            }

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†åƒµå°¸ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "memory_cleaned": 0,
                "mongo_cleaned": 0,
                "total_cleaned": 0
            }

    async def get_zombie_tasks(self, max_running_hours: int = 2) -> List[Dict[str, Any]]:
        """è·å–åƒµå°¸ä»»åŠ¡åˆ—è¡¨ï¼ˆä¸æ‰§è¡Œæ¸…ç†ï¼Œä»…æŸ¥è¯¢ï¼‰

        Args:
            max_running_hours: æœ€å¤§è¿è¡Œæ—¶é•¿ï¼ˆå°æ—¶ï¼‰

        Returns:
            åƒµå°¸ä»»åŠ¡åˆ—è¡¨
        """
        try:
            db = get_mongo_db()
            from datetime import timedelta
            cutoff_time = datetime.utcnow() - timedelta(hours=max_running_hours)

            # æŸ¥æ‰¾é•¿æ—¶é—´å¤„äº processing çŠ¶æ€çš„ä»»åŠ¡
            zombie_filter = {
                "status": {"$in": ["processing", "running", "pending"]},
                "$or": [
                    {"started_at": {"$lt": cutoff_time}},
                    {"created_at": {"$lt": cutoff_time, "started_at": None}}
                ]
            }

            cursor = db.analysis_tasks.find(zombie_filter).sort("created_at", -1)
            zombie_tasks = []

            async for doc in cursor:
                task = {
                    "task_id": doc.get("task_id"),
                    "user_id": str(doc.get("user_id", doc.get("user"))),
                    "stock_code": doc.get("stock_code"),
                    "stock_name": doc.get("stock_name"),
                    "status": doc.get("status"),
                    "created_at": doc.get("created_at").isoformat() if doc.get("created_at") else None,
                    "started_at": doc.get("started_at").isoformat() if doc.get("started_at") else None,
                    "running_hours": None
                }

                # è®¡ç®—è¿è¡Œæ—¶é•¿
                start_time = doc.get("started_at") or doc.get("created_at")
                if start_time:
                    running_seconds = (datetime.utcnow() - start_time).total_seconds()
                    task["running_hours"] = round(running_seconds / 3600, 2)

                zombie_tasks.append(task)

            logger.info(f"ğŸ“‹ æŸ¥è¯¢åˆ° {len(zombie_tasks)} ä¸ªåƒµå°¸ä»»åŠ¡")
            return zombie_tasks

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢åƒµå°¸ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return []



    async def _update_task_status(
        self,
        task_id: str,
        status: AnalysisStatus,
        progress: int,
        error_message: str = None
    ):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            db = get_mongo_db()
            update_data = {
                "status": status,
                "progress": progress,
                "updated_at": datetime.utcnow()
            }

            if status == AnalysisStatus.PROCESSING and progress == 10:
                update_data["started_at"] = datetime.utcnow()
            elif status == AnalysisStatus.COMPLETED:
                update_data["completed_at"] = datetime.utcnow()
            elif status == AnalysisStatus.FAILED:
                update_data["last_error"] = error_message
                update_data["completed_at"] = datetime.utcnow()

            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": update_data}
            )

            logger.debug(f"ğŸ“Š ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°: {task_id} -> {status} ({progress}%)")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")

    async def _save_analysis_result(self, task_id: str, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœï¼ˆåŸå§‹æ–¹æ³•ï¼‰"""
        try:
            db = get_mongo_db()
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": {"result": result}}
            )
            logger.debug(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {task_id}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {task_id} - {e}")

    async def _save_analysis_result_web_style(self, task_id: str, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ - é‡‡ç”¨webç›®å½•çš„æ–¹å¼ï¼Œä¿å­˜åˆ°analysis_reportsé›†åˆ"""
        try:
            db = get_mongo_db()

            # ç”Ÿæˆåˆ†æIDï¼ˆä¸webç›®å½•ä¿æŒä¸€è‡´ï¼‰
            from datetime import datetime
            timestamp = datetime.utcnow()  # å­˜å‚¨ UTC æ—¶é—´ï¼ˆæ ‡å‡†åšæ³•ï¼‰
            stock_symbol = result.get('stock_symbol') or result.get('stock_code', 'UNKNOWN')
            analysis_id = f"{stock_symbol}_{timestamp.strftime('%Y%m%d_%H%M%S')}"

            # å¤„ç†reportså­—æ®µ - ä»stateä¸­æå–æ‰€æœ‰åˆ†ææŠ¥å‘Š
            reports = {}
            if 'state' in result:
                try:
                    state = result['state']

                    # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æŠ¥å‘Šå­—æ®µ
                    report_fields = [
                        'market_report',
                        'sentiment_report',
                        'news_report',
                        'fundamentals_report',
                        'investment_plan',
                        'trader_investment_plan',
                        'final_trade_decision'
                    ]

                    # ä»stateä¸­æå–æŠ¥å‘Šå†…å®¹
                    for field in report_fields:
                        if hasattr(state, field):
                            value = getattr(state, field, "")
                        elif isinstance(state, dict) and field in state:
                            value = state[field]
                        else:
                            value = ""

                        if isinstance(value, str) and len(value.strip()) > 10:  # åªä¿å­˜æœ‰å®é™…å†…å®¹çš„æŠ¥å‘Š
                            reports[field] = value.strip()

                    # å¤„ç†ç ”ç©¶å›¢é˜Ÿè¾©è®ºçŠ¶æ€æŠ¥å‘Š
                    if hasattr(state, 'investment_debate_state') or (isinstance(state, dict) and 'investment_debate_state' in state):
                        debate_state = getattr(state, 'investment_debate_state', None) if hasattr(state, 'investment_debate_state') else state.get('investment_debate_state')
                        if debate_state:
                            # æå–å¤šå¤´ç ”ç©¶å‘˜å†å²
                            if hasattr(debate_state, 'bull_history'):
                                bull_content = getattr(debate_state, 'bull_history', "")
                            elif isinstance(debate_state, dict) and 'bull_history' in debate_state:
                                bull_content = debate_state['bull_history']
                            else:
                                bull_content = ""

                            if bull_content and len(bull_content.strip()) > 10:
                                reports['bull_researcher'] = bull_content.strip()

                            # æå–ç©ºå¤´ç ”ç©¶å‘˜å†å²
                            if hasattr(debate_state, 'bear_history'):
                                bear_content = getattr(debate_state, 'bear_history', "")
                            elif isinstance(debate_state, dict) and 'bear_history' in debate_state:
                                bear_content = debate_state['bear_history']
                            else:
                                bear_content = ""

                            if bear_content and len(bear_content.strip()) > 10:
                                reports['bear_researcher'] = bear_content.strip()

                            # æå–ç ”ç©¶ç»ç†å†³ç­–
                            if hasattr(debate_state, 'judge_decision'):
                                decision_content = getattr(debate_state, 'judge_decision', "")
                            elif isinstance(debate_state, dict) and 'judge_decision' in debate_state:
                                decision_content = debate_state['judge_decision']
                            else:
                                decision_content = str(debate_state)

                            if decision_content and len(decision_content.strip()) > 10:
                                reports['research_team_decision'] = decision_content.strip()

                    # å¤„ç†é£é™©ç®¡ç†å›¢é˜Ÿè¾©è®ºçŠ¶æ€æŠ¥å‘Š
                    if hasattr(state, 'risk_debate_state') or (isinstance(state, dict) and 'risk_debate_state' in state):
                        risk_state = getattr(state, 'risk_debate_state', None) if hasattr(state, 'risk_debate_state') else state.get('risk_debate_state')
                        if risk_state:
                            # æå–æ¿€è¿›åˆ†æå¸ˆå†å²
                            if hasattr(risk_state, 'risky_history'):
                                risky_content = getattr(risk_state, 'risky_history', "")
                            elif isinstance(risk_state, dict) and 'risky_history' in risk_state:
                                risky_content = risk_state['risky_history']
                            else:
                                risky_content = ""

                            if risky_content and len(risky_content.strip()) > 10:
                                reports['risky_analyst'] = risky_content.strip()

                            # æå–ä¿å®ˆåˆ†æå¸ˆå†å²
                            if hasattr(risk_state, 'safe_history'):
                                safe_content = getattr(risk_state, 'safe_history', "")
                            elif isinstance(risk_state, dict) and 'safe_history' in risk_state:
                                safe_content = risk_state['safe_history']
                            else:
                                safe_content = ""

                            if safe_content and len(safe_content.strip()) > 10:
                                reports['safe_analyst'] = safe_content.strip()

                            # æå–ä¸­æ€§åˆ†æå¸ˆå†å²
                            if hasattr(risk_state, 'neutral_history'):
                                neutral_content = getattr(risk_state, 'neutral_history', "")
                            elif isinstance(risk_state, dict) and 'neutral_history' in risk_state:
                                neutral_content = risk_state['neutral_history']
                            else:
                                neutral_content = ""

                            if neutral_content and len(neutral_content.strip()) > 10:
                                reports['neutral_analyst'] = neutral_content.strip()

                            # æå–æŠ•èµ„ç»„åˆç»ç†å†³ç­–
                            if hasattr(risk_state, 'judge_decision'):
                                risk_decision = getattr(risk_state, 'judge_decision', "")
                            elif isinstance(risk_state, dict) and 'judge_decision' in risk_state:
                                risk_decision = risk_state['judge_decision']
                            else:
                                risk_decision = str(risk_state)

                            if risk_decision and len(risk_decision.strip()) > 10:
                                reports['risk_management_decision'] = risk_decision.strip()

                    logger.info(f"ğŸ“Š ä»stateä¸­æå–åˆ° {len(reports)} ä¸ªæŠ¥å‘Š: {list(reports.keys())}")

                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†stateä¸­çš„reportsæ—¶å‡ºé”™: {e}")
                    # é™çº§åˆ°ä»detailed_analysisæå–
                    if 'detailed_analysis' in result:
                        try:
                            detailed_analysis = result['detailed_analysis']
                            if isinstance(detailed_analysis, dict):
                                for key, value in detailed_analysis.items():
                                    if isinstance(value, str) and len(value) > 50:
                                        reports[key] = value
                                logger.info(f"ğŸ“Š é™çº§ï¼šä»detailed_analysisä¸­æå–åˆ° {len(reports)} ä¸ªæŠ¥å‘Š")
                        except Exception as fallback_error:
                            logger.warning(f"âš ï¸ é™çº§æå–ä¹Ÿå¤±è´¥: {fallback_error}")

            # ğŸ”¥ æ ¹æ®è‚¡ç¥¨ä»£ç æ¨æ–­å¸‚åœºç±»å‹
            from tradingagents.utils.stock_utils import StockUtils
            market_info = StockUtils.get_market_info(stock_symbol)
            market_type_map = {
                "china_a": "Aè‚¡",
                "hong_kong": "æ¸¯è‚¡",
                "us": "ç¾è‚¡",
                "unknown": "Aè‚¡"  # é»˜è®¤ä¸ºAè‚¡
            }
            market_type = market_type_map.get(market_info.get("market", "unknown"), "Aè‚¡")
            logger.info(f"ğŸ“Š æ¨æ–­å¸‚åœºç±»å‹: {stock_symbol} -> {market_type}")

            # ğŸ”¥ è·å–è‚¡ç¥¨åç§°
            stock_name = stock_symbol  # é»˜è®¤ä½¿ç”¨è‚¡ç¥¨ä»£ç 
            try:
                if market_info.get("market") == "china_a":
                    # Aè‚¡ï¼šä½¿ç”¨ç»Ÿä¸€æ¥å£è·å–è‚¡ç¥¨ä¿¡æ¯
                    from tradingagents.dataflows.interface import get_china_stock_info_unified
                    stock_info = get_china_stock_info_unified(stock_symbol)
                    logger.debug(f"ğŸ“Š è·å–è‚¡ç¥¨ä¿¡æ¯è¿”å›: {stock_info[:200] if stock_info else 'None'}...")

                    if stock_info and "è‚¡ç¥¨åç§°:" in stock_info:
                        stock_name = stock_info.split("è‚¡ç¥¨åç§°:")[1].split("\n")[0].strip()
                        logger.info(f"âœ… è·å–Aè‚¡åç§°: {stock_symbol} -> {stock_name}")
                    else:
                        # é™çº§æ–¹æ¡ˆï¼šå°è¯•ç›´æ¥ä»æ•°æ®æºç®¡ç†å™¨è·å–
                        logger.warning(f"âš ï¸ æ— æ³•ä»ç»Ÿä¸€æ¥å£è§£æè‚¡ç¥¨åç§°: {stock_symbol}ï¼Œå°è¯•é™çº§æ–¹æ¡ˆ")
                        try:
                            from tradingagents.dataflows.data_source_manager import get_china_stock_info_unified as get_info_dict
                            info_dict = get_info_dict(stock_symbol)
                            if info_dict and info_dict.get('name'):
                                stock_name = info_dict['name']
                                logger.info(f"âœ… é™çº§æ–¹æ¡ˆæˆåŠŸè·å–è‚¡ç¥¨åç§°: {stock_symbol} -> {stock_name}")
                        except Exception as fallback_e:
                            logger.error(f"âŒ é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_e}")

                elif market_info.get("market") == "hong_kong":
                    # æ¸¯è‚¡ï¼šä½¿ç”¨æ”¹è¿›çš„æ¸¯è‚¡å·¥å…·
                    try:
                        from tradingagents.dataflows.providers.hk.improved_hk import get_hk_company_name_improved
                        stock_name = get_hk_company_name_improved(stock_symbol)
                        logger.info(f"ğŸ“Š è·å–æ¸¯è‚¡åç§°: {stock_symbol} -> {stock_name}")
                    except Exception:
                        clean_ticker = stock_symbol.replace('.HK', '').replace('.hk', '')
                        stock_name = f"æ¸¯è‚¡{clean_ticker}"
                elif market_info.get("market") == "us":
                    # ç¾è‚¡ï¼šä½¿ç”¨ç®€å•æ˜ å°„
                    us_stock_names = {
                        'AAPL': 'è‹¹æœå…¬å¸', 'TSLA': 'ç‰¹æ–¯æ‹‰', 'NVDA': 'è‹±ä¼Ÿè¾¾',
                        'MSFT': 'å¾®è½¯', 'GOOGL': 'è°·æ­Œ', 'AMZN': 'äºšé©¬é€Š',
                        'META': 'Meta', 'NFLX': 'å¥ˆé£'
                    }
                    stock_name = us_stock_names.get(stock_symbol.upper(), f"ç¾è‚¡{stock_symbol}")
                    logger.info(f"ğŸ“Š è·å–ç¾è‚¡åç§°: {stock_symbol} -> {stock_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–è‚¡ç¥¨åç§°å¤±è´¥: {stock_symbol} - {e}")
                stock_name = stock_symbol

            # æ„å»ºæ–‡æ¡£ï¼ˆä¸webç›®å½•çš„MongoDBReportManagerä¿æŒä¸€è‡´ï¼‰
            document = {
                "analysis_id": analysis_id,
                "stock_symbol": stock_symbol,
                "stock_name": stock_name,  # ğŸ”¥ æ·»åŠ è‚¡ç¥¨åç§°å­—æ®µ
                "market_type": market_type,  # ğŸ”¥ æ·»åŠ å¸‚åœºç±»å‹å­—æ®µ
                "model_info": result.get("model_info", "Unknown"),  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
                "analysis_date": timestamp.strftime('%Y-%m-%d'),
                "timestamp": timestamp,
                "status": "completed",
                "source": "api",

                # åˆ†æç»“æœæ‘˜è¦
                "summary": result.get("summary", ""),
                "analysts": result.get("analysts", []),
                "research_depth": result.get("research_depth", 1),

                # æŠ¥å‘Šå†…å®¹
                "reports": reports,

                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ ¼å¼åŒ–åçš„decisionå­—æ®µï¼
                "decision": result.get("decision", {}),

                # å…ƒæ•°æ®
                "created_at": timestamp,
                "updated_at": timestamp,

                # APIç‰¹æœ‰å­—æ®µ
                "task_id": task_id,
                "recommendation": result.get("recommendation", ""),
                "confidence_score": result.get("confidence_score", 0.0),
                "risk_level": result.get("risk_level", "ä¸­ç­‰"),
                "key_points": result.get("key_points", []),
                "execution_time": result.get("execution_time", 0),
                "tokens_used": result.get("tokens_used", 0),

                # ğŸ†• æ€§èƒ½æŒ‡æ ‡æ•°æ®
                "performance_metrics": result.get("performance_metrics", {})
            }

            # ä¿å­˜åˆ°analysis_reportsé›†åˆï¼ˆä¸webç›®å½•ä¿æŒä¸€è‡´ï¼‰
            result_insert = await db.analysis_reports.insert_one(document)

            if result_insert.inserted_id:
                logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°MongoDB analysis_reports: {analysis_id}")

                # åŒæ—¶æ›´æ–°analysis_tasksé›†åˆä¸­çš„resultå­—æ®µï¼Œä¿æŒAPIå…¼å®¹æ€§
                await db.analysis_tasks.update_one(
                    {"task_id": task_id},
                    {"$set": {"result": {
                        "analysis_id": analysis_id,
                        "stock_symbol": stock_symbol,
                        "stock_code": result.get('stock_code', stock_symbol),
                        "analysis_date": result.get('analysis_date'),
                        "summary": result.get("summary", ""),
                        "recommendation": result.get("recommendation", ""),
                        "confidence_score": result.get("confidence_score", 0.0),
                        "risk_level": result.get("risk_level", "ä¸­ç­‰"),
                        "key_points": result.get("key_points", []),
                        "detailed_analysis": result.get("detailed_analysis", {}),
                        "execution_time": result.get("execution_time", 0),
                        "tokens_used": result.get("tokens_used", 0),
                        "reports": reports,  # åŒ…å«æå–çš„æŠ¥å‘Šå†…å®¹
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ ¼å¼åŒ–åçš„decisionå­—æ®µï¼
                        "decision": result.get("decision", {})
                    }}}
                )
                logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜ (webé£æ ¼): {task_id}")
            else:
                logger.error("âŒ MongoDBæ’å…¥å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {task_id} - {e}")
            # é™çº§åˆ°ç®€å•ä¿å­˜
            try:
                simple_result = {
                    'task_id': task_id,
                    'success': result.get('success', True),
                    'error': str(e),
                    'completed_at': datetime.utcnow().isoformat()
                }
                await db.analysis_tasks.update_one(
                    {"task_id": task_id},
                    {"$set": {"result": simple_result}}
                )
                logger.info(f"ğŸ’¾ ä½¿ç”¨ç®€åŒ–ç»“æœä¿å­˜: {task_id}")
            except Exception as fallback_error:
                logger.error(f"âŒ ç®€åŒ–ä¿å­˜ä¹Ÿå¤±è´¥: {task_id} - {fallback_error}")

    async def _save_analysis_results_complete(self, task_id: str, result: Dict[str, Any]):
        """å®Œæ•´çš„åˆ†æç»“æœä¿å­˜ - å®Œå…¨é‡‡ç”¨webç›®å½•çš„åŒé‡ä¿å­˜æ–¹å¼"""
        try:
            # è°ƒè¯•ï¼šæ‰“å°resultä¸­çš„æ‰€æœ‰é”®
            logger.info(f"ğŸ” [è°ƒè¯•] resultä¸­çš„æ‰€æœ‰é”®: {list(result.keys())}")
            logger.info(f"ğŸ” [è°ƒè¯•] stock_code: {result.get('stock_code', 'NOT_FOUND')}")
            logger.info(f"ğŸ” [è°ƒè¯•] stock_symbol: {result.get('stock_symbol', 'NOT_FOUND')}")

            # ä¼˜å…ˆä½¿ç”¨stock_symbolï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨stock_code
            stock_symbol = result.get('stock_symbol') or result.get('stock_code', 'UNKNOWN')
            logger.info(f"ğŸ’¾ å¼€å§‹å®Œæ•´ä¿å­˜åˆ†æç»“æœ: {stock_symbol}")

            # 1. ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•
            logger.info(f"ğŸ“ [æœ¬åœ°ä¿å­˜] å¼€å§‹ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•")
            local_files = await self._save_modular_reports_to_data_dir(result, stock_symbol)
            if local_files:
                logger.info(f"âœ… [æœ¬åœ°ä¿å­˜] å·²ä¿å­˜ {len(local_files)} ä¸ªæœ¬åœ°æŠ¥å‘Šæ–‡ä»¶")
                for module, path in local_files.items():
                    logger.info(f"  - {module}: {path}")
            else:
                logger.warning(f"âš ï¸ [æœ¬åœ°ä¿å­˜] æœ¬åœ°æŠ¥å‘Šæ–‡ä»¶ä¿å­˜å¤±è´¥")

            # 2. ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ•°æ®åº“
            logger.info(f"ğŸ—„ï¸ [æ•°æ®åº“ä¿å­˜] å¼€å§‹ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ•°æ®åº“")
            await self._save_analysis_result_web_style(task_id, result)
            logger.info(f"âœ… [æ•°æ®åº“ä¿å­˜] åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")

            # 3. è®°å½•ä¿å­˜ç»“æœ
            if local_files:
                logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°æ•°æ®åº“å’Œæœ¬åœ°æ–‡ä»¶")
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“ä¿å­˜æˆåŠŸï¼Œä½†æœ¬åœ°æ–‡ä»¶ä¿å­˜å¤±è´¥")

        except Exception as save_error:
            logger.error(f"âŒ [å®Œæ•´ä¿å­˜] ä¿å­˜åˆ†ææŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(save_error)}")
            # é™çº§åˆ°ä»…æ•°æ®åº“ä¿å­˜
            try:
                await self._save_analysis_result_web_style(task_id, result)
                logger.info(f"ğŸ’¾ é™çº§ä¿å­˜æˆåŠŸ (ä»…æ•°æ®åº“): {task_id}")
            except Exception as fallback_error:
                logger.error(f"âŒ é™çº§ä¿å­˜ä¹Ÿå¤±è´¥: {task_id} - {fallback_error}")

    async def _save_modular_reports_to_data_dir(self, result: Dict[str, Any], stock_symbol: str) -> Dict[str, str]:
        """ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°dataç›®å½• - å®Œå…¨é‡‡ç”¨webç›®å½•çš„æ–‡ä»¶ç»“æ„"""
        try:
            import os
            from pathlib import Path
            from datetime import datetime
            import json

            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent

            # ç¡®å®šresultsç›®å½•è·¯å¾„ - ä¸webç›®å½•ä¿æŒä¸€è‡´
            results_dir_env = os.getenv("TRADINGAGENTS_RESULTS_DIR")
            if results_dir_env:
                if not os.path.isabs(results_dir_env):
                    results_dir = project_root / results_dir_env
                else:
                    results_dir = Path(results_dir_env)
            else:
                # é»˜è®¤ä½¿ç”¨dataç›®å½•è€Œä¸æ˜¯resultsç›®å½•
                results_dir = project_root / "data" / "analysis_results"

            # åˆ›å»ºè‚¡ç¥¨ä¸“ç”¨ç›®å½• - å®Œå…¨æŒ‰ç…§webç›®å½•çš„ç»“æ„
            analysis_date_raw = result.get('analysis_date', datetime.now())

            # ç¡®ä¿ analysis_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if isinstance(analysis_date_raw, datetime):
                analysis_date_str = analysis_date_raw.strftime('%Y-%m-%d')
            elif isinstance(analysis_date_raw, str):
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œæ£€æŸ¥æ ¼å¼
                try:
                    # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
                    parsed_date = datetime.strptime(analysis_date_raw, '%Y-%m-%d')
                    analysis_date_str = analysis_date_raw
                except ValueError:
                    # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                    analysis_date_str = datetime.now().strftime('%Y-%m-%d')
            else:
                # å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                analysis_date_str = datetime.now().strftime('%Y-%m-%d')

            stock_dir = results_dir / stock_symbol / analysis_date_str
            reports_dir = stock_dir / "reports"
            reports_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºmessage_tool.logæ–‡ä»¶ - ä¸webç›®å½•ä¿æŒä¸€è‡´
            log_file = stock_dir / "message_tool.log"
            log_file.touch(exist_ok=True)

            logger.info(f"ğŸ“ åˆ›å»ºåˆ†æç»“æœç›®å½•: {reports_dir}")
            logger.info(f"ğŸ” [è°ƒè¯•] analysis_date_raw ç±»å‹: {type(analysis_date_raw)}, å€¼: {analysis_date_raw}")
            logger.info(f"ğŸ” [è°ƒè¯•] analysis_date_str: {analysis_date_str}")
            logger.info(f"ğŸ” [è°ƒè¯•] å®Œæ•´è·¯å¾„: {os.path.normpath(str(reports_dir))}")

            state = result.get('state', {})
            saved_files = {}

            # å®šä¹‰æŠ¥å‘Šæ¨¡å—æ˜ å°„ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„å®šä¹‰
            report_modules = {
                'market_report': {
                    'filename': 'market_report.md',
                    'title': f'{stock_symbol} è‚¡ç¥¨æŠ€æœ¯åˆ†ææŠ¥å‘Š',
                    'state_key': 'market_report'
                },
                'sentiment_report': {
                    'filename': 'sentiment_report.md',
                    'title': f'{stock_symbol} å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š',
                    'state_key': 'sentiment_report'
                },
                'news_report': {
                    'filename': 'news_report.md',
                    'title': f'{stock_symbol} æ–°é—»äº‹ä»¶åˆ†ææŠ¥å‘Š',
                    'state_key': 'news_report'
                },
                'fundamentals_report': {
                    'filename': 'fundamentals_report.md',
                    'title': f'{stock_symbol} åŸºæœ¬é¢åˆ†ææŠ¥å‘Š',
                    'state_key': 'fundamentals_report'
                },
                'investment_plan': {
                    'filename': 'investment_plan.md',
                    'title': f'{stock_symbol} æŠ•èµ„å†³ç­–æŠ¥å‘Š',
                    'state_key': 'investment_plan'
                },
                'trader_investment_plan': {
                    'filename': 'trader_investment_plan.md',
                    'title': f'{stock_symbol} äº¤æ˜“è®¡åˆ’æŠ¥å‘Š',
                    'state_key': 'trader_investment_plan'
                },
                'final_trade_decision': {
                    'filename': 'final_trade_decision.md',
                    'title': f'{stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–',
                    'state_key': 'final_trade_decision'
                },
                'investment_debate_state': {
                    'filename': 'research_team_decision.md',
                    'title': f'{stock_symbol} ç ”ç©¶å›¢é˜Ÿå†³ç­–æŠ¥å‘Š',
                    'state_key': 'investment_debate_state'
                },
                'risk_debate_state': {
                    'filename': 'risk_management_decision.md',
                    'title': f'{stock_symbol} é£é™©ç®¡ç†å›¢é˜Ÿå†³ç­–æŠ¥å‘Š',
                    'state_key': 'risk_debate_state'
                }
            }

            # ä¿å­˜å„æ¨¡å—æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            for module_key, module_info in report_modules.items():
                try:
                    state_key = module_info['state_key']
                    if state_key in state:
                        # æå–æ¨¡å—å†…å®¹
                        module_content = state[state_key]
                        if isinstance(module_content, str):
                            report_content = module_content
                        else:
                            report_content = str(module_content)

                        # ä¿å­˜åˆ°æ–‡ä»¶ - ä½¿ç”¨webç›®å½•çš„æ–‡ä»¶å
                        file_path = reports_dir / module_info['filename']
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(report_content)

                        saved_files[module_key] = str(file_path)
                        logger.info(f"âœ… ä¿å­˜æ¨¡å—æŠ¥å‘Š: {file_path}")

                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æ¨¡å— {module_key} å¤±è´¥: {e}")

            # ä¿å­˜æœ€ç»ˆå†³ç­–æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            decision = result.get('decision', {})
            if decision:
                decision_content = f"# {stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–\n\n"

                if isinstance(decision, dict):
                    decision_content += f"## æŠ•èµ„å»ºè®®\n\n"
                    decision_content += f"**è¡ŒåŠ¨**: {decision.get('action', 'N/A')}\n\n"
                    decision_content += f"**ç½®ä¿¡åº¦**: {decision.get('confidence', 0):.1%}\n\n"
                    decision_content += f"**é£é™©è¯„åˆ†**: {decision.get('risk_score', 0):.1%}\n\n"
                    decision_content += f"**ç›®æ ‡ä»·ä½**: {decision.get('target_price', 'N/A')}\n\n"
                    decision_content += f"## åˆ†ææ¨ç†\n\n{decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')}\n\n"
                else:
                    decision_content += f"{str(decision)}\n\n"

                decision_file = reports_dir / "final_trade_decision.md"
                with open(decision_file, 'w', encoding='utf-8') as f:
                    f.write(decision_content)

                saved_files['final_trade_decision'] = str(decision_file)
                logger.info(f"âœ… ä¿å­˜æœ€ç»ˆå†³ç­–: {decision_file}")

            # ä¿å­˜åˆ†æå…ƒæ•°æ®æ–‡ä»¶ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            metadata = {
                'stock_symbol': stock_symbol,
                'analysis_date': analysis_date_str,
                'timestamp': datetime.now().isoformat(),
                'research_depth': result.get('research_depth', 1),
                'analysts': result.get('analysts', []),
                'status': 'completed',
                'reports_count': len(saved_files),
                'report_types': list(saved_files.keys())
            }

            metadata_file = reports_dir.parent / "analysis_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… ä¿å­˜åˆ†æå…ƒæ•°æ®: {metadata_file}")
            logger.info(f"âœ… åˆ†æ¨¡å—æŠ¥å‘Šä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶")
            logger.info(f"ğŸ“ ä¿å­˜ç›®å½•: {os.path.normpath(str(reports_dir))}")

            return saved_files

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šå¤±è´¥: {e}")
            import traceback
            logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}

# é‡å¤çš„ get_task_status æ–¹æ³•å·²åˆ é™¤ï¼Œä½¿ç”¨ç¬¬469è¡Œçš„å†…å­˜ç‰ˆæœ¬


# å…¨å±€æœåŠ¡å®ä¾‹
_analysis_service = None

def get_simple_analysis_service() -> SimpleAnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹"""
    global _analysis_service
    if _analysis_service is None:
        logger.info("ğŸ”§ [å•ä¾‹] åˆ›å»ºæ–°çš„ SimpleAnalysisService å®ä¾‹")
        _analysis_service = SimpleAnalysisService()
    else:
        logger.info(f"ğŸ”§ [å•ä¾‹] è¿”å›ç°æœ‰çš„ SimpleAnalysisService å®ä¾‹: {id(_analysis_service)}")
    return _analysis_service
