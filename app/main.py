"""
TradingAgents-CN v1.0.0-preview FastAPI Backend
ä¸»åº”ç”¨ç¨‹åºå…¥å£

Copyright (c) 2025 hsliuping. All rights reserved.
ç‰ˆæƒæ‰€æœ‰ (c) 2025 hsliupingã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

This software is proprietary and confidential. Unauthorized copying, distribution,
or use of this software, via any medium, is strictly prohibited.
æœ¬è½¯ä»¶ä¸ºä¸“æœ‰å’Œæœºå¯†è½¯ä»¶ã€‚ä¸¥ç¦é€šè¿‡ä»»ä½•åª’ä»‹æœªç»æˆæƒå¤åˆ¶ã€åˆ†å‘æˆ–ä½¿ç”¨æœ¬è½¯ä»¶ã€‚

For commercial licensing, please contact: hsliup@163.com
å•†ä¸šè®¸å¯å’¨è¯¢ï¼Œè¯·è”ç³»ï¼šhsliup@163.com
"""

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import logging
import time
from datetime import datetime
from contextlib import asynccontextmanager
import asyncio
from pathlib import Path

from app.core.config import settings
from app.core.database import init_db, close_db
from app.core.logging_config import setup_logging
from app.routers import auth_db as auth, analysis, screening, queue, sse, health, favorites, config, reports, database, operation_logs, tags, tushare_init, akshare_init, baostock_init, historical_data, multi_period_sync, financial_data, news_data, social_media, internal_messages, usage_statistics, model_capabilities, cache, logs
from app.routers import sync as sync_router, multi_source_sync
from app.routers import stocks as stocks_router
from app.routers import stock_data as stock_data_router
from app.routers import stock_sync as stock_sync_router
from app.routers import multi_market_stocks as multi_market_stocks_router
from app.routers import notifications as notifications_router
from app.routers import websocket_notifications as websocket_notifications_router
from app.routers import scheduler as scheduler_router
from app.services.basics_sync_service import get_basics_sync_service
from app.services.multi_source_basics_sync_service import MultiSourceBasicsSyncService
from app.services.scheduler_service import set_scheduler_instance
from app.worker.tushare_sync_service import (
    run_tushare_basic_info_sync,
    run_tushare_quotes_sync,
    run_tushare_historical_sync,
    run_tushare_financial_sync,
    run_tushare_status_check
)
from app.worker.akshare_sync_service import (
    run_akshare_basic_info_sync,
    run_akshare_quotes_sync,
    run_akshare_historical_sync,
    run_akshare_financial_sync,
    run_akshare_status_check
)
from app.worker.baostock_sync_service import (
    run_baostock_basic_info_sync,
    run_baostock_daily_quotes_sync,
    run_baostock_historical_sync,
    run_baostock_status_check
)
# æ¸¯è‚¡å’Œç¾è‚¡æ”¹ä¸ºæŒ‰éœ€è·å–+ç¼“å­˜æ¨¡å¼ï¼Œä¸å†éœ€è¦å®šæ—¶åŒæ­¥ä»»åŠ¡
# from app.worker.hk_sync_service import ...
# from app.worker.us_sync_service import ...
from app.middleware.operation_log_middleware import OperationLogMiddleware
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from app.services.quotes_ingestion_service import QuotesIngestionService
from app.routers import paper as paper_router


def get_version() -> str:
    """ä» VERSION æ–‡ä»¶è¯»å–ç‰ˆæœ¬å·"""
    try:
        version_file = Path(__file__).parent.parent / "VERSION"
        if version_file.exists():
            return version_file.read_text(encoding='utf-8').strip()
    except Exception:
        pass
    return "1.0.0"  # é»˜è®¤ç‰ˆæœ¬å·


async def _print_config_summary(logger):
    """æ˜¾ç¤ºé…ç½®æ‘˜è¦"""
    try:
        logger.info("=" * 70)
        logger.info("ğŸ“‹ TradingAgents-CN Configuration Summary")
        logger.info("=" * 70)

        # .env æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        import os
        from pathlib import Path
        
        current_dir = Path.cwd()
        logger.info(f"ğŸ“ Current working directory: {current_dir}")
        
        # æ£€æŸ¥å¯èƒ½çš„ .env æ–‡ä»¶ä½ç½®
        env_files_to_check = [
            current_dir / ".env",
            current_dir / "app" / ".env",
            Path(__file__).parent.parent / ".env",  # é¡¹ç›®æ ¹ç›®å½•
        ]
        
        logger.info("ğŸ” Checking .env file locations:")
        env_file_found = False
        for env_file in env_files_to_check:
            if env_file.exists():
                logger.info(f"  âœ… Found: {env_file} (size: {env_file.stat().st_size} bytes)")
                env_file_found = True
                # æ˜¾ç¤ºæ–‡ä»¶çš„å‰å‡ è¡Œï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
                try:
                    with open(env_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()[:5]  # åªè¯»å‰5è¡Œ
                        logger.info(f"     Preview (first 5 lines):")
                        for i, line in enumerate(lines, 1):
                            # éšè—åŒ…å«å¯†ç ã€å¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯çš„è¡Œ
                            if any(keyword in line.upper() for keyword in ['PASSWORD', 'SECRET', 'KEY', 'TOKEN']):
                                logger.info(f"       {i}: {line.split('=')[0]}=***")
                            else:
                                logger.info(f"       {i}: {line.strip()}")
                except Exception as e:
                    logger.warning(f"     Could not preview file: {e}")
            else:
                logger.info(f"  âŒ Not found: {env_file}")
        
        if not env_file_found:
            logger.warning("âš ï¸  No .env file found in checked locations")
        
        # Pydantic Settings é…ç½®åŠ è½½çŠ¶æ€
        logger.info("âš™ï¸  Pydantic Settings Configuration:")
        logger.info(f"  â€¢ Settings class: {settings.__class__.__name__}")
        logger.info(f"  â€¢ Config source: {getattr(settings.model_config, 'env_file', 'Not specified')}")
        logger.info(f"  â€¢ Encoding: {getattr(settings.model_config, 'env_file_encoding', 'Not specified')}")
        
        # æ˜¾ç¤ºä¸€äº›å…³é”®é…ç½®å€¼çš„æ¥æºï¼ˆç¯å¢ƒå˜é‡ vs é»˜è®¤å€¼ï¼‰
        key_settings = ['HOST', 'PORT', 'DEBUG', 'MONGODB_HOST', 'REDIS_HOST']
        logger.info("  â€¢ Key settings sources:")
        for setting_name in key_settings:
            env_var_name = setting_name
            env_value = os.getenv(env_var_name)
            config_value = getattr(settings, setting_name, None)
            if env_value is not None:
                logger.info(f"    - {setting_name}: from environment variable ({config_value})")
            else:
                logger.info(f"    - {setting_name}: using default value ({config_value})")
        
        # ç¯å¢ƒä¿¡æ¯
        env = "Production" if settings.is_production else "Development"
        logger.info(f"Environment: {env}")

        # æ•°æ®åº“è¿æ¥
        logger.info(f"MongoDB: {settings.MONGODB_HOST}:{settings.MONGODB_PORT}/{settings.MONGODB_DATABASE}")
        logger.info(f"Redis: {settings.REDIS_HOST}:{settings.REDIS_PORT}/{settings.REDIS_DB}")

        # ä»£ç†é…ç½®
        import os
        if settings.HTTP_PROXY or settings.HTTPS_PROXY:
            logger.info("Proxy Configuration:")
            if settings.HTTP_PROXY:
                logger.info(f"  HTTP_PROXY: {settings.HTTP_PROXY}")
            if settings.HTTPS_PROXY:
                logger.info(f"  HTTPS_PROXY: {settings.HTTPS_PROXY}")
            if settings.NO_PROXY:
                # åªæ˜¾ç¤ºå‰3ä¸ªåŸŸå
                no_proxy_list = settings.NO_PROXY.split(',')
                if len(no_proxy_list) <= 3:
                    logger.info(f"  NO_PROXY: {settings.NO_PROXY}")
                else:
                    logger.info(f"  NO_PROXY: {','.join(no_proxy_list[:3])}... ({len(no_proxy_list)} domains)")
            logger.info(f"  âœ… Proxy environment variables set successfully")
        else:
            logger.info("Proxy: Not configured (direct connection)")

        # æ£€æŸ¥å¤§æ¨¡å‹é…ç½®
        try:
            from app.services.config_service import config_service
            config = await config_service.get_system_config()
            if config and config.llm_configs:
                enabled_llms = [llm for llm in config.llm_configs if llm.enabled]
                logger.info(f"Enabled LLMs: {len(enabled_llms)}")
                if enabled_llms:
                    for llm in enabled_llms[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        logger.info(f"  â€¢ {llm.provider}: {llm.model_name}")
                    if len(enabled_llms) > 3:
                        logger.info(f"  â€¢ ... and {len(enabled_llms) - 3} more")
                else:
                    logger.warning("âš ï¸  No LLM enabled. Please configure at least one LLM in Web UI.")
            else:
                logger.warning("âš ï¸  No LLM configured. Please configure at least one LLM in Web UI.")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to check LLM configs: {e}")

        # æ£€æŸ¥æ•°æ®æºé…ç½®
        try:
            if config and config.data_source_configs:
                enabled_sources = [ds for ds in config.data_source_configs if ds.enabled]
                logger.info(f"Enabled Data Sources: {len(enabled_sources)}")
                if enabled_sources:
                    for ds in enabled_sources[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        logger.info(f"  â€¢ {ds.type.value}: {ds.name}")
                    if len(enabled_sources) > 3:
                        logger.info(f"  â€¢ ... and {len(enabled_sources) - 3} more")
            else:
                logger.info("Data Sources: Using default (AKShare)")
        except Exception as e:
            logger.warning(f"âš ï¸  Failed to check data source configs: {e}")

        logger.info("=" * 70)
    except Exception as e:
        logger.error(f"Failed to print config summary: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    setup_logging()
    logger = logging.getLogger("app.main")

    # éªŒè¯å¯åŠ¨é…ç½®
    try:
        from app.core.startup_validator import validate_startup_config
        validate_startup_config()
    except Exception as e:
        logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        raise

    await init_db()

    #  é…ç½®æ¡¥æ¥ï¼šå°†ç»Ÿä¸€é…ç½®å†™å…¥ç¯å¢ƒå˜é‡ï¼Œä¾› TradingAgents æ ¸å¿ƒåº“ä½¿ç”¨
    try:
        from app.core.config_bridge import bridge_config_to_env
        bridge_config_to_env()
    except Exception as e:
        logger.warning(f"âš ï¸  é…ç½®æ¡¥æ¥å¤±è´¥: {e}")
        logger.warning("âš ï¸  TradingAgents å°†ä½¿ç”¨ .env æ–‡ä»¶ä¸­çš„é…ç½®")

    # Apply dynamic settings (log_level, enable_monitoring) from ConfigProvider
    try:
        from app.services.config_provider import provider as config_provider  # local import to avoid early DB init issues
        eff = await config_provider.get_effective_system_settings()
        desired_level = str(eff.get("log_level", "INFO")).upper()
        setup_logging(log_level=desired_level)
        for name in ("webapi", "worker", "uvicorn", "fastapi"):
            logging.getLogger(name).setLevel(desired_level)
        try:
            from app.middleware.operation_log_middleware import set_operation_log_enabled
            set_operation_log_enabled(bool(eff.get("enable_monitoring", True)))
        except Exception:
            pass
    except Exception as e:
        logging.getLogger("webapi").warning(f"Failed to apply dynamic settings: {e}")

    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    await _print_config_summary(logger)

    logger.info("TradingAgents FastAPI backend started")

    # å¯åŠ¨æœŸï¼šè‹¥éœ€è¦åœ¨ä¼‘å¸‚æ—¶è¡¥å……ä¸Šä¸€äº¤æ˜“æ—¥æ”¶ç›˜å¿«ç…§
    if settings.QUOTES_BACKFILL_ON_STARTUP:
        try:
            qi = QuotesIngestionService()
            await qi.ensure_indexes()
            await qi.backfill_last_close_snapshot_if_needed()
        except Exception as e:
            logger.warning(f"Startup backfill failed (ignored): {e}")

    # å¯åŠ¨æ¯æ—¥å®šæ—¶ä»»åŠ¡ï¼šå¯é…ç½®
    scheduler: AsyncIOScheduler | None = None
    try:
        from croniter import croniter
    except Exception:
        croniter = None  # å¯é€‰ä¾èµ–
    try:
        scheduler = AsyncIOScheduler(timezone=settings.TIMEZONE)

        # ä½¿ç”¨å¤šæ•°æ®æºåŒæ­¥æœåŠ¡ï¼ˆæ”¯æŒè‡ªåŠ¨åˆ‡æ¢ï¼‰
        multi_source_service = MultiSourceBasicsSyncService()

        # æ ¹æ® TUSHARE_ENABLED é…ç½®å†³å®šä¼˜å…ˆæ•°æ®æº
        # å¦‚æœ Tushare è¢«ç¦ç”¨ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨å…¶ä»–å¯ç”¨æ•°æ®æºï¼ˆAKShare/BaoStockï¼‰
        preferred_sources = None  # None è¡¨ç¤ºä½¿ç”¨é»˜è®¤ä¼˜å…ˆçº§é¡ºåº

        if settings.TUSHARE_ENABLED:
            # Tushare å¯ç”¨æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ Tushare
            preferred_sources = ["tushare", "akshare", "baostock"]
            logger.info(f"ğŸ“Š è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ä¼˜å…ˆæ•°æ®æº: Tushare > AKShare > BaoStock")
        else:
            # Tushare ç¦ç”¨æ—¶ï¼Œä½¿ç”¨ AKShare å’Œ BaoStock
            preferred_sources = ["akshare", "baostock"]
            logger.info(f"ğŸ“Š è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ä¼˜å…ˆæ•°æ®æº: AKShare > BaoStock (Tushareå·²ç¦ç”¨)")

        # ç«‹å³åœ¨å¯åŠ¨åå°è¯•ä¸€æ¬¡ï¼ˆä¸é˜»å¡ï¼‰
        async def run_sync_with_sources():
            await multi_source_service.run_full_sync(force=False, preferred_sources=preferred_sources)

        asyncio.create_task(run_sync_with_sources())

        # é…ç½®è°ƒåº¦ï¼šä¼˜å…ˆä½¿ç”¨ CRONï¼Œå…¶æ¬¡ä½¿ç”¨ HH:MM
        if settings.SYNC_STOCK_BASICS_ENABLED:
            if settings.SYNC_STOCK_BASICS_CRON:
                # å¦‚æœæä¾›äº†cronè¡¨è¾¾å¼
                scheduler.add_job(
                    lambda: multi_source_service.run_full_sync(force=False, preferred_sources=preferred_sources),
                    CronTrigger.from_crontab(settings.SYNC_STOCK_BASICS_CRON, timezone=settings.TIMEZONE),
                    id="basics_sync_service",
                    name="è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ï¼ˆå¤šæ•°æ®æºï¼‰"
                )
                logger.info(f"ğŸ“… Stock basics sync scheduled by CRON: {settings.SYNC_STOCK_BASICS_CRON} ({settings.TIMEZONE})")
            else:
                hh, mm = (settings.SYNC_STOCK_BASICS_TIME or "06:30").split(":")
                scheduler.add_job(
                    lambda: multi_source_service.run_full_sync(force=False, preferred_sources=preferred_sources),
                    CronTrigger(hour=int(hh), minute=int(mm), timezone=settings.TIMEZONE),
                    id="basics_sync_service",
                    name="è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ï¼ˆå¤šæ•°æ®æºï¼‰"
                )
                logger.info(f"ğŸ“… Stock basics sync scheduled daily at {settings.SYNC_STOCK_BASICS_TIME} ({settings.TIMEZONE})")

        # å®æ—¶è¡Œæƒ…å…¥åº“ä»»åŠ¡ï¼ˆæ¯Nç§’ï¼‰ï¼Œå†…éƒ¨è‡ªåˆ¤äº¤æ˜“æ—¶æ®µ
        if settings.QUOTES_INGEST_ENABLED:
            quotes_ingestion = QuotesIngestionService()
            await quotes_ingestion.ensure_indexes()
            scheduler.add_job(
                quotes_ingestion.run_once,  # coroutine function; AsyncIOScheduler will await it
                IntervalTrigger(seconds=settings.QUOTES_INGEST_INTERVAL_SECONDS, timezone=settings.TIMEZONE),
                id="quotes_ingestion_service",
                name="å®æ—¶è¡Œæƒ…å…¥åº“æœåŠ¡"
            )
            logger.info(f"â± å®æ—¶è¡Œæƒ…å…¥åº“ä»»åŠ¡å·²å¯åŠ¨: æ¯ {settings.QUOTES_INGEST_INTERVAL_SECONDS}s")

        # Tushareç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡é…ç½®
        logger.info("ğŸ”„ é…ç½®Tushareç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡...")

        # åŸºç¡€ä¿¡æ¯åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_tushare_basic_info_sync,
            CronTrigger.from_crontab(settings.TUSHARE_BASIC_INFO_SYNC_CRON, timezone=settings.TIMEZONE),
            id="tushare_basic_info_sync",
            name="è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ï¼ˆTushareï¼‰",
            kwargs={"force_update": False}
        )
        if not (settings.TUSHARE_UNIFIED_ENABLED and settings.TUSHARE_BASIC_INFO_SYNC_ENABLED):
            scheduler.pause_job("tushare_basic_info_sync")
            logger.info(f"â¸ï¸ TushareåŸºç¡€ä¿¡æ¯åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.TUSHARE_BASIC_INFO_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“… TushareåŸºç¡€ä¿¡æ¯åŒæ­¥å·²é…ç½®: {settings.TUSHARE_BASIC_INFO_SYNC_CRON}")

        # å®æ—¶è¡Œæƒ…åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_tushare_quotes_sync,
            CronTrigger.from_crontab(settings.TUSHARE_QUOTES_SYNC_CRON, timezone=settings.TIMEZONE),
            id="tushare_quotes_sync",
            name="å®æ—¶è¡Œæƒ…åŒæ­¥ï¼ˆTushareï¼‰"
        )
        if not (settings.TUSHARE_UNIFIED_ENABLED and settings.TUSHARE_QUOTES_SYNC_ENABLED):
            scheduler.pause_job("tushare_quotes_sync")
            logger.info(f"â¸ï¸ Tushareè¡Œæƒ…åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.TUSHARE_QUOTES_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“ˆ Tushareè¡Œæƒ…åŒæ­¥å·²é…ç½®: {settings.TUSHARE_QUOTES_SYNC_CRON}")

        # å†å²æ•°æ®åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_tushare_historical_sync,
            CronTrigger.from_crontab(settings.TUSHARE_HISTORICAL_SYNC_CRON, timezone=settings.TIMEZONE),
            id="tushare_historical_sync",
            name="å†å²æ•°æ®åŒæ­¥ï¼ˆTushareï¼‰",
            kwargs={"incremental": True}
        )
        if not (settings.TUSHARE_UNIFIED_ENABLED and settings.TUSHARE_HISTORICAL_SYNC_ENABLED):
            scheduler.pause_job("tushare_historical_sync")
            logger.info(f"â¸ï¸ Tushareå†å²æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.TUSHARE_HISTORICAL_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“Š Tushareå†å²æ•°æ®åŒæ­¥å·²é…ç½®: {settings.TUSHARE_HISTORICAL_SYNC_CRON}")

        # è´¢åŠ¡æ•°æ®åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_tushare_financial_sync,
            CronTrigger.from_crontab(settings.TUSHARE_FINANCIAL_SYNC_CRON, timezone=settings.TIMEZONE),
            id="tushare_financial_sync",
            name="è´¢åŠ¡æ•°æ®åŒæ­¥ï¼ˆTushareï¼‰"
        )
        if not (settings.TUSHARE_UNIFIED_ENABLED and settings.TUSHARE_FINANCIAL_SYNC_ENABLED):
            scheduler.pause_job("tushare_financial_sync")
            logger.info(f"â¸ï¸ Tushareè´¢åŠ¡æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.TUSHARE_FINANCIAL_SYNC_CRON}")
        else:
            logger.info(f"ğŸ’° Tushareè´¢åŠ¡æ•°æ®åŒæ­¥å·²é…ç½®: {settings.TUSHARE_FINANCIAL_SYNC_CRON}")

        # çŠ¶æ€æ£€æŸ¥ä»»åŠ¡
        scheduler.add_job(
            run_tushare_status_check,
            CronTrigger.from_crontab(settings.TUSHARE_STATUS_CHECK_CRON, timezone=settings.TIMEZONE),
            id="tushare_status_check",
            name="æ•°æ®æºçŠ¶æ€æ£€æŸ¥ï¼ˆTushareï¼‰"
        )
        if not (settings.TUSHARE_UNIFIED_ENABLED and settings.TUSHARE_STATUS_CHECK_ENABLED):
            scheduler.pause_job("tushare_status_check")
            logger.info(f"â¸ï¸ TushareçŠ¶æ€æ£€æŸ¥å·²æ·»åŠ ä½†æš‚åœ: {settings.TUSHARE_STATUS_CHECK_CRON}")
        else:
            logger.info(f"ğŸ” TushareçŠ¶æ€æ£€æŸ¥å·²é…ç½®: {settings.TUSHARE_STATUS_CHECK_CRON}")

        # AKShareç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡é…ç½®
        logger.info("ğŸ”„ é…ç½®AKShareç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡...")

        # åŸºç¡€ä¿¡æ¯åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_akshare_basic_info_sync,
            CronTrigger.from_crontab(settings.AKSHARE_BASIC_INFO_SYNC_CRON, timezone=settings.TIMEZONE),
            id="akshare_basic_info_sync",
            name="è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ï¼ˆAKShareï¼‰",
            kwargs={"force_update": False}
        )
        if not (settings.AKSHARE_UNIFIED_ENABLED and settings.AKSHARE_BASIC_INFO_SYNC_ENABLED):
            scheduler.pause_job("akshare_basic_info_sync")
            logger.info(f"â¸ï¸ AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.AKSHARE_BASIC_INFO_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“… AKShareåŸºç¡€ä¿¡æ¯åŒæ­¥å·²é…ç½®: {settings.AKSHARE_BASIC_INFO_SYNC_CRON}")

        # å®æ—¶è¡Œæƒ…åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_akshare_quotes_sync,
            CronTrigger.from_crontab(settings.AKSHARE_QUOTES_SYNC_CRON, timezone=settings.TIMEZONE),
            id="akshare_quotes_sync",
            name="å®æ—¶è¡Œæƒ…åŒæ­¥ï¼ˆAKShareï¼‰"
        )
        if not (settings.AKSHARE_UNIFIED_ENABLED and settings.AKSHARE_QUOTES_SYNC_ENABLED):
            scheduler.pause_job("akshare_quotes_sync")
            logger.info(f"â¸ï¸ AKShareè¡Œæƒ…åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.AKSHARE_QUOTES_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“ˆ AKShareè¡Œæƒ…åŒæ­¥å·²é…ç½®: {settings.AKSHARE_QUOTES_SYNC_CRON}")

        # å†å²æ•°æ®åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_akshare_historical_sync,
            CronTrigger.from_crontab(settings.AKSHARE_HISTORICAL_SYNC_CRON, timezone=settings.TIMEZONE),
            id="akshare_historical_sync",
            name="å†å²æ•°æ®åŒæ­¥ï¼ˆAKShareï¼‰",
            kwargs={"incremental": True}
        )
        if not (settings.AKSHARE_UNIFIED_ENABLED and settings.AKSHARE_HISTORICAL_SYNC_ENABLED):
            scheduler.pause_job("akshare_historical_sync")
            logger.info(f"â¸ï¸ AKShareå†å²æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.AKSHARE_HISTORICAL_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“Š AKShareå†å²æ•°æ®åŒæ­¥å·²é…ç½®: {settings.AKSHARE_HISTORICAL_SYNC_CRON}")

        # è´¢åŠ¡æ•°æ®åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_akshare_financial_sync,
            CronTrigger.from_crontab(settings.AKSHARE_FINANCIAL_SYNC_CRON, timezone=settings.TIMEZONE),
            id="akshare_financial_sync",
            name="è´¢åŠ¡æ•°æ®åŒæ­¥ï¼ˆAKShareï¼‰"
        )
        if not (settings.AKSHARE_UNIFIED_ENABLED and settings.AKSHARE_FINANCIAL_SYNC_ENABLED):
            scheduler.pause_job("akshare_financial_sync")
            logger.info(f"â¸ï¸ AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.AKSHARE_FINANCIAL_SYNC_CRON}")
        else:
            logger.info(f"ğŸ’° AKShareè´¢åŠ¡æ•°æ®åŒæ­¥å·²é…ç½®: {settings.AKSHARE_FINANCIAL_SYNC_CRON}")

        # çŠ¶æ€æ£€æŸ¥ä»»åŠ¡
        scheduler.add_job(
            run_akshare_status_check,
            CronTrigger.from_crontab(settings.AKSHARE_STATUS_CHECK_CRON, timezone=settings.TIMEZONE),
            id="akshare_status_check",
            name="æ•°æ®æºçŠ¶æ€æ£€æŸ¥ï¼ˆAKShareï¼‰"
        )
        if not (settings.AKSHARE_UNIFIED_ENABLED and settings.AKSHARE_STATUS_CHECK_ENABLED):
            scheduler.pause_job("akshare_status_check")
            logger.info(f"â¸ï¸ AKShareçŠ¶æ€æ£€æŸ¥å·²æ·»åŠ ä½†æš‚åœ: {settings.AKSHARE_STATUS_CHECK_CRON}")
        else:
            logger.info(f"ğŸ” AKShareçŠ¶æ€æ£€æŸ¥å·²é…ç½®: {settings.AKSHARE_STATUS_CHECK_CRON}")

        # BaoStockç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡é…ç½®
        logger.info("ğŸ”„ é…ç½®BaoStockç»Ÿä¸€æ•°æ®åŒæ­¥ä»»åŠ¡...")

        # åŸºç¡€ä¿¡æ¯åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_baostock_basic_info_sync,
            CronTrigger.from_crontab(settings.BAOSTOCK_BASIC_INFO_SYNC_CRON, timezone=settings.TIMEZONE),
            id="baostock_basic_info_sync",
            name="è‚¡ç¥¨åŸºç¡€ä¿¡æ¯åŒæ­¥ï¼ˆBaoStockï¼‰"
        )
        if not (settings.BAOSTOCK_UNIFIED_ENABLED and settings.BAOSTOCK_BASIC_INFO_SYNC_ENABLED):
            scheduler.pause_job("baostock_basic_info_sync")
            logger.info(f"â¸ï¸ BaoStockåŸºç¡€ä¿¡æ¯åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.BAOSTOCK_BASIC_INFO_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“‹ BaoStockåŸºç¡€ä¿¡æ¯åŒæ­¥å·²é…ç½®: {settings.BAOSTOCK_BASIC_INFO_SYNC_CRON}")

        # æ—¥Kçº¿åŒæ­¥ä»»åŠ¡ï¼ˆæ³¨æ„ï¼šBaoStockä¸æ”¯æŒå®æ—¶è¡Œæƒ…ï¼‰
        scheduler.add_job(
            run_baostock_daily_quotes_sync,
            CronTrigger.from_crontab(settings.BAOSTOCK_DAILY_QUOTES_SYNC_CRON, timezone=settings.TIMEZONE),
            id="baostock_daily_quotes_sync",
            name="æ—¥Kçº¿æ•°æ®åŒæ­¥ï¼ˆBaoStockï¼‰"
        )
        if not (settings.BAOSTOCK_UNIFIED_ENABLED and settings.BAOSTOCK_DAILY_QUOTES_SYNC_ENABLED):
            scheduler.pause_job("baostock_daily_quotes_sync")
            logger.info(f"â¸ï¸ BaoStockæ—¥Kçº¿åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.BAOSTOCK_DAILY_QUOTES_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“ˆ BaoStockæ—¥Kçº¿åŒæ­¥å·²é…ç½®: {settings.BAOSTOCK_DAILY_QUOTES_SYNC_CRON} (æ³¨æ„ï¼šBaoStockä¸æ”¯æŒå®æ—¶è¡Œæƒ…)")

        # å†å²æ•°æ®åŒæ­¥ä»»åŠ¡
        scheduler.add_job(
            run_baostock_historical_sync,
            CronTrigger.from_crontab(settings.BAOSTOCK_HISTORICAL_SYNC_CRON, timezone=settings.TIMEZONE),
            id="baostock_historical_sync",
            name="å†å²æ•°æ®åŒæ­¥ï¼ˆBaoStockï¼‰"
        )
        if not (settings.BAOSTOCK_UNIFIED_ENABLED and settings.BAOSTOCK_HISTORICAL_SYNC_ENABLED):
            scheduler.pause_job("baostock_historical_sync")
            logger.info(f"â¸ï¸ BaoStockå†å²æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.BAOSTOCK_HISTORICAL_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“Š BaoStockå†å²æ•°æ®åŒæ­¥å·²é…ç½®: {settings.BAOSTOCK_HISTORICAL_SYNC_CRON}")

        # çŠ¶æ€æ£€æŸ¥ä»»åŠ¡
        scheduler.add_job(
            run_baostock_status_check,
            CronTrigger.from_crontab(settings.BAOSTOCK_STATUS_CHECK_CRON, timezone=settings.TIMEZONE),
            id="baostock_status_check",
            name="æ•°æ®æºçŠ¶æ€æ£€æŸ¥ï¼ˆBaoStockï¼‰"
        )
        if not (settings.BAOSTOCK_UNIFIED_ENABLED and settings.BAOSTOCK_STATUS_CHECK_ENABLED):
            scheduler.pause_job("baostock_status_check")
            logger.info(f"â¸ï¸ BaoStockçŠ¶æ€æ£€æŸ¥å·²æ·»åŠ ä½†æš‚åœ: {settings.BAOSTOCK_STATUS_CHECK_CRON}")
        else:
            logger.info(f"ğŸ” BaoStockçŠ¶æ€æ£€æŸ¥å·²é…ç½®: {settings.BAOSTOCK_STATUS_CHECK_CRON}")

        # æ–°é—»æ•°æ®åŒæ­¥ä»»åŠ¡é…ç½®ï¼ˆä½¿ç”¨AKShareåŒæ­¥æ‰€æœ‰è‚¡ç¥¨æ–°é—»ï¼‰
        logger.info("ğŸ”„ é…ç½®æ–°é—»æ•°æ®åŒæ­¥ä»»åŠ¡...")

        from app.worker.akshare_sync_service import get_akshare_sync_service

        async def run_news_sync():
            """è¿è¡Œæ–°é—»åŒæ­¥ä»»åŠ¡ - ä½¿ç”¨AKShareåŒæ­¥è‡ªé€‰è‚¡æ–°é—»"""
            try:
                logger.info("ğŸ“° å¼€å§‹æ–°é—»æ•°æ®åŒæ­¥ï¼ˆAKShare - ä»…è‡ªé€‰è‚¡ï¼‰...")
                service = await get_akshare_sync_service()
                result = await service.sync_news_data(
                    symbols=None,  # None + favorites_only=True è¡¨ç¤ºåªåŒæ­¥è‡ªé€‰è‚¡
                    max_news_per_stock=settings.NEWS_SYNC_MAX_PER_SOURCE,
                    favorites_only=True  # åªåŒæ­¥è‡ªé€‰è‚¡
                )
                logger.info(
                    f"âœ… æ–°é—»åŒæ­¥å®Œæˆ: "
                    f"å¤„ç†{result['total_processed']}åªè‡ªé€‰è‚¡, "
                    f"æˆåŠŸ{result['success_count']}åª, "
                    f"å¤±è´¥{result['error_count']}åª, "
                    f"æ–°é—»æ€»æ•°{result['news_count']}æ¡, "
                    f"è€—æ—¶{(datetime.utcnow() - result['start_time']).total_seconds():.2f}ç§’"
                )
            except Exception as e:
                logger.error(f"âŒ æ–°é—»åŒæ­¥å¤±è´¥: {e}", exc_info=True)

        # ==================== æ¸¯è‚¡/ç¾è‚¡æ•°æ®é…ç½® ====================
        # æ¸¯è‚¡å’Œç¾è‚¡é‡‡ç”¨æŒ‰éœ€è·å–+ç¼“å­˜æ¨¡å¼ï¼Œä¸å†é…ç½®å®šæ—¶åŒæ­¥ä»»åŠ¡
        logger.info("ğŸ‡­ğŸ‡° æ¸¯è‚¡æ•°æ®é‡‡ç”¨æŒ‰éœ€è·å–+ç¼“å­˜æ¨¡å¼")
        logger.info("ğŸ‡ºğŸ‡¸ ç¾è‚¡æ•°æ®é‡‡ç”¨æŒ‰éœ€è·å–+ç¼“å­˜æ¨¡å¼")

        scheduler.add_job(
            run_news_sync,
            CronTrigger.from_crontab(settings.NEWS_SYNC_CRON, timezone=settings.TIMEZONE),
            id="news_sync",
            name="æ–°é—»æ•°æ®åŒæ­¥ï¼ˆAKShare - ä»…è‡ªé€‰è‚¡ï¼‰"
        )
        if not settings.NEWS_SYNC_ENABLED:
            scheduler.pause_job("news_sync")
            logger.info(f"â¸ï¸ æ–°é—»æ•°æ®åŒæ­¥å·²æ·»åŠ ä½†æš‚åœ: {settings.NEWS_SYNC_CRON}")
        else:
            logger.info(f"ğŸ“° æ–°é—»æ•°æ®åŒæ­¥å·²é…ç½®ï¼ˆä»…è‡ªé€‰è‚¡ï¼‰: {settings.NEWS_SYNC_CRON}")

        scheduler.start()

        # è®¾ç½®è°ƒåº¦å™¨å®ä¾‹åˆ°æœåŠ¡ä¸­ï¼Œä»¥ä¾¿APIå¯ä»¥ç®¡ç†ä»»åŠ¡
        set_scheduler_instance(scheduler)
        logger.info("âœ… è°ƒåº¦å™¨æœåŠ¡å·²åˆå§‹åŒ–")
    except Exception as e:
        logger.error(f"âŒ è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        raise  # æŠ›å‡ºå¼‚å¸¸ï¼Œé˜»æ­¢åº”ç”¨å¯åŠ¨

    try:
        yield
    finally:
        # å…³é—­æ—¶æ¸…ç†
        if scheduler:
            try:
                scheduler.shutdown(wait=False)
                logger.info("ğŸ›‘ Scheduler stopped")
            except Exception as e:
                logger.warning(f"Scheduler shutdown error: {e}")

        # å…³é—­ UserService MongoDB è¿æ¥
        try:
            from app.services.user_service import user_service
            user_service.close()
        except Exception as e:
            logger.warning(f"UserService cleanup error: {e}")

        await close_db()
        logger.info("TradingAgents FastAPI backend stopped")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="TradingAgents-CN API",
    description="è‚¡ç¥¨åˆ†æä¸æ‰¹é‡é˜Ÿåˆ—ç³»ç»Ÿ API",
    version=get_version(),
    docs_url="/docs" if settings.DEBUG else None,
    redoc_url="/redoc" if settings.DEBUG else None,
    lifespan=lifespan
)

# å®‰å…¨ä¸­é—´ä»¶
if not settings.DEBUG:
    app.add_middleware(
        TrustedHostMiddleware,
        allowed_hosts=settings.ALLOWED_HOSTS
    )

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)


# æ“ä½œæ—¥å¿—ä¸­é—´ä»¶
app.add_middleware(OperationLogMiddleware)


# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()

    # è·³è¿‡å¥åº·æ£€æŸ¥å’Œé™æ€æ–‡ä»¶è¯·æ±‚çš„æ—¥å¿—
    if request.url.path in ["/health", "/favicon.ico"] or request.url.path.startswith("/static"):
        response = await call_next(request)
        return response

    # ä½¿ç”¨webapi loggerè®°å½•è¯·æ±‚
    logger = logging.getLogger("webapi")
    logger.info(f"ğŸ”„ {request.method} {request.url.path} - å¼€å§‹å¤„ç†")

    response = await call_next(request)
    process_time = time.time() - start_time

    # è®°å½•è¯·æ±‚å®Œæˆ
    status_emoji = "âœ…" if response.status_code < 400 else "âŒ"
    logger.info(f"{status_emoji} {request.method} {request.url.path} - çŠ¶æ€: {response.status_code} - è€—æ—¶: {process_time:.3f}s")

    return response


# å…¨å±€å¼‚å¸¸å¤„ç†
# è¯·æ±‚ID/Trace-ID ä¸­é—´ä»¶ï¼ˆéœ€ä½œä¸ºæœ€å¤–å±‚ï¼Œæ”¾åœ¨å‡½æ•°å¼ä¸­é—´ä»¶ä¹‹åï¼‰
from app.middleware.request_id import RequestIDMiddleware
app.add_middleware(RequestIDMiddleware)

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logging.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "code": "INTERNAL_SERVER_ERROR",
                "message": "Internal server error occurred",
                "request_id": getattr(request.state, "request_id", None)
            }
        }
    )


# æµ‹è¯•ç«¯ç‚¹ - éªŒè¯ä¸­é—´ä»¶æ˜¯å¦å·¥ä½œ
@app.get("/api/test-log")
async def test_log():
    """æµ‹è¯•æ—¥å¿—ä¸­é—´ä»¶æ˜¯å¦å·¥ä½œ"""
    print("ğŸ§ª æµ‹è¯•ç«¯ç‚¹è¢«è°ƒç”¨ - è¿™æ¡æ¶ˆæ¯åº”è¯¥å‡ºç°åœ¨æ§åˆ¶å°")
    return {"message": "æµ‹è¯•æˆåŠŸ", "timestamp": time.time()}

# æ³¨å†Œè·¯ç”±
app.include_router(health.router, prefix="/api", tags=["health"])
app.include_router(auth.router, prefix="/api/auth", tags=["authentication"])
app.include_router(analysis.router, prefix="/api/analysis", tags=["analysis"])
app.include_router(reports.router, tags=["reports"])
app.include_router(screening.router, prefix="/api/screening", tags=["screening"])
app.include_router(queue.router, prefix="/api/queue", tags=["queue"])
app.include_router(favorites.router, prefix="/api", tags=["favorites"])
app.include_router(stocks_router.router, prefix="/api", tags=["stocks"])
app.include_router(multi_market_stocks_router.router, prefix="/api", tags=["multi-market"])
app.include_router(stock_data_router.router, tags=["stock-data"])
app.include_router(stock_sync_router.router, tags=["stock-sync"])
app.include_router(tags.router, prefix="/api", tags=["tags"])
app.include_router(config.router, prefix="/api", tags=["config"])
app.include_router(model_capabilities.router, tags=["model-capabilities"])
app.include_router(usage_statistics.router, tags=["usage-statistics"])
app.include_router(database.router, prefix="/api/system", tags=["database"])
app.include_router(cache.router, tags=["cache"])
app.include_router(operation_logs.router, prefix="/api/system", tags=["operation_logs"])
app.include_router(logs.router, prefix="/api/system", tags=["logs"])
# æ–°å¢ï¼šç³»ç»Ÿé…ç½®åªè¯»æ‘˜è¦
from app.routers import system_config as system_config_router
app.include_router(system_config_router.router, prefix="/api/system", tags=["system"])

# é€šçŸ¥æ¨¡å—ï¼ˆREST + SSEï¼‰
app.include_router(notifications_router.router, prefix="/api", tags=["notifications"])

# ğŸ”¥ WebSocket é€šçŸ¥æ¨¡å—ï¼ˆæ›¿ä»£ SSE + Redis PubSubï¼‰
app.include_router(websocket_notifications_router.router, prefix="/api", tags=["websocket"])

# å®šæ—¶ä»»åŠ¡ç®¡ç†
app.include_router(scheduler_router.router, tags=["scheduler"])

app.include_router(sse.router, prefix="/api/stream", tags=["streaming"])
app.include_router(sync_router.router)
app.include_router(multi_source_sync.router)
app.include_router(paper_router.router, prefix="/api", tags=["paper"])
app.include_router(tushare_init.router, prefix="/api", tags=["tushare-init"])
app.include_router(akshare_init.router, prefix="/api", tags=["akshare-init"])
app.include_router(baostock_init.router, prefix="/api", tags=["baostock-init"])
app.include_router(historical_data.router, tags=["historical-data"])
app.include_router(multi_period_sync.router, tags=["multi-period-sync"])
app.include_router(financial_data.router, tags=["financial-data"])
app.include_router(news_data.router, tags=["news-data"])
app.include_router(social_media.router, tags=["social-media"])
app.include_router(internal_messages.router, tags=["internal-messages"])


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›APIä¿¡æ¯"""
    print("ğŸ  æ ¹è·¯å¾„è¢«è®¿é—®")
    return {
        "name": "TradingAgents-CN API",
        "version": get_version(),
        "status": "running",
        "docs_url": "/docs" if settings.DEBUG else None
    }


if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info",
        reload_dirs=["app"] if settings.DEBUG else None,
        reload_excludes=[
            "__pycache__",
            "*.pyc",
            "*.pyo",
            "*.pyd",
            ".git",
            ".pytest_cache",
            "*.log",
            "*.tmp"
        ] if settings.DEBUG else None,
        reload_includes=["*.py"] if settings.DEBUG else None
    )