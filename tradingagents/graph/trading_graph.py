# TradingAgents/graph/trading_graph.py

import os
from pathlib import Path
import json
from datetime import date
from typing import Dict, Any, Tuple, List, Optional
import time

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from tradingagents.llm_adapters import ChatDashScopeOpenAI, ChatGoogleOpenAI

from langgraph.prebuilt import ToolNode

from tradingagents.agents import *
from tradingagents.default_config import DEFAULT_CONFIG
from tradingagents.agents.utils.memory import FinancialSituationMemory

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')
from tradingagents.agents.utils.agent_states import (
    AgentState,
    InvestDebateState,
    RiskDebateState,
)
from tradingagents.dataflows.interface import set_config

from .conditional_logic import ConditionalLogic
from .setup import GraphSetup
from .propagation import Propagator
from .reflection import Reflector
from .signal_processing import SignalProcessor


def create_llm_by_provider(provider: str, model: str, backend_url: str, temperature: float, max_tokens: int, timeout: int, api_key: str = None):
    """
    æ ¹æ® provider åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹

    Args:
        provider: ä¾›åº”å•†åç§° (google, dashscope, deepseek, openai, etc.)
        model: æ¨¡å‹åç§°
        backend_url: API åœ°å€
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
        timeout: è¶…æ—¶æ—¶é—´
        api_key: API Keyï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰

    Returns:
        LLM å®ä¾‹
    """
    from tradingagents.llm_adapters.deepseek_adapter import ChatDeepSeek
    from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

    logger.info(f"ğŸ”§ [åˆ›å»ºLLM] provider={provider}, model={model}, url={backend_url}")
    logger.info(f"ğŸ”‘ [API Key] æ¥æº: {'æ•°æ®åº“é…ç½®' if api_key else 'ç¯å¢ƒå˜é‡'}")

    if provider.lower() == "google":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        google_api_key = api_key or os.getenv('GOOGLE_API_KEY')
        if not google_api_key:
            raise ValueError("ä½¿ç”¨Googleéœ€è¦è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")

        # ä¼ é€’ base_url å‚æ•°ï¼Œä½¿å‚å®¶é…ç½®çš„ default_base_url ç”Ÿæ•ˆ
        return ChatGoogleOpenAI(
            model=model,
            google_api_key=google_api_key,
            base_url=backend_url if backend_url else None,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "dashscope":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        dashscope_api_key = api_key or os.getenv('DASHSCOPE_API_KEY')

        # ä¼ é€’ base_url å‚æ•°ï¼Œä½¿å‚å®¶é…ç½®çš„ default_base_url ç”Ÿæ•ˆ
        return ChatDashScopeOpenAI(
            model=model,
            api_key=dashscope_api_key,  # ğŸ”¥ ä¼ é€’ API Key
            base_url=backend_url if backend_url else None,  # å¦‚æœæœ‰è‡ªå®šä¹‰ URL åˆ™ä½¿ç”¨
            temperature=temperature,
            max_tokens=max_tokens,
            request_timeout=timeout
        )

    elif provider.lower() == "deepseek":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        deepseek_api_key = api_key or os.getenv('DEEPSEEK_API_KEY')
        if not deepseek_api_key:
            raise ValueError("ä½¿ç”¨DeepSeekéœ€è¦è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")

        return ChatDeepSeek(
            model=model,
            api_key=deepseek_api_key,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "zhipu":
        # æ™ºè°±AIå¤„ç†
        zhipu_api_key = api_key or os.getenv('ZHIPU_API_KEY')
        if not zhipu_api_key:
            raise ValueError("ä½¿ç”¨æ™ºè°±AIéœ€è¦è®¾ç½®ZHIPU_API_KEYç¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")
        
        return create_openai_compatible_llm(
            provider="zhipu",
            model=model,
            api_key=zhipu_api_key,
            base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·æä¾›çš„backend_url
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() in ["openai", "siliconflow", "openrouter", "ollama"]:
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        if not api_key:
            if provider.lower() == "siliconflow":
                api_key = os.getenv('SILICONFLOW_API_KEY')
            elif provider.lower() == "openrouter":
                api_key = os.getenv('OPENROUTER_API_KEY') or os.getenv('OPENAI_API_KEY')
            elif provider.lower() == "openai":
                api_key = os.getenv('OPENAI_API_KEY')

        return ChatOpenAI(
            model=model,
            base_url=backend_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "anthropic":
        return ChatAnthropic(
            model=model,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() in ["qianfan", "custom_openai"]:
        return create_openai_compatible_llm(
            provider=provider,
            model=model,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    else:
        # ğŸ”§ è‡ªå®šä¹‰å‚å®¶ï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        logger.info(f"ğŸ”§ ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼å¤„ç†è‡ªå®šä¹‰å‚å®¶: {provider}")

        # å°è¯•ä»ç¯å¢ƒå˜é‡è·å– API Keyï¼ˆæ”¯æŒå¤šç§å‘½åæ ¼å¼ï¼‰
        api_key_candidates = [
            f"{provider.upper()}_API_KEY",  # ä¾‹å¦‚: KYX_API_KEY
            f"{provider}_API_KEY",          # ä¾‹å¦‚: kyx_API_KEY
            "CUSTOM_OPENAI_API_KEY"         # é€šç”¨ç¯å¢ƒå˜é‡
        ]

        custom_api_key = None
        for env_var in api_key_candidates:
            custom_api_key = os.getenv(env_var)
            if custom_api_key:
                logger.info(f"âœ… ä»ç¯å¢ƒå˜é‡ {env_var} è·å–åˆ° API Key")
                break

        if not custom_api_key:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è‡ªå®šä¹‰å‚å®¶ {provider} çš„ API Keyï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®")

        return ChatOpenAI(
            model=model,
            base_url=backend_url,
            api_key=custom_api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )


class TradingAgentsGraph:
    """Main class that orchestrates the trading agents framework."""

    def __init__(
        self,
        selected_analysts=["market", "social", "news", "fundamentals"],
        debug=False,
        config: Dict[str, Any] = None,
    ):
        """Initialize the trading agents graph and components.

        Args:
            selected_analysts: List of analyst types to include
            debug: Whether to run in debug mode
            config: Configuration dictionary. If None, uses default config
        """
        self.debug = debug
        self.config = config or DEFAULT_CONFIG

        # Update the interface's config
        set_config(self.config)

        # Create necessary directories
        os.makedirs(
            os.path.join(self.config["project_dir"], "dataflows/data_cache"),
            exist_ok=True,
        )

        # Initialize LLMs
        # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        quick_config = self.config.get("quick_model_config", {})
        deep_config = self.config.get("deep_model_config", {})

        # è¯»å–å¿«é€Ÿæ¨¡å‹å‚æ•°
        quick_max_tokens = quick_config.get("max_tokens", 4000)
        quick_temperature = quick_config.get("temperature", 0.7)
        quick_timeout = quick_config.get("timeout", 180)

        # è¯»å–æ·±åº¦æ¨¡å‹å‚æ•°
        deep_max_tokens = deep_config.get("max_tokens", 4000)
        deep_temperature = deep_config.get("temperature", 0.7)
        deep_timeout = deep_config.get("timeout", 180)

        # ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆæ¨¡å¼ï¼ˆå¿«é€Ÿæ¨¡å‹å’Œæ·±åº¦æ¨¡å‹æ¥è‡ªä¸åŒå‚å®¶ï¼‰
        quick_provider = self.config.get("quick_provider")
        deep_provider = self.config.get("deep_provider")
        quick_backend_url = self.config.get("quick_backend_url")
        deep_backend_url = self.config.get("deep_backend_url")

        if quick_provider and deep_provider and quick_provider != deep_provider:
            # æ··åˆæ¨¡å¼ï¼šå¿«é€Ÿæ¨¡å‹å’Œæ·±åº¦æ¨¡å‹æ¥è‡ªä¸åŒå‚å®¶
            logger.info(f"ğŸ”€ [æ··åˆæ¨¡å¼] æ£€æµ‹åˆ°ä¸åŒå‚å®¶çš„æ¨¡å‹ç»„åˆ")
            logger.info(f"   å¿«é€Ÿæ¨¡å‹: {self.config['quick_think_llm']} ({quick_provider})")
            logger.info(f"   æ·±åº¦æ¨¡å‹: {self.config['deep_think_llm']} ({deep_provider})")

            # ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°åˆ›å»º LLM å®ä¾‹
            self.quick_thinking_llm = create_llm_by_provider(
                provider=quick_provider,
                model=self.config["quick_think_llm"],
                backend_url=quick_backend_url or self.config.get("backend_url", ""),
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout,
                api_key=self.config.get("quick_api_key")  # ğŸ”¥ ä¼ é€’ API Key
            )

            self.deep_thinking_llm = create_llm_by_provider(
                provider=deep_provider,
                model=self.config["deep_think_llm"],
                backend_url=deep_backend_url or self.config.get("backend_url", ""),
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout,
                api_key=self.config.get("deep_api_key")  # ğŸ”¥ ä¼ é€’ API Key
            )

            logger.info(f"âœ… [æ··åˆæ¨¡å¼] LLM å®ä¾‹åˆ›å»ºæˆåŠŸ")

        elif self.config["llm_provider"].lower() == "openai":
            logger.info(f"ğŸ”§ [OpenAI-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [OpenAI-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "siliconflow":
            # SiliconFlowæ”¯æŒï¼šä½¿ç”¨OpenAIå…¼å®¹API
            siliconflow_api_key = os.getenv('SILICONFLOW_API_KEY')
            if not siliconflow_api_key:
                raise ValueError("ä½¿ç”¨SiliconFlowéœ€è¦è®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡")

            logger.info(f"ğŸŒ [SiliconFlow] ä½¿ç”¨APIå¯†é’¥: {siliconflow_api_key[:20]}...")
            logger.info(f"ğŸ”§ [SiliconFlow-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [SiliconFlow-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                api_key=siliconflow_api_key,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                api_key=siliconflow_api_key,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "openrouter":
            # OpenRouteræ”¯æŒï¼šä¼˜å…ˆä½¿ç”¨OPENROUTER_API_KEYï¼Œå¦åˆ™ä½¿ç”¨OPENAI_API_KEY
            openrouter_api_key = os.getenv('OPENROUTER_API_KEY') or os.getenv('OPENAI_API_KEY')
            if not openrouter_api_key:
                raise ValueError("ä½¿ç”¨OpenRouteréœ€è¦è®¾ç½®OPENROUTER_API_KEYæˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡")

            logger.info(f"ğŸŒ [OpenRouter] ä½¿ç”¨APIå¯†é’¥: {openrouter_api_key[:20]}...")
            logger.info(f"ğŸ”§ [OpenRouter-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [OpenRouter-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                api_key=openrouter_api_key,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                api_key=openrouter_api_key,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "ollama":
            logger.info(f"ğŸ”§ [Ollama-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [Ollama-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"].lower() == "anthropic":
            logger.info(f"ğŸ”§ [Anthropic-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [Anthropic-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatAnthropic(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatAnthropic(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"].lower() == "google":
            # ä½¿ç”¨ Google OpenAI å…¼å®¹é€‚é…å™¨ï¼Œè§£å†³å·¥å…·è°ƒç”¨æ ¼å¼ä¸åŒ¹é…é—®é¢˜
            logger.info(f"ğŸ”§ ä½¿ç”¨Google AI OpenAI å…¼å®¹é€‚é…å™¨ (è§£å†³å·¥å…·è°ƒç”¨é—®é¢˜)")

            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            google_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('GOOGLE_API_KEY')
            if not google_api_key:
                raise ValueError("ä½¿ç”¨Google AIéœ€è¦åœ¨æ•°æ®åº“ä¸­é…ç½®API Keyæˆ–è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡")

            logger.info(f"ğŸ”‘ [Google AI] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'ç¯å¢ƒå˜é‡'}")

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [Google-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [Google-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # è·å– backend_urlï¼ˆå¦‚æœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ğŸ”§ [Google AI] ä½¿ç”¨é…ç½®çš„ backend_url: {backend_url}")
            else:
                logger.info(f"ğŸ”§ [Google AI] æœªé…ç½® backend_urlï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")

            self.deep_thinking_llm = ChatGoogleOpenAI(
                model=self.config["deep_think_llm"],
                google_api_key=google_api_key,
                base_url=backend_url if backend_url else None,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatGoogleOpenAI(
                model=self.config["quick_think_llm"],
                google_api_key=google_api_key,
                base_url=backend_url if backend_url else None,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout,
                transport="rest"
            )

            logger.info(f"âœ… [Google AI] å·²å¯ç”¨ä¼˜åŒ–çš„å·¥å…·è°ƒç”¨å’Œå†…å®¹æ ¼å¼å¤„ç†å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        elif (self.config["llm_provider"].lower() == "dashscope" or
              self.config["llm_provider"].lower() == "alibaba" or
              "dashscope" in self.config["llm_provider"].lower() or
              "é˜¿é‡Œç™¾ç‚¼" in self.config["llm_provider"]):
            # ä½¿ç”¨ OpenAI å…¼å®¹é€‚é…å™¨ï¼Œæ”¯æŒåŸç”Ÿ Function Calling
            logger.info(f"ğŸ”§ ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼ OpenAI å…¼å®¹é€‚é…å™¨ (æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨)")

            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            dashscope_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('DASHSCOPE_API_KEY')
            logger.info(f"ğŸ”‘ [é˜¿é‡Œç™¾ç‚¼] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'ç¯å¢ƒå˜é‡'}")

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            # è¯»å–å¿«é€Ÿæ¨¡å‹å‚æ•°
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            # è¯»å–æ·±åº¦æ¨¡å‹å‚æ•°
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [é˜¿é‡Œç™¾ç‚¼-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [é˜¿é‡Œç™¾ç‚¼-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # è·å– backend_urlï¼ˆå¦‚æœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ğŸ”§ [é˜¿é‡Œç™¾ç‚¼] ä½¿ç”¨è‡ªå®šä¹‰ API åœ°å€: {backend_url}")

            # ğŸ”¥ è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°æ‰€æœ‰ LLM åˆå§‹åŒ–å‚æ•°
            logger.info("=" * 80)
            logger.info("ğŸ¤– [LLMåˆå§‹åŒ–] é˜¿é‡Œç™¾ç‚¼æ·±åº¦æ¨¡å‹å‚æ•°:")
            logger.info(f"   model: {self.config['deep_think_llm']}")
            logger.info(f"   api_key: {'æœ‰å€¼' if dashscope_api_key else 'ç©º'} (é•¿åº¦: {len(dashscope_api_key) if dashscope_api_key else 0})")
            logger.info(f"   base_url: {backend_url if backend_url else 'é»˜è®¤'}")
            logger.info(f"   temperature: {deep_temperature}")
            logger.info(f"   max_tokens: {deep_max_tokens}")
            logger.info(f"   request_timeout: {deep_timeout}")
            logger.info("=" * 80)

            self.deep_thinking_llm = ChatDashScopeOpenAI(
                model=self.config["deep_think_llm"],
                api_key=dashscope_api_key,  # ğŸ”¥ ä¼ é€’ API Key
                base_url=backend_url if backend_url else None,  # ä¼ é€’ base_url
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                request_timeout=deep_timeout
            )

            logger.info("=" * 80)
            logger.info("ğŸ¤– [LLMåˆå§‹åŒ–] é˜¿é‡Œç™¾ç‚¼å¿«é€Ÿæ¨¡å‹å‚æ•°:")
            logger.info(f"   model: {self.config['quick_think_llm']}")
            logger.info(f"   api_key: {'æœ‰å€¼' if dashscope_api_key else 'ç©º'} (é•¿åº¦: {len(dashscope_api_key) if dashscope_api_key else 0})")
            logger.info(f"   base_url: {backend_url if backend_url else 'é»˜è®¤'}")
            logger.info(f"   temperature: {quick_temperature}")
            logger.info(f"   max_tokens: {quick_max_tokens}")
            logger.info(f"   request_timeout: {quick_timeout}")
            logger.info("=" * 80)

            self.quick_thinking_llm = ChatDashScopeOpenAI(
                model=self.config["quick_think_llm"],
                api_key=dashscope_api_key,  # ğŸ”¥ ä¼ é€’ API Key
                base_url=backend_url if backend_url else None,  # ä¼ é€’ base_url
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                request_timeout=quick_timeout
            )
            logger.info(f"âœ… [é˜¿é‡Œç™¾ç‚¼] å·²åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        elif (self.config["llm_provider"].lower() == "deepseek" or
              "deepseek" in self.config["llm_provider"].lower()):
            # DeepSeek V3é…ç½® - ä½¿ç”¨æ”¯æŒtokenç»Ÿè®¡çš„é€‚é…å™¨
            from tradingagents.llm_adapters.deepseek_adapter import ChatDeepSeek

            deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
            if not deepseek_api_key:
                raise ValueError("ä½¿ç”¨DeepSeekéœ€è¦è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")

            deepseek_base_url = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            # è¯»å–å¿«é€Ÿæ¨¡å‹å‚æ•°
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            # è¯»å–æ·±åº¦æ¨¡å‹å‚æ•°
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [DeepSeek-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [DeepSeek-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨æ”¯æŒtokenç»Ÿè®¡çš„DeepSeeké€‚é…å™¨
            self.deep_thinking_llm = ChatDeepSeek(
                model=self.config["deep_think_llm"],
                api_key=deepseek_api_key,
                base_url=deepseek_base_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatDeepSeek(
                model=self.config["quick_think_llm"],
                api_key=deepseek_api_key,
                base_url=deepseek_base_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [DeepSeek] å·²å¯ç”¨tokenç»Ÿè®¡åŠŸèƒ½å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "custom_openai":
            # è‡ªå®šä¹‰OpenAIç«¯ç‚¹é…ç½®
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            custom_api_key = os.getenv('CUSTOM_OPENAI_API_KEY')
            if not custom_api_key:
                raise ValueError("ä½¿ç”¨è‡ªå®šä¹‰OpenAIç«¯ç‚¹éœ€è¦è®¾ç½®CUSTOM_OPENAI_API_KEYç¯å¢ƒå˜é‡")

            custom_base_url = self.config.get("custom_openai_base_url", "https://api.openai.com/v1")

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI] ä½¿ç”¨ç«¯ç‚¹: {custom_base_url}")
            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨OpenAIå…¼å®¹é€‚é…å™¨åˆ›å»ºLLMå®ä¾‹
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["deep_think_llm"],
                base_url=custom_base_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["quick_think_llm"],
                base_url=custom_base_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [è‡ªå®šä¹‰OpenAI] å·²é…ç½®è‡ªå®šä¹‰ç«¯ç‚¹å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "qianfan":
            # ç™¾åº¦åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰é…ç½® - ç»Ÿä¸€ç”±é€‚é…å™¨å†…éƒ¨è¯»å–ä¸æ ¡éªŒ QIANFAN_API_KEY
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [åƒå¸†-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [åƒå¸†-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨OpenAIå…¼å®¹é€‚é…å™¨åˆ›å»ºLLMå®ä¾‹ï¼ˆåŸºç±»ä¼šä½¿ç”¨åƒå¸†é»˜è®¤base_urlå¹¶è´Ÿè´£å¯†é’¥æ ¡éªŒï¼‰
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="qianfan",
                model=self.config["deep_think_llm"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="qianfan",
                model=self.config["quick_think_llm"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
            logger.info("âœ… [åƒå¸†] æ–‡å¿ƒä¸€è¨€é€‚é…å™¨å·²é…ç½®æˆåŠŸå¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "zhipu":
            # æ™ºè°±AI GLMé…ç½® - ä½¿ç”¨ä¸“é—¨çš„ChatZhipuOpenAIé€‚é…å™¨
            from tradingagents.llm_adapters.openai_compatible_base import ChatZhipuOpenAI
            
            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            zhipu_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('ZHIPU_API_KEY')
            logger.info(f"ğŸ”‘ [æ™ºè°±AI] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'ç¯å¢ƒå˜é‡'}")
            
            if not zhipu_api_key:
                raise ValueError("ä½¿ç”¨æ™ºè°±AIéœ€è¦åœ¨æ•°æ®åº“ä¸­é…ç½®API Keyæˆ–è®¾ç½®ZHIPU_API_KEYç¯å¢ƒå˜é‡")
            
            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})
            
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)
            
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)
            
            logger.info(f"ğŸ”§ [æ™ºè°±AI-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [æ™ºè°±AI-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")
            
            # è·å– backend_urlï¼ˆå¦‚æœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ğŸ”§ [æ™ºè°±AI] ä½¿ç”¨é…ç½®çš„ backend_url: {backend_url}")
            else:
                logger.info(f"ğŸ”§ [æ™ºè°±AI] æœªé…ç½® backend_urlï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")
            
            # ä½¿ç”¨ä¸“é—¨çš„ChatZhipuOpenAIé€‚é…å™¨åˆ›å»ºLLMå®ä¾‹
            self.deep_thinking_llm = ChatZhipuOpenAI(
                model=self.config["deep_think_llm"],
                api_key=zhipu_api_key,
                base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„backend_url
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatZhipuOpenAI(
                model=self.config["quick_think_llm"],
                api_key=zhipu_api_key,
                base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„backend_url
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
            
            logger.info("âœ… [æ™ºè°±AI] å·²ä½¿ç”¨ä¸“ç”¨é€‚é…å™¨é…ç½®æˆåŠŸå¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        else:
            # ğŸ”§ é€šç”¨çš„ OpenAI å…¼å®¹å‚å®¶æ”¯æŒï¼ˆç”¨äºè‡ªå®šä¹‰å‚å®¶ï¼‰
            logger.info(f"ğŸ”§ ä½¿ç”¨é€šç”¨ OpenAI å…¼å®¹é€‚é…å™¨å¤„ç†è‡ªå®šä¹‰å‚å®¶: {self.config['llm_provider']}")
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            # è·å–å‚å®¶é…ç½®ä¸­çš„ API Key å’Œ base_url
            provider_name = self.config['llm_provider']

            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å– API Keyï¼ˆæ”¯æŒå¤šç§å‘½åæ ¼å¼ï¼‰
            api_key_candidates = [
                f"{provider_name.upper()}_API_KEY",  # ä¾‹å¦‚: KYX_API_KEY
                f"{provider_name}_API_KEY",          # ä¾‹å¦‚: kyx_API_KEY
                "CUSTOM_OPENAI_API_KEY"              # é€šç”¨ç¯å¢ƒå˜é‡
            ]

            custom_api_key = None
            for env_var in api_key_candidates:
                custom_api_key = os.getenv(env_var)
                if custom_api_key:
                    logger.info(f"âœ… ä»ç¯å¢ƒå˜é‡ {env_var} è·å–åˆ° API Key")
                    break

            if not custom_api_key:
                raise ValueError(
                    f"ä½¿ç”¨è‡ªå®šä¹‰å‚å®¶ {provider_name} éœ€è¦è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¹‹ä¸€:\n"
                    f"  - {provider_name.upper()}_API_KEY\n"
                    f"  - CUSTOM_OPENAI_API_KEY"
                )

            # è·å– backend_urlï¼ˆä»é…ç½®ä¸­è·å–ï¼‰
            backend_url = self.config.get("backend_url")
            if not backend_url:
                raise ValueError(
                    f"ä½¿ç”¨è‡ªå®šä¹‰å‚å®¶ {provider_name} éœ€è¦åœ¨æ•°æ®åº“é…ç½®ä¸­è®¾ç½® default_base_url"
                )

            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰å‚å®¶ {provider_name}] ä½¿ç”¨ç«¯ç‚¹: {backend_url}")

            # ğŸ”§ ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ğŸ”§ [{provider_name}-å¿«é€Ÿæ¨¡å‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ğŸ”§ [{provider_name}-æ·±åº¦æ¨¡å‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨ custom_openai é€‚é…å™¨åˆ›å»º LLM å®ä¾‹
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["deep_think_llm"],
                api_key=custom_api_key,
                base_url=backend_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["quick_think_llm"],
                api_key=custom_api_key,
                base_url=backend_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [è‡ªå®šä¹‰å‚å®¶ {provider_name}] å·²é…ç½®è‡ªå®šä¹‰ç«¯ç‚¹å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡å‹å‚æ•°")
        
        self.toolkit = Toolkit(config=self.config)

        # Initialize memories (å¦‚æœå¯ç”¨)
        memory_enabled = self.config.get("memory_enabled", True)
        if memory_enabled:
            # ä½¿ç”¨å•ä¾‹ChromaDBç®¡ç†å™¨ï¼Œé¿å…å¹¶å‘åˆ›å»ºå†²çª
            self.bull_memory = FinancialSituationMemory("bull_memory", self.config)
            self.bear_memory = FinancialSituationMemory("bear_memory", self.config)
            self.trader_memory = FinancialSituationMemory("trader_memory", self.config)
            self.invest_judge_memory = FinancialSituationMemory("invest_judge_memory", self.config)
            self.risk_manager_memory = FinancialSituationMemory("risk_manager_memory", self.config)
        else:
            # åˆ›å»ºç©ºçš„å†…å­˜å¯¹è±¡
            self.bull_memory = None
            self.bear_memory = None
            self.trader_memory = None
            self.invest_judge_memory = None
            self.risk_manager_memory = None

        # Create tool nodes
        self.tool_nodes = self._create_tool_nodes()

        # Initialize components
        # ğŸ”¥ [ä¿®å¤] ä»é…ç½®ä¸­è¯»å–è¾©è®ºè½®æ¬¡å‚æ•°
        self.conditional_logic = ConditionalLogic(
            max_debate_rounds=self.config.get("max_debate_rounds", 1),
            max_risk_discuss_rounds=self.config.get("max_risk_discuss_rounds", 1)
        )
        logger.info(f"ğŸ”§ [ConditionalLogic] åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   - max_debate_rounds: {self.conditional_logic.max_debate_rounds}")
        logger.info(f"   - max_risk_discuss_rounds: {self.conditional_logic.max_risk_discuss_rounds}")

        self.graph_setup = GraphSetup(
            self.quick_thinking_llm,
            self.deep_thinking_llm,
            self.toolkit,
            self.tool_nodes,
            self.bull_memory,
            self.bear_memory,
            self.trader_memory,
            self.invest_judge_memory,
            self.risk_manager_memory,
            self.conditional_logic,
            self.config,
            getattr(self, 'react_llm', None),
        )

        self.propagator = Propagator()
        self.reflector = Reflector(self.quick_thinking_llm)
        self.signal_processor = SignalProcessor(self.quick_thinking_llm)

        # State tracking
        self.curr_state = None
        self.ticker = None
        self.log_states_dict = {}  # date to full state dict

        # Set up the graph
        self.graph = self.graph_setup.setup_graph(selected_analysts)

    def _create_tool_nodes(self) -> Dict[str, ToolNode]:
        """Create tool nodes for different data sources.

        æ³¨æ„ï¼šToolNode åŒ…å«æ‰€æœ‰å¯èƒ½çš„å·¥å…·ï¼Œä½† LLM åªä¼šè°ƒç”¨å®ƒç»‘å®šçš„å·¥å…·ã€‚
        ToolNode çš„ä½œç”¨æ˜¯æ‰§è¡Œ LLM ç”Ÿæˆçš„ tool_callsï¼Œè€Œä¸æ˜¯é™åˆ¶ LLM å¯ä»¥è°ƒç”¨å“ªäº›å·¥å…·ã€‚
        """
        return {
            "market": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_market_data_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_YFin_data_online,
                    self.toolkit.get_stockstats_indicators_report_online,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_YFin_data,
                    self.toolkit.get_stockstats_indicators_report,
                ]
            ),
            "social": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_sentiment_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_stock_news_openai,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_reddit_stock_info,
                ]
            ),
            "news": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_news_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_global_news_openai,
                    self.toolkit.get_google_news,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_finnhub_news,
                    self.toolkit.get_reddit_news,
                ]
            ),
            "fundamentals": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæ¨èï¼‰
                    self.toolkit.get_stock_fundamentals_unified,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_finnhub_company_insider_sentiment,
                    self.toolkit.get_finnhub_company_insider_transactions,
                    self.toolkit.get_simfin_balance_sheet,
                    self.toolkit.get_simfin_cashflow,
                    self.toolkit.get_simfin_income_stmt,
                    # ä¸­å›½å¸‚åœºå·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_china_stock_data,
                    self.toolkit.get_china_fundamentals,
                ]
            ),
        }

    def propagate(self, company_name, trade_date, progress_callback=None, task_id=None):
        """Run the trading agents graph for a company on a specific date.

        Args:
            company_name: Company name or stock symbol
            trade_date: Date for analysis
            progress_callback: Optional callback function for progress updates
            task_id: Optional task ID for tracking performance data
        """

        # æ·»åŠ è¯¦ç»†çš„æ¥æ”¶æ—¥å¿—
        logger.debug(f"ğŸ” [GRAPH DEBUG] ===== TradingAgentsGraph.propagate æ¥æ”¶å‚æ•° =====")
        logger.debug(f"ğŸ” [GRAPH DEBUG] æ¥æ”¶åˆ°çš„company_name: '{company_name}' (ç±»å‹: {type(company_name)})")
        logger.debug(f"ğŸ” [GRAPH DEBUG] æ¥æ”¶åˆ°çš„trade_date: '{trade_date}' (ç±»å‹: {type(trade_date)})")
        logger.debug(f"ğŸ” [GRAPH DEBUG] æ¥æ”¶åˆ°çš„task_id: '{task_id}'")

        self.ticker = company_name
        logger.debug(f"ğŸ” [GRAPH DEBUG] è®¾ç½®self.ticker: '{self.ticker}'")

        # Initialize state
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œä¼ é€’å‚æ•°: company_name='{company_name}', trade_date='{trade_date}'")
        init_agent_state = self.propagator.create_initial_state(
            company_name, trade_date
        )
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„company_of_interest: '{init_agent_state.get('company_of_interest', 'NOT_FOUND')}'")
        logger.debug(f"ğŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„trade_date: '{init_agent_state.get('trade_date', 'NOT_FOUND')}'")

        # åˆå§‹åŒ–è®¡æ—¶å™¨
        node_timings = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´
        total_start_time = time.time()  # æ€»ä½“å¼€å§‹æ—¶é—´
        current_node_start = None  # å½“å‰èŠ‚ç‚¹å¼€å§‹æ—¶é—´
        current_node_name = None  # å½“å‰èŠ‚ç‚¹åç§°

        # ä¿å­˜task_idç”¨äºåç»­ä¿å­˜æ€§èƒ½æ•°æ®
        self._current_task_id = task_id

        # æ ¹æ®æ˜¯å¦æœ‰è¿›åº¦å›è°ƒé€‰æ‹©ä¸åŒçš„stream_mode
        args = self.propagator.get_graph_args(use_progress_callback=bool(progress_callback))

        if self.debug:
            # Debug mode with tracing and progress updates
            trace = []
            final_state = None
            for chunk in self.graph.stream(init_agent_state, **args):
                # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                for node_name in chunk.keys():
                    if not node_name.startswith('__'):
                        # å¦‚æœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                        if current_node_name and current_node_start:
                            elapsed = time.time() - current_node_start
                            node_timings[current_node_name] = elapsed
                            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

                        # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                        current_node_name = node_name
                        current_node_start = time.time()
                        break

                # åœ¨ updates æ¨¡å¼ä¸‹ï¼Œchunk æ ¼å¼ä¸º {node_name: state_update}
                # åœ¨ values æ¨¡å¼ä¸‹ï¼Œchunk æ ¼å¼ä¸ºå®Œæ•´çš„çŠ¶æ€
                if progress_callback and args.get("stream_mode") == "updates":
                    # updates æ¨¡å¼ï¼šchunk = {"Market Analyst": {...}}
                    self._send_progress_update(chunk, progress_callback)
                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            final_state.update(node_update)
                else:
                    # values æ¨¡å¼ï¼šchunk = {"messages": [...], ...}
                    if len(chunk.get("messages", [])) > 0:
                        chunk["messages"][-1].pretty_print()
                    trace.append(chunk)
                    final_state = chunk

            if not trace and final_state:
                # updates æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ç´¯ç§¯çš„çŠ¶æ€
                pass
            elif trace:
                final_state = trace[-1]
        else:
            # Standard mode without tracing but with progress updates
            if progress_callback:
                # ä½¿ç”¨ updates æ¨¡å¼ä»¥ä¾¿è·å–èŠ‚ç‚¹çº§åˆ«çš„è¿›åº¦
                trace = []
                final_state = None
                for chunk in self.graph.stream(init_agent_state, **args):
                    # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                    for node_name in chunk.keys():
                        if not node_name.startswith('__'):
                            # å¦‚æœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                            if current_node_name and current_node_start:
                                elapsed = time.time() - current_node_start
                                node_timings[current_node_name] = elapsed
                                logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")
                                logger.info(f"ğŸ” [TIMING] èŠ‚ç‚¹åˆ‡æ¢: {current_node_name} â†’ {node_name}")

                            # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                            current_node_name = node_name
                            current_node_start = time.time()
                            logger.info(f"ğŸ” [TIMING] å¼€å§‹è®¡æ—¶: {node_name}")
                            break

                    self._send_progress_update(chunk, progress_callback)
                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            final_state.update(node_update)
            else:
                # åŸæœ‰çš„invokeæ¨¡å¼ï¼ˆä¹Ÿéœ€è¦è®¡æ—¶ï¼‰
                logger.info("â±ï¸ ä½¿ç”¨ invoke æ¨¡å¼æ‰§è¡Œåˆ†æï¼ˆæ— è¿›åº¦å›è°ƒï¼‰")
                # ä½¿ç”¨streamæ¨¡å¼ä»¥ä¾¿è®¡æ—¶ï¼Œä½†ä¸å‘é€è¿›åº¦æ›´æ–°
                trace = []
                final_state = None
                for chunk in self.graph.stream(init_agent_state, **args):
                    # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                    for node_name in chunk.keys():
                        if not node_name.startswith('__'):
                            # å¦‚æœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                            if current_node_name and current_node_start:
                                elapsed = time.time() - current_node_start
                                node_timings[current_node_name] = elapsed
                                logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

                            # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                            current_node_name = node_name
                            current_node_start = time.time()
                            break

                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            final_state.update(node_update)

        # è®°å½•æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´
        if current_node_name and current_node_start:
            elapsed = time.time() - current_node_start
            node_timings[current_node_name] = elapsed
            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

        # è®¡ç®—æ€»æ—¶é—´
        total_elapsed = time.time() - total_start_time

        # è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” [TIMING DEBUG] èŠ‚ç‚¹è®¡æ—¶æ•°é‡: {len(node_timings)}")
        logger.info(f"ğŸ” [TIMING DEBUG] æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
        logger.info(f"ğŸ” [TIMING DEBUG] èŠ‚ç‚¹åˆ—è¡¨: {list(node_timings.keys())}")

        # æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡
        logger.info("ğŸ” [TIMING DEBUG] å‡†å¤‡è°ƒç”¨ _print_timing_summary")
        self._print_timing_summary(node_timings, total_elapsed)
        logger.info("ğŸ” [TIMING DEBUG] _print_timing_summary è°ƒç”¨å®Œæˆ")

        # æ„å»ºæ€§èƒ½æ•°æ®
        performance_data = self._build_performance_data(node_timings, total_elapsed)

        # å°†æ€§èƒ½æ•°æ®æ·»åŠ åˆ°çŠ¶æ€ä¸­
        final_state['performance_metrics'] = performance_data

        # Store current state for reflection
        self.curr_state = final_state

        # Log state
        self._log_state(trade_date, final_state)

        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = ""
        try:
            if hasattr(self.deep_thinking_llm, 'model_name'):
                model_info = f"{self.deep_thinking_llm.__class__.__name__}:{self.deep_thinking_llm.model_name}"
            else:
                model_info = self.deep_thinking_llm.__class__.__name__
        except Exception:
            model_info = "Unknown"

        # å¤„ç†å†³ç­–å¹¶æ·»åŠ æ¨¡å‹ä¿¡æ¯
        decision = self.process_signal(final_state["final_trade_decision"], company_name)
        decision['model_info'] = model_info

        # Return decision and processed signal
        return final_state, decision

    def _send_progress_update(self, chunk, progress_callback):
        """å‘é€è¿›åº¦æ›´æ–°åˆ°å›è°ƒå‡½æ•°

        LangGraph stream è¿”å›çš„ chunk æ ¼å¼ï¼š{node_name: {...}}
        èŠ‚ç‚¹åç§°ç¤ºä¾‹ï¼š
        - "Market Analyst", "Fundamentals Analyst", "News Analyst", "Social Analyst"
        - "tools_market", "tools_fundamentals", "tools_news", "tools_social"
        - "Msg Clear Market", "Msg Clear Fundamentals", etc.
        - "Bull Researcher", "Bear Researcher", "Research Manager"
        - "Trader"
        - "Risky Analyst", "Safe Analyst", "Neutral Analyst", "Risk Judge"
        """
        try:
            # ä»chunkä¸­æå–å½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹ä¿¡æ¯
            if not isinstance(chunk, dict):
                return

            # è·å–ç¬¬ä¸€ä¸ªéç‰¹æ®Šé”®ä½œä¸ºèŠ‚ç‚¹å
            node_name = None
            for key in chunk.keys():
                if not key.startswith('__'):
                    node_name = key
                    break

            if not node_name:
                return

            logger.info(f"ğŸ” [Progress] èŠ‚ç‚¹åç§°: {node_name}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸèŠ‚ç‚¹
            if '__end__' in chunk:
                logger.info(f"ğŸ“Š [Progress] æ£€æµ‹åˆ°__end__èŠ‚ç‚¹")
                progress_callback("ğŸ“Š ç”ŸæˆæŠ¥å‘Š")
                return

            # èŠ‚ç‚¹åç§°æ˜ å°„è¡¨ï¼ˆåŒ¹é… LangGraph å®é™…èŠ‚ç‚¹åï¼‰
            node_mapping = {
                # åˆ†æå¸ˆèŠ‚ç‚¹
                'Market Analyst': "ğŸ“Š å¸‚åœºåˆ†æå¸ˆ",
                'Fundamentals Analyst': "ğŸ’¼ åŸºæœ¬é¢åˆ†æå¸ˆ",
                'News Analyst': "ğŸ“° æ–°é—»åˆ†æå¸ˆ",
                'Social Analyst': "ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ",
                # å·¥å…·èŠ‚ç‚¹ï¼ˆä¸å‘é€è¿›åº¦æ›´æ–°ï¼Œé¿å…é‡å¤ï¼‰
                'tools_market': None,
                'tools_fundamentals': None,
                'tools_news': None,
                'tools_social': None,
                # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹ï¼ˆä¸å‘é€è¿›åº¦æ›´æ–°ï¼‰
                'Msg Clear Market': None,
                'Msg Clear Fundamentals': None,
                'Msg Clear News': None,
                'Msg Clear Social': None,
                # ç ”ç©¶å‘˜èŠ‚ç‚¹
                'Bull Researcher': "ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜",
                'Bear Researcher': "ğŸ» çœ‹è·Œç ”ç©¶å‘˜",
                'Research Manager': "ğŸ‘” ç ”ç©¶ç»ç†",
                # äº¤æ˜“å‘˜èŠ‚ç‚¹
                'Trader': "ğŸ’¼ äº¤æ˜“å‘˜å†³ç­–",
                # é£é™©è¯„ä¼°èŠ‚ç‚¹
                'Risky Analyst': "ğŸ”¥ æ¿€è¿›é£é™©è¯„ä¼°",
                'Safe Analyst': "ğŸ›¡ï¸ ä¿å®ˆé£é™©è¯„ä¼°",
                'Neutral Analyst': "âš–ï¸ ä¸­æ€§é£é™©è¯„ä¼°",
                'Risk Judge': "ğŸ¯ é£é™©ç»ç†",
            }

            # æŸ¥æ‰¾æ˜ å°„çš„æ¶ˆæ¯
            message = node_mapping.get(node_name)

            if message is None:
                # None è¡¨ç¤ºè·³è¿‡ï¼ˆå·¥å…·èŠ‚ç‚¹ã€æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹ï¼‰
                logger.debug(f"â­ï¸ [Progress] è·³è¿‡èŠ‚ç‚¹: {node_name}")
                return

            if message:
                # å‘é€è¿›åº¦æ›´æ–°
                logger.info(f"ğŸ“¤ [Progress] å‘é€è¿›åº¦æ›´æ–°: {message}")
                progress_callback(message)
            else:
                # æœªçŸ¥èŠ‚ç‚¹ï¼Œä½¿ç”¨èŠ‚ç‚¹åç§°
                logger.warning(f"âš ï¸ [Progress] æœªçŸ¥èŠ‚ç‚¹: {node_name}")
                progress_callback(f"ğŸ” {node_name}")

        except Exception as e:
            logger.error(f"âŒ è¿›åº¦æ›´æ–°å¤±è´¥: {e}", exc_info=True)

    def _build_performance_data(self, node_timings: Dict[str, float], total_elapsed: float) -> Dict[str, Any]:
        """æ„å»ºæ€§èƒ½æ•°æ®ç»“æ„

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´

        Returns:
            æ€§èƒ½æ•°æ®å­—å…¸
        """
        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£é™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºåˆ†æå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = {}
        tool_nodes = {}
        msg_clear_nodes = {}
        research_nodes = {}
        trader_nodes = {}
        risk_nodes = {}
        other_nodes = {}

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£é™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes[node_name] = elapsed
            # ç„¶ååŒ¹é…åˆ†æå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes[node_name] = elapsed
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes[node_name] = elapsed
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes[node_name] = elapsed
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes[node_name] = elapsed
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes[node_name] = elapsed
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes[node_name] = elapsed

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        slowest_node = max(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        fastest_node = min(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        avg_time = sum(node_timings.values()) / len(node_timings) if node_timings else 0

        return {
            "total_time": round(total_elapsed, 2),
            "total_time_minutes": round(total_elapsed / 60, 2),
            "node_count": len(node_timings),
            "average_node_time": round(avg_time, 2),
            "slowest_node": {
                "name": slowest_node[0],
                "time": round(slowest_node[1], 2)
            } if slowest_node[0] else None,
            "fastest_node": {
                "name": fastest_node[0],
                "time": round(fastest_node[1], 2)
            } if fastest_node[0] else None,
            "node_timings": {k: round(v, 2) for k, v in node_timings.items()},
            "category_timings": {
                "analyst_team": {
                    "nodes": {k: round(v, 2) for k, v in analyst_nodes.items()},
                    "total": round(sum(analyst_nodes.values()), 2),
                    "percentage": round(sum(analyst_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "tool_calls": {
                    "nodes": {k: round(v, 2) for k, v in tool_nodes.items()},
                    "total": round(sum(tool_nodes.values()), 2),
                    "percentage": round(sum(tool_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "message_clearing": {
                    "nodes": {k: round(v, 2) for k, v in msg_clear_nodes.items()},
                    "total": round(sum(msg_clear_nodes.values()), 2),
                    "percentage": round(sum(msg_clear_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "research_team": {
                    "nodes": {k: round(v, 2) for k, v in research_nodes.items()},
                    "total": round(sum(research_nodes.values()), 2),
                    "percentage": round(sum(research_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "trader_team": {
                    "nodes": {k: round(v, 2) for k, v in trader_nodes.items()},
                    "total": round(sum(trader_nodes.values()), 2),
                    "percentage": round(sum(trader_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "risk_management_team": {
                    "nodes": {k: round(v, 2) for k, v in risk_nodes.items()},
                    "total": round(sum(risk_nodes.values()), 2),
                    "percentage": round(sum(risk_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "other": {
                    "nodes": {k: round(v, 2) for k, v in other_nodes.items()},
                    "total": round(sum(other_nodes.values()), 2),
                    "percentage": round(sum(other_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                }
            },
            "llm_config": {
                "provider": self.config.get('llm_provider', 'unknown'),
                "deep_think_model": self.config.get('deep_think_llm', 'unknown'),
                "quick_think_model": self.config.get('quick_think_llm', 'unknown')
            }
        }

    def _print_timing_summary(self, node_timings: Dict[str, float], total_elapsed: float):
        """æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡æŠ¥å‘Š

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´
        """
        logger.info("ğŸ” [_print_timing_summary] æ–¹æ³•è¢«è°ƒç”¨")
        logger.info("ğŸ” [_print_timing_summary] node_timings æ•°é‡: " + str(len(node_timings)))
        logger.info("ğŸ” [_print_timing_summary] total_elapsed: " + str(total_elapsed))

        logger.info("=" * 80)
        logger.info("â±ï¸  åˆ†ææ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)

        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£é™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºåˆ†æå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = []
        tool_nodes = []
        msg_clear_nodes = []
        research_nodes = []
        trader_nodes = []
        risk_nodes = []
        other_nodes = []

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£é™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes.append((node_name, elapsed))
            # ç„¶ååŒ¹é…åˆ†æå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes.append((node_name, elapsed))
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes.append((node_name, elapsed))
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes.append((node_name, elapsed))
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes.append((node_name, elapsed))
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes.append((node_name, elapsed))
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes.append((node_name, elapsed))

        # æ‰“å°åˆ†ç±»ç»Ÿè®¡
        def print_category(title: str, nodes: List[Tuple[str, float]]):
            if not nodes:
                return
            logger.info(f"\nğŸ“Š {title}")
            logger.info("-" * 80)
            total_category_time = sum(t for _, t in nodes)
            for node_name, elapsed in sorted(nodes, key=lambda x: x[1], reverse=True):
                percentage = (elapsed / total_elapsed * 100) if total_elapsed > 0 else 0
                logger.info(f"  â€¢ {node_name:40s} {elapsed:8.2f}ç§’  ({percentage:5.1f}%)")
            logger.info(f"  {'å°è®¡':40s} {total_category_time:8.2f}ç§’  ({total_category_time/total_elapsed*100:5.1f}%)")

        print_category("åˆ†æå¸ˆå›¢é˜Ÿ", analyst_nodes)
        print_category("å·¥å…·è°ƒç”¨", tool_nodes)
        print_category("æ¶ˆæ¯æ¸…ç†", msg_clear_nodes)
        print_category("ç ”ç©¶å›¢é˜Ÿ", research_nodes)
        print_category("äº¤æ˜“å›¢é˜Ÿ", trader_nodes)
        print_category("é£é™©ç®¡ç†å›¢é˜Ÿ", risk_nodes)
        print_category("å…¶ä»–èŠ‚ç‚¹", other_nodes)

        # æ‰“å°æ€»ä½“ç»Ÿè®¡
        logger.info("\n" + "=" * 80)
        logger.info(f"ğŸ¯ æ€»æ‰§è¡Œæ—¶é—´: {total_elapsed:.2f}ç§’ ({total_elapsed/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"ğŸ“ˆ èŠ‚ç‚¹æ€»æ•°: {len(node_timings)}")
        if node_timings:
            avg_time = sum(node_timings.values()) / len(node_timings)
            logger.info(f"â±ï¸  å¹³å‡èŠ‚ç‚¹è€—æ—¶: {avg_time:.2f}ç§’")
            slowest_node = max(node_timings.items(), key=lambda x: x[1])
            logger.info(f"ğŸŒ æœ€æ…¢èŠ‚ç‚¹: {slowest_node[0]} ({slowest_node[1]:.2f}ç§’)")
            fastest_node = min(node_timings.items(), key=lambda x: x[1])
            logger.info(f"âš¡ æœ€å¿«èŠ‚ç‚¹: {fastest_node[0]} ({fastest_node[1]:.2f}ç§’)")

        # æ‰“å°LLMé…ç½®ä¿¡æ¯
        logger.info(f"\nğŸ¤– LLMé…ç½®:")
        logger.info(f"  â€¢ æä¾›å•†: {self.config.get('llm_provider', 'unknown')}")
        logger.info(f"  â€¢ æ·±åº¦æ€è€ƒæ¨¡å‹: {self.config.get('deep_think_llm', 'unknown')}")
        logger.info(f"  â€¢ å¿«é€Ÿæ€è€ƒæ¨¡å‹: {self.config.get('quick_think_llm', 'unknown')}")
        logger.info("=" * 80)

    def _log_state(self, trade_date, final_state):
        """Log the final state to a JSON file."""
        self.log_states_dict[str(trade_date)] = {
            "company_of_interest": final_state["company_of_interest"],
            "trade_date": final_state["trade_date"],
            "market_report": final_state["market_report"],
            "sentiment_report": final_state["sentiment_report"],
            "news_report": final_state["news_report"],
            "fundamentals_report": final_state["fundamentals_report"],
            "investment_debate_state": {
                "bull_history": final_state["investment_debate_state"]["bull_history"],
                "bear_history": final_state["investment_debate_state"]["bear_history"],
                "history": final_state["investment_debate_state"]["history"],
                "current_response": final_state["investment_debate_state"][
                    "current_response"
                ],
                "judge_decision": final_state["investment_debate_state"][
                    "judge_decision"
                ],
            },
            "trader_investment_decision": final_state["trader_investment_plan"],
            "risk_debate_state": {
                "risky_history": final_state["risk_debate_state"]["risky_history"],
                "safe_history": final_state["risk_debate_state"]["safe_history"],
                "neutral_history": final_state["risk_debate_state"]["neutral_history"],
                "history": final_state["risk_debate_state"]["history"],
                "judge_decision": final_state["risk_debate_state"]["judge_decision"],
            },
            "investment_plan": final_state["investment_plan"],
            "final_trade_decision": final_state["final_trade_decision"],
        }

        # Save to file
        directory = Path(f"eval_results/{self.ticker}/TradingAgentsStrategy_logs/")
        directory.mkdir(parents=True, exist_ok=True)

        with open(
            f"eval_results/{self.ticker}/TradingAgentsStrategy_logs/full_states_log.json",
            "w",
        ) as f:
            json.dump(self.log_states_dict, f, indent=4)

    def reflect_and_remember(self, returns_losses):
        """Reflect on decisions and update memory based on returns."""
        self.reflector.reflect_bull_researcher(
            self.curr_state, returns_losses, self.bull_memory
        )
        self.reflector.reflect_bear_researcher(
            self.curr_state, returns_losses, self.bear_memory
        )
        self.reflector.reflect_trader(
            self.curr_state, returns_losses, self.trader_memory
        )
        self.reflector.reflect_invest_judge(
            self.curr_state, returns_losses, self.invest_judge_memory
        )
        self.reflector.reflect_risk_manager(
            self.curr_state, returns_losses, self.risk_manager_memory
        )

    def process_signal(self, full_signal, stock_symbol=None):
        """Process a signal to extract the core decision."""
        return self.signal_processor.process_signal(full_signal, stock_symbol)
