"""
OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€çš„åŸºç¡€å®ç°
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_llm_logging

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('agents')
logger = setup_llm_logging()

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")


class OpenAICompatibleBase(ChatOpenAI):
    """
    OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
    ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€å®ç°
    """
    
    def __init__(
        self,
        provider_name: str,
        model: str,
        api_key_env_var: str,
        base_url: str,
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–OpenAIå…¼å®¹é€‚é…å™¨
        
        Args:
            provider_name: æä¾›å•†åç§° (å¦‚: "deepseek", "dashscope")
            model: æ¨¡å‹åç§°
            api_key_env_var: APIå¯†é’¥ç¯å¢ƒå˜é‡å
            base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # ğŸ” [DEBUG] è¯»å–ç¯å¢ƒå˜é‡å‰çš„æ—¥å¿—
        logger.info(f"ğŸ” [{provider_name}åˆå§‹åŒ–] å¼€å§‹åˆå§‹åŒ– OpenAI å…¼å®¹é€‚é…å™¨")
        logger.info(f"ğŸ” [{provider_name}åˆå§‹åŒ–] æ¨¡å‹: {model}")
        logger.info(f"ğŸ” [{provider_name}åˆå§‹åŒ–] API Key ç¯å¢ƒå˜é‡å: {api_key_env_var}")
        logger.info(f"ğŸ” [{provider_name}åˆå§‹åŒ–] æ˜¯å¦ä¼ å…¥ api_key å‚æ•°: {api_key is not None}")

        # åœ¨çˆ¶ç±»åˆå§‹åŒ–å‰å…ˆç¼“å­˜å…ƒä¿¡æ¯åˆ°ç§æœ‰å±æ€§ï¼ˆé¿å…Pydanticå­—æ®µé™åˆ¶ï¼‰
        object.__setattr__(self, "_provider_name", provider_name)
        object.__setattr__(self, "_model_name_alias", model)

        # è·å–APIå¯†é’¥
        if api_key is None:
            # å¯¼å…¥ API Key éªŒè¯å·¥å…·
            try:
                from app.utils.api_key_utils import is_valid_api_key
            except ImportError:
                def is_valid_api_key(key):
                    if not key or len(key) <= 10:
                        return False
                    if key.startswith('your_') or key.startswith('your-'):
                        return False
                    if key.endswith('_here') or key.endswith('-here'):
                        return False
                    if '...' in key:
                        return False
                    return True

            # ä»ç¯å¢ƒå˜é‡è¯»å– API Key
            env_api_key = os.getenv(api_key_env_var)
            logger.info(f"ğŸ” [{provider_name}åˆå§‹åŒ–] ä»ç¯å¢ƒå˜é‡è¯»å– {api_key_env_var}: {'æœ‰å€¼' if env_api_key else 'ç©º'}")

            # éªŒè¯ç¯å¢ƒå˜é‡ä¸­çš„ API Key æ˜¯å¦æœ‰æ•ˆï¼ˆæ’é™¤å ä½ç¬¦ï¼‰
            if env_api_key and is_valid_api_key(env_api_key):
                logger.info(f"âœ… [{provider_name}åˆå§‹åŒ–] ç¯å¢ƒå˜é‡ä¸­çš„ API Key æœ‰æ•ˆï¼Œé•¿åº¦: {len(env_api_key)}, å‰10ä½: {env_api_key[:10]}...")
                api_key = env_api_key
            elif env_api_key:
                logger.warning(f"âš ï¸ [{provider_name}åˆå§‹åŒ–] ç¯å¢ƒå˜é‡ä¸­çš„ API Key æ— æ•ˆï¼ˆå¯èƒ½æ˜¯å ä½ç¬¦ï¼‰ï¼Œå°†è¢«å¿½ç•¥")
                api_key = None
            else:
                logger.warning(f"âš ï¸ [{provider_name}åˆå§‹åŒ–] {api_key_env_var} ç¯å¢ƒå˜é‡ä¸ºç©º")
                api_key = None

            if not api_key:
                logger.error(f"âŒ [{provider_name}åˆå§‹åŒ–] API Key æ£€æŸ¥å¤±è´¥ï¼Œå³å°†æŠ›å‡ºå¼‚å¸¸")
                raise ValueError(
                    f"{provider_name} APIå¯†é’¥æœªæ‰¾åˆ°ã€‚"
                    f"è¯·åœ¨ Web ç•Œé¢é…ç½® API Key (è®¾ç½® -> å¤§æ¨¡å‹å‚å®¶) æˆ–è®¾ç½® {api_key_env_var} ç¯å¢ƒå˜é‡ã€‚"
                )
        else:
            logger.info(f"âœ… [{provider_name}åˆå§‹åŒ–] ä½¿ç”¨ä¼ å…¥çš„ API Keyï¼ˆæ¥è‡ªæ•°æ®åº“é…ç½®ï¼‰ï¼Œé•¿åº¦: {len(api_key)}")
        
        # è®¾ç½®OpenAIå…¼å®¹å‚æ•°
        # æ³¨æ„ï¼šmodelå‚æ•°ä¼šè¢«Pydanticæ˜ å°„åˆ°model_nameå­—æ®µ
        openai_kwargs = {
            "model": model,  # è¿™ä¼šè¢«æ˜ å°„åˆ°model_nameå­—æ®µ
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }
        
        # æ ¹æ®LangChainç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„å‚æ•°å
        try:
            # æ–°ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "api_key": api_key,
                "base_url": base_url
            })
        except:
            # æ—§ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "openai_api_key": api_key,
                "openai_api_base": base_url
            })
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(**openai_kwargs)

        # å†æ¬¡ç¡®ä¿å…ƒä¿¡æ¯å­˜åœ¨ï¼ˆæœ‰äº›å®ç°ä¼šåœ¨super()ä¸­é‡ç½®__dict__ï¼‰
        object.__setattr__(self, "_provider_name", provider_name)
        object.__setattr__(self, "_model_name_alias", model)

        logger.info(f"âœ… {provider_name} OpenAIå…¼å®¹é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   API Base: {base_url}")

    @property
    def provider_name(self) -> Optional[str]:
        return getattr(self, "_provider_name", None)

    # ç§»é™¤model_name propertyå®šä¹‰ï¼Œä½¿ç”¨Pydanticå­—æ®µ
    # model_nameå­—æ®µç”±ChatOpenAIåŸºç±»çš„Pydanticå­—æ®µæä¾›
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œå¹¶è®°å½•tokenä½¿ç”¨é‡
        """
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è°ƒç”¨çˆ¶ç±»ç”Ÿæˆæ–¹æ³•
        result = super()._generate(messages, stop, run_manager, **kwargs)
        
        # è®°å½•tokenä½¿ç”¨
        self._track_token_usage(result, kwargs, start_time)
        
        return result

    def _track_token_usage(self, result: ChatResult, kwargs: Dict, start_time: float):
        """è®°å½•tokenä½¿ç”¨é‡å¹¶è¾“å‡ºæ—¥å¿—"""
        if not TOKEN_TRACKING_ENABLED:
            return
        try:
            # ç»Ÿè®¡tokenä¿¡æ¯
            usage = getattr(result, "usage_metadata", None)
            total_tokens = usage.get("total_tokens") if usage else None
            prompt_tokens = usage.get("input_tokens") if usage else None
            completion_tokens = usage.get("output_tokens") if usage else None

            elapsed = time.time() - start_time
            logger.info(
                f"ğŸ“Š Tokenä½¿ç”¨ - Provider: {getattr(self, 'provider_name', 'unknown')}, Model: {getattr(self, 'model_name', 'unknown')}, "
                f"æ€»tokens: {total_tokens}, æç¤º: {prompt_tokens}, è¡¥å…¨: {completion_tokens}, ç”¨æ—¶: {elapsed:.2f}s"
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Tokenè·Ÿè¸ªè®°å½•å¤±è´¥: {e}")


class ChatDeepSeekOpenAI(OpenAICompatibleBase):
    """DeepSeek OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        super().__init__(
            provider_name="deepseek",
            model=model,
            api_key_env_var="DEEPSEEK_API_KEY",
            base_url="https://api.deepseek.com",
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


class ChatDashScopeOpenAIUnified(OpenAICompatibleBase):
    """é˜¿é‡Œç™¾ç‚¼ DashScope OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "qwen-turbo",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        super().__init__(
            provider_name="dashscope",
            model=model,
            api_key_env_var="DASHSCOPE_API_KEY",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


class ChatQianfanOpenAI(OpenAICompatibleBase):
    """æ–‡å¿ƒä¸€è¨€åƒå¸†å¹³å° OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "ernie-3.5-8k",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        # åƒå¸†æ–°ä¸€ä»£APIä½¿ç”¨å•ä¸€API Keyè®¤è¯
        # æ ¼å¼: bce-v3/ALTAK-xxx/xxx

        # å¦‚æœæ²¡æœ‰ä¼ å…¥ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        if not api_key:
            # å¯¼å…¥ API Key éªŒè¯å·¥å…·
            try:
                from app.utils.api_key_utils import is_valid_api_key
            except ImportError:
                def is_valid_api_key(key):
                    if not key or len(key) <= 10:
                        return False
                    if key.startswith('your_') or key.startswith('your-'):
                        return False
                    if key.endswith('_here') or key.endswith('-here'):
                        return False
                    if '...' in key:
                        return False
                    return True

            env_api_key = os.getenv('QIANFAN_API_KEY')
            if env_api_key and is_valid_api_key(env_api_key):
                qianfan_api_key = env_api_key
            else:
                qianfan_api_key = None
        else:
            qianfan_api_key = api_key

        if not qianfan_api_key:
            raise ValueError(
                "åƒå¸†æ¨¡å‹éœ€è¦é…ç½® API Keyã€‚"
                "è¯·åœ¨ Web ç•Œé¢é…ç½® (è®¾ç½® -> å¤§æ¨¡å‹å‚å®¶) æˆ–è®¾ç½® QIANFAN_API_KEY ç¯å¢ƒå˜é‡ï¼Œ"
                "æ ¼å¼ä¸º: bce-v3/ALTAK-xxx/xxx"
            )

        if not qianfan_api_key.startswith('bce-v3/'):
            raise ValueError(
                "QIANFAN_API_KEYæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º: bce-v3/ALTAK-xxx/xxx"
            )
        
        super().__init__(
            provider_name="qianfan",
            model=model,
            api_key_env_var="QIANFAN_API_KEY",
            base_url="https://qianfan.baidubce.com/v2",
            api_key=qianfan_api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆåƒå¸†æ¨¡å‹ä¸“ç”¨ï¼‰"""
        # åƒå¸†æ¨¡å‹çš„tokenä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        # ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        return max(1, len(text) // 2)
    
    def _truncate_messages(self, messages: List[BaseMessage], max_tokens: int = 4500) -> List[BaseMessage]:
        """æˆªæ–­æ¶ˆæ¯ä»¥é€‚åº”åƒå¸†æ¨¡å‹çš„tokené™åˆ¶"""
        # ä¸ºåƒå¸†æ¨¡å‹é¢„ç•™ä¸€äº›tokenç©ºé—´ï¼Œä½¿ç”¨4500è€Œä¸æ˜¯5120
        truncated_messages = []
        total_tokens = 0
        
        # ä»æœ€åä¸€æ¡æ¶ˆæ¯å¼€å§‹ï¼Œå‘å‰ä¿ç•™æ¶ˆæ¯
        for message in reversed(messages):
            content = str(message.content) if hasattr(message, 'content') else str(message)
            message_tokens = self._estimate_tokens(content)
            
            if total_tokens + message_tokens <= max_tokens:
                truncated_messages.insert(0, message)
                total_tokens += message_tokens
            else:
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸”è¶…é•¿ï¼Œè¿›è¡Œå†…å®¹æˆªæ–­
                if not truncated_messages:
                    remaining_tokens = max_tokens - 100  # é¢„ç•™100ä¸ªtoken
                    max_chars = remaining_tokens * 2  # 2å­—ç¬¦/token
                    truncated_content = content[:max_chars] + "...(å†…å®¹å·²æˆªæ–­)"
                    
                    # åˆ›å»ºæˆªæ–­åçš„æ¶ˆæ¯
                    if hasattr(message, 'content'):
                        message.content = truncated_content
                    truncated_messages.insert(0, message)
                break
        
        if len(truncated_messages) < len(messages):
            logger.warning(f"âš ï¸ åƒå¸†æ¨¡å‹è¾“å…¥è¿‡é•¿ï¼Œå·²æˆªæ–­ {len(messages) - len(truncated_messages)} æ¡æ¶ˆæ¯")
        
        return truncated_messages
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”ŸæˆèŠå¤©å“åº”ï¼ŒåŒ…å«åƒå¸†æ¨¡å‹çš„tokenæˆªæ–­é€»è¾‘"""
        
        # å¯¹åƒå¸†æ¨¡å‹è¿›è¡Œè¾“å…¥tokenæˆªæ–­
        truncated_messages = self._truncate_messages(messages)
        
        # è°ƒç”¨çˆ¶ç±»çš„_generateæ–¹æ³•
        return super()._generate(truncated_messages, stop, run_manager, **kwargs)


class ChatZhipuOpenAI(OpenAICompatibleBase):
    """æ™ºè°±AI GLM OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "glm-4.6",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        if base_url is None:
            env_base_url = os.getenv("ZHIPU_BASE_URL")
            # åªä½¿ç”¨æœ‰æ•ˆçš„ç¯å¢ƒå˜é‡å€¼ï¼ˆä¸æ˜¯å ä½ç¬¦ï¼‰
            if env_base_url and not env_base_url.startswith('your_') and not env_base_url.startswith('your-'):
                base_url = env_base_url
            else:
                base_url = "https://open.bigmodel.cn/api/paas/v4"
                
        super().__init__(
            provider_name="zhipu",
            model=model,
            api_key_env_var="ZHIPU_API_KEY",
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆGLMæ¨¡å‹ä¸“ç”¨ï¼‰"""
        # GLMæ¨¡å‹çš„tokenä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        # ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        return max(1, len(text) // 2)


class ChatCustomOpenAI(OpenAICompatibleBase):
    """è‡ªå®šä¹‰OpenAIç«¯ç‚¹é€‚é…å™¨ï¼ˆä»£ç†/èšåˆå¹³å°ï¼‰"""

    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ base_urlï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        if base_url is None:
            env_base_url = os.getenv("CUSTOM_OPENAI_BASE_URL")
            # åªä½¿ç”¨æœ‰æ•ˆçš„ç¯å¢ƒå˜é‡å€¼ï¼ˆä¸æ˜¯å ä½ç¬¦ï¼‰
            if env_base_url and not env_base_url.startswith('your_') and not env_base_url.startswith('your-'):
                base_url = env_base_url
            else:
                base_url = "https://api.openai.com/v1"

        super().__init__(
            provider_name="custom_openai",
            model=model,
            api_key_env_var="CUSTOM_OPENAI_API_KEY",
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


# æ”¯æŒçš„OpenAIå…¼å®¹æ¨¡å‹é…ç½®
OPENAI_COMPATIBLE_PROVIDERS = {
    "deepseek": {
        "adapter_class": ChatDeepSeekOpenAI,
        "base_url": "https://api.deepseek.com",
        "api_key_env": "DEEPSEEK_API_KEY",
        "models": {
            "deepseek-chat": {"context_length": 32768, "supports_function_calling": True},
            "deepseek-coder": {"context_length": 16384, "supports_function_calling": True}
        }
    },
    "dashscope": {
        "adapter_class": ChatDashScopeOpenAIUnified,
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "api_key_env": "DASHSCOPE_API_KEY",
        "models": {
            "qwen-turbo": {"context_length": 8192, "supports_function_calling": True},
            "qwen-plus": {"context_length": 32768, "supports_function_calling": True},
            "qwen-plus-latest": {"context_length": 32768, "supports_function_calling": True},
            "qwen-max": {"context_length": 32768, "supports_function_calling": True},
            "qwen-max-latest": {"context_length": 32768, "supports_function_calling": True}
        }
    },
    "qianfan": {
        "adapter_class": ChatQianfanOpenAI,
        "base_url": "https://qianfan.baidubce.com/v2",
        "api_key_env": "QIANFAN_API_KEY",
        "models": {
            "ernie-3.5-8k": {"context_length": 5120, "supports_function_calling": True},
            "ernie-4.0-turbo-8k": {"context_length": 5120, "supports_function_calling": True},
            "ERNIE-Speed-8K": {"context_length": 5120, "supports_function_calling": True},
            "ERNIE-Lite-8K": {"context_length": 5120, "supports_function_calling": True}
        }
    },
    "zhipu": {
        "adapter_class": ChatZhipuOpenAI,
        "base_url": "https://open.bigmodel.cn/api/paas/v4",
        "api_key_env": "ZHIPU_API_KEY",
        "models": {
            "glm-4.6": {"context_length": 200000, "supports_function_calling": True},
            "glm-4": {"context_length": 128000, "supports_function_calling": True},
            "glm-4-plus": {"context_length": 128000, "supports_function_calling": True},
            "glm-3-turbo": {"context_length": 128000, "supports_function_calling": True}
        }
    },
    "custom_openai": {
        "adapter_class": ChatCustomOpenAI,
        "base_url": None,  # å°†ç”±ç”¨æˆ·é…ç½®
        "api_key_env": "CUSTOM_OPENAI_API_KEY",
        "models": {
            "gpt-3.5-turbo": {"context_length": 16384, "supports_function_calling": True},
            "gpt-4": {"context_length": 8192, "supports_function_calling": True},
            "gpt-4-turbo": {"context_length": 128000, "supports_function_calling": True},
            "gpt-4o": {"context_length": 128000, "supports_function_calling": True},
            "gpt-4o-mini": {"context_length": 128000, "supports_function_calling": True},
            "claude-3-haiku": {"context_length": 200000, "supports_function_calling": True},
            "claude-3-sonnet": {"context_length": 200000, "supports_function_calling": True},
            "claude-3-opus": {"context_length": 200000, "supports_function_calling": True},
            "claude-3.5-sonnet": {"context_length": 200000, "supports_function_calling": True},
            "gemini-pro": {"context_length": 32768, "supports_function_calling": True},
            "gemini-1.5-pro": {"context_length": 1000000, "supports_function_calling": True},
            "llama-3.1-8b": {"context_length": 128000, "supports_function_calling": True},
            "llama-3.1-70b": {"context_length": 128000, "supports_function_calling": True},
            "llama-3.1-405b": {"context_length": 128000, "supports_function_calling": True},
            "custom-model": {"context_length": 32768, "supports_function_calling": True}
        }
    }
}


def create_openai_compatible_llm(
    provider: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    base_url: Optional[str] = None,
    **kwargs
) -> OpenAICompatibleBase:
    """åˆ›å»ºOpenAIå…¼å®¹LLMå®ä¾‹çš„ç»Ÿä¸€å·¥å‚å‡½æ•°"""
    provider_info = OPENAI_COMPATIBLE_PROVIDERS.get(provider)
    if not provider_info:
        raise ValueError(f"ä¸æ”¯æŒçš„OpenAIå…¼å®¹æä¾›å•†: {provider}")

    adapter_class = provider_info["adapter_class"]

    # å¦‚æœè°ƒç”¨æœªæä¾› base_urlï¼Œåˆ™é‡‡ç”¨ provider çš„é»˜è®¤å€¼ï¼ˆå¯èƒ½ä¸º Noneï¼‰
    if base_url is None:
        base_url = provider_info.get("base_url")

    # ä»…å½“ provider æœªå†…ç½® base_urlï¼ˆå¦‚ custom_openaiï¼‰æ—¶ï¼Œæ‰å°† base_url ä¼ é€’ç»™é€‚é…å™¨ï¼Œ
    # é¿å…ä¸é€‚é…å™¨å†…éƒ¨çš„ super().__init__(..., base_url=...) å†²çªå¯¼è‡´ "multiple values" é”™è¯¯ã€‚
    init_kwargs = dict(
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs,
    )
    if provider_info.get("base_url") is None and base_url:
        init_kwargs["base_url"] = base_url

    return adapter_class(**init_kwargs)


def test_openai_compatible_adapters():
    """å¿«é€Ÿæµ‹è¯•æ‰€æœ‰é€‚é…å™¨æ˜¯å¦èƒ½è¢«æ­£ç¡®å®ä¾‹åŒ–ï¼ˆä¸å‘èµ·çœŸå®è¯·æ±‚ï¼‰"""
    for provider, info in OPENAI_COMPATIBLE_PROVIDERS.items():
        cls = info["adapter_class"]
        try:
            if provider == "custom_openai":
                cls(model="gpt-3.5-turbo", api_key="test", base_url="https://api.openai.com/v1")
            elif provider == "qianfan":
                # åƒå¸†æ–°ä¸€ä»£APIä»…éœ€QIANFAN_API_KEYï¼Œæ ¼å¼: bce-v3/ALTAK-xxx/xxx
                cls(model="ernie-3.5-8k", api_key="bce-v3/test-key/test-secret")
            else:
                cls(model=list(info["models"].keys())[0], api_key="test")
            logger.info(f"âœ… é€‚é…å™¨å®ä¾‹åŒ–æˆåŠŸ: {provider}")
        except Exception as e:
            logger.warning(f"âš ï¸ é€‚é…å™¨å®ä¾‹åŒ–å¤±è´¥ï¼ˆé¢„æœŸæˆ–å¯å¿½ç•¥ï¼‰: {provider} - {e}")


if __name__ == "__main__":
    test_openai_compatible_adapters()
