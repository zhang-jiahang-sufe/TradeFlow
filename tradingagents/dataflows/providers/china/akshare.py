"""
AKShareç»Ÿä¸€æ•°æ®æä¾›å™¨
åŸºäºAKShare SDKçš„ç»Ÿä¸€æ•°æ®åŒæ­¥æ–¹æ¡ˆï¼Œæä¾›æ ‡å‡†åŒ–çš„æ•°æ®æ¥å£
"""
import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List, Optional, Union
import pandas as pd

from ..base_provider import BaseStockDataProvider

logger = logging.getLogger(__name__)


class AKShareProvider(BaseStockDataProvider):
    """
    AKShareç»Ÿä¸€æ•°æ®æä¾›å™¨
    
    æä¾›æ ‡å‡†åŒ–çš„è‚¡ç¥¨æ•°æ®æ¥å£ï¼Œæ”¯æŒï¼š
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è·å–
    - å†å²è¡Œæƒ…æ•°æ®
    - å®æ—¶è¡Œæƒ…æ•°æ®
    - è´¢åŠ¡æ•°æ®
    - æ¸¯è‚¡æ•°æ®æ”¯æŒ
    """
    
    def __init__(self):
        super().__init__("AKShare")
        self.ak = None
        self.connected = False
        self._stock_list_cache = None  # ç¼“å­˜è‚¡ç¥¨åˆ—è¡¨ï¼Œé¿å…é‡å¤è·å–
        self._cache_time = None  # ç¼“å­˜æ—¶é—´
        self._initialize_akshare()
    
    def _initialize_akshare(self):
        """åˆå§‹åŒ–AKShareè¿æ¥"""
        try:
            import akshare as ak
            import requests
            import time

            # å°è¯•å¯¼å…¥ curl_cffiï¼Œå¦‚æœå¯ç”¨åˆ™ä½¿ç”¨å®ƒæ¥ç»•è¿‡åçˆ¬è™«
            try:
                from curl_cffi import requests as curl_requests
                use_curl_cffi = True
                logger.info("ğŸ”§ æ£€æµ‹åˆ° curl_cffiï¼Œå°†ä½¿ç”¨å®ƒæ¥æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ TLS æŒ‡çº¹")
            except ImportError:
                use_curl_cffi = False
                logger.warning("âš ï¸ curl_cffi æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡† requestsï¼ˆå¯èƒ½è¢«åçˆ¬è™«æ‹¦æˆªï¼‰")
                logger.warning("   å»ºè®®å®‰è£…: pip install curl-cffi")

            # ä¿®å¤AKShareçš„bugï¼šè®¾ç½®requestsçš„é»˜è®¤headersï¼Œå¹¶æ·»åŠ è¯·æ±‚å»¶è¿Ÿ
            # AKShareçš„stock_news_em()å‡½æ•°æ²¡æœ‰è®¾ç½®å¿…è¦çš„headersï¼Œå¯¼è‡´APIè¿”å›ç©ºå“åº”
            if not hasattr(requests, '_akshare_headers_patched'):
                original_get = requests.get
                last_request_time = {'time': 0}  # ä½¿ç”¨å­—å…¸ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

                def patched_get(url, **kwargs):
                    """
                    åŒ…è£…requests.getæ–¹æ³•ï¼Œè‡ªåŠ¨æ·»åŠ å¿…è¦çš„headerså’Œè¯·æ±‚å»¶è¿Ÿ
                    ä¿®å¤AKShare stock_news_em()å‡½æ•°ç¼ºå°‘headersçš„é—®é¢˜
                    å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨ curl_cffi æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ TLS æŒ‡çº¹
                    """
                    # æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬è™«å°ç¦
                    # åªå¯¹ä¸œæ–¹è´¢å¯Œç½‘çš„è¯·æ±‚æ·»åŠ å»¶è¿Ÿ
                    if 'eastmoney.com' in url:
                        current_time = time.time()
                        time_since_last_request = current_time - last_request_time['time']
                        if time_since_last_request < 0.5:  # è‡³å°‘é—´éš”0.5ç§’
                            time.sleep(0.5 - time_since_last_request)
                        last_request_time['time'] = time.time()

                    # å¦‚æœæ˜¯ä¸œæ–¹è´¢å¯Œç½‘çš„è¯·æ±‚ï¼Œä¸” curl_cffi å¯ç”¨ï¼Œä½¿ç”¨å®ƒæ¥ç»•è¿‡åçˆ¬è™«
                    if use_curl_cffi and 'eastmoney.com' in url:
                        try:
                            # ä½¿ç”¨ curl_cffi æ¨¡æ‹Ÿ Chrome 120 çš„ TLS æŒ‡çº¹
                            # æ³¨æ„ï¼šä½¿ç”¨ impersonate æ—¶ï¼Œä¸è¦ä¼ é€’è‡ªå®šä¹‰ headersï¼Œè®© curl_cffi è‡ªåŠ¨è®¾ç½®
                            curl_kwargs = {
                                'timeout': kwargs.get('timeout', 10),
                                'impersonate': "chrome120"  # æ¨¡æ‹Ÿ Chrome 120
                            }

                            # åªä¼ é€’é headers çš„å‚æ•°
                            if 'params' in kwargs:
                                curl_kwargs['params'] = kwargs['params']
                            # ä¸ä¼ é€’ headersï¼Œè®© impersonate è‡ªåŠ¨è®¾ç½®
                            if 'data' in kwargs:
                                curl_kwargs['data'] = kwargs['data']
                            if 'json' in kwargs:
                                curl_kwargs['json'] = kwargs['json']

                            response = curl_requests.get(url, **curl_kwargs)
                            # curl_cffi çš„å“åº”å¯¹è±¡å·²ç»å…¼å®¹ requests.Response
                            return response
                        except Exception as e:
                            # curl_cffi å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡† requests
                            error_msg = str(e)
                            # å¿½ç•¥ TLS åº“é”™è¯¯å’Œ 400 é”™è¯¯çš„è¯¦ç»†æ—¥å¿—ï¼ˆè¿™æ˜¯ Docker ç¯å¢ƒçš„å·²çŸ¥é—®é¢˜ï¼‰
                            if 'invalid library' not in error_msg and '400' not in error_msg:
                                logger.warning(f"âš ï¸ curl_cffi è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡† requests: {e}")

                    # æ ‡å‡† requests è¯·æ±‚ï¼ˆéä¸œæ–¹è´¢å¯Œç½‘ï¼Œæˆ– curl_cffi ä¸å¯ç”¨/å¤±è´¥ï¼‰
                    # è®¾ç½®æµè§ˆå™¨è¯·æ±‚å¤´
                    if 'headers' not in kwargs or kwargs['headers'] is None:
                        kwargs['headers'] = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'Referer': 'https://www.eastmoney.com/',
                            'Connection': 'keep-alive',
                        }
                    elif isinstance(kwargs['headers'], dict):
                        # å¦‚æœå·²æœ‰headersï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„å­—æ®µ
                        if 'User-Agent' not in kwargs['headers']:
                            kwargs['headers']['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                        if 'Referer' not in kwargs['headers']:
                            kwargs['headers']['Referer'] = 'https://www.eastmoney.com/'
                        if 'Accept' not in kwargs['headers']:
                            kwargs['headers']['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                        if 'Accept-Language' not in kwargs['headers']:
                            kwargs['headers']['Accept-Language'] = 'zh-CN,zh;q=0.9,en;q=0.8'

                    # æ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            return original_get(url, **kwargs)
                        except Exception as e:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯SSLé”™è¯¯
                            error_str = str(e)
                            is_ssl_error = ('SSL' in error_str or 'ssl' in error_str or
                                          'UNEXPECTED_EOF_WHILE_READING' in error_str)

                            if is_ssl_error and attempt < max_retries - 1:
                                # SSLé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                                wait_time = 0.5 * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                                time.sleep(wait_time)
                                continue
                            else:
                                # éSSLé”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç›´æ¥æŠ›å‡º
                                raise

                # åº”ç”¨patch
                requests.get = patched_get
                requests._akshare_headers_patched = True

                if use_curl_cffi:
                    logger.info("ğŸ”§ å·²ä¿®å¤AKShareçš„headersé—®é¢˜ï¼Œä½¿ç”¨ curl_cffi æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼ˆChrome 120ï¼‰")
                else:
                    logger.info("ğŸ”§ å·²ä¿®å¤AKShareçš„headersé—®é¢˜ï¼Œå¹¶æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼ˆ0.5ç§’ï¼‰")

            self.ak = ak
            self.connected = True

            # é…ç½®è¶…æ—¶å’Œé‡è¯•
            self._configure_timeout()

            logger.info("âœ… AKShareè¿æ¥æˆåŠŸ")
        except ImportError as e:
            logger.error(f"âŒ AKShareæœªå®‰è£…: {e}")
            self.connected = False
        except Exception as e:
            logger.error(f"âŒ AKShareåˆå§‹åŒ–å¤±è´¥: {e}")
            self.connected = False

    def _get_stock_news_direct(self, symbol: str, limit: int = 10) -> Optional[pd.DataFrame]:
        """
        ç›´æ¥è°ƒç”¨ä¸œæ–¹è´¢å¯Œç½‘æ–°é—» APIï¼ˆç»•è¿‡ AKShareï¼‰
        ä½¿ç”¨ curl_cffi æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé€‚ç”¨äº Docker ç¯å¢ƒ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æ–°é—» DataFrame æˆ– None
        """
        try:
            from curl_cffi import requests as curl_requests
            import json
            import time
            import os

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
            symbol_6 = symbol.zfill(6)

            # æ„å»ºè¯·æ±‚å‚æ•°
            url = "https://search-api-web.eastmoney.com/search/jsonp"
            param = {
                "uid": "",
                "keyword": symbol_6,
                "type": ["cmsArticleWebOld"],
                "client": "web",
                "clientType": "web",
                "clientVersion": "curr",
                "param": {
                    "cmsArticleWebOld": {
                        "searchScope": "default",
                        "sort": "default",
                        "pageIndex": 1,
                        "pageSize": limit,
                        "preTag": "<em>",
                        "postTag": "</em>"
                    }
                }
            }

            params = {
                "cb": f"jQuery{int(time.time() * 1000)}",
                "param": json.dumps(param),
                "_": str(int(time.time() * 1000))
            }

            # ä½¿ç”¨ curl_cffi å‘é€è¯·æ±‚
            response = curl_requests.get(
                url,
                params=params,
                timeout=10,
                impersonate="chrome120"
            )

            if response.status_code != 200:
                self.logger.error(f"âŒ {symbol} ä¸œæ–¹è´¢å¯Œç½‘ API è¿”å›é”™è¯¯: {response.status_code}")
                return None

            # è§£æ JSONP å“åº”
            text = response.text
            if text.startswith("jQuery"):
                text = text[text.find("(")+1:text.rfind(")")]

            data = json.loads(text)

            # æ£€æŸ¥è¿”å›æ•°æ®
            if "result" not in data or "cmsArticleWebOld" not in data["result"]:
                self.logger.error(f"âŒ {symbol} ä¸œæ–¹è´¢å¯Œç½‘ API è¿”å›æ•°æ®ç»“æ„å¼‚å¸¸")
                return None

            articles = data["result"]["cmsArticleWebOld"]

            if not articles:
                self.logger.warning(f"âš ï¸ {symbol} æœªè·å–åˆ°æ–°é—»")
                return None

            # è½¬æ¢ä¸º DataFrameï¼ˆä¸ AKShare æ ¼å¼å…¼å®¹ï¼‰
            news_data = []
            for article in articles:
                news_data.append({
                    "æ–°é—»æ ‡é¢˜": article.get("title", ""),
                    "æ–°é—»å†…å®¹": article.get("content", ""),
                    "å‘å¸ƒæ—¶é—´": article.get("date", ""),
                    "æ–°é—»é“¾æ¥": article.get("url", ""),
                    "å…³é”®è¯": article.get("keywords", ""),
                    "æ–°é—»æ¥æº": article.get("source", "ä¸œæ–¹è´¢å¯Œç½‘"),
                    "æ–°é—»ç±»å‹": article.get("type", "")
                })

            df = pd.DataFrame(news_data)
            self.logger.info(f"âœ… {symbol} ç›´æ¥è°ƒç”¨ API è·å–æ–°é—»æˆåŠŸ: {len(df)} æ¡")
            return df

        except Exception as e:
            self.logger.error(f"âŒ {symbol} ç›´æ¥è°ƒç”¨ API å¤±è´¥: {e}")
            return None

    def _configure_timeout(self):
        """é…ç½®AKShareçš„è¶…æ—¶è®¾ç½®"""
        try:
            import socket
            socket.setdefaulttimeout(60)  # 60ç§’è¶…æ—¶
            logger.info("ğŸ”§ AKShareè¶…æ—¶é…ç½®å®Œæˆ: 60ç§’")
        except Exception as e:
            logger.warning(f"âš ï¸ AKShareè¶…æ—¶é…ç½®å¤±è´¥: {e}")
    
    async def connect(self) -> bool:
        """è¿æ¥åˆ°AKShareæ•°æ®æº"""
        return await self.test_connection()

    async def test_connection(self) -> bool:
        """æµ‹è¯•AKShareè¿æ¥"""
        if not self.connected:
            return False

        # AKShare æ˜¯åŸºäºç½‘ç»œçˆ¬è™«çš„åº“ï¼Œä¸éœ€è¦ä¼ ç»Ÿçš„"è¿æ¥"æµ‹è¯•
        # åªè¦åº“å·²ç»å¯¼å…¥æˆåŠŸï¼Œå°±è®¤ä¸ºå¯ç”¨
        # å®é™…çš„ç½‘ç»œè¯·æ±‚ä¼šåœ¨å…·ä½“è°ƒç”¨æ—¶è¿›è¡Œï¼Œå¹¶æœ‰å„è‡ªçš„é”™è¯¯å¤„ç†
        logger.info("âœ… AKShareè¿æ¥æµ‹è¯•æˆåŠŸï¼ˆåº“å·²åŠ è½½ï¼‰")
        return True
    
    def get_stock_list_sync(self) -> Optional[pd.DataFrame]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if not self.connected:
            return None

        try:
            logger.info("ğŸ“‹ è·å–AKShareè‚¡ç¥¨åˆ—è¡¨ï¼ˆåŒæ­¥ï¼‰...")
            stock_df = self.ak.stock_info_a_code_name()

            if stock_df is None or stock_df.empty:
                logger.warning("âš ï¸ AKShareè‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
                return None

            logger.info(f"âœ… AKShareè‚¡ç¥¨åˆ—è¡¨è·å–æˆåŠŸ: {len(stock_df)}åªè‚¡ç¥¨")
            return stock_df

        except Exception as e:
            logger.error(f"âŒ AKShareè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    async def get_stock_list(self) -> List[Dict[str, Any]]:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨

        Returns:
            è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒ…å«ä»£ç å’Œåç§°
        """
        if not self.connected:
            return []

        try:
            logger.info("ğŸ“‹ è·å–AKShareè‚¡ç¥¨åˆ—è¡¨...")

            # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
            def fetch_stock_list():
                return self.ak.stock_info_a_code_name()

            stock_df = await asyncio.to_thread(fetch_stock_list)

            if stock_df is None or stock_df.empty:
                logger.warning("âš ï¸ AKShareè‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
                return []

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            stock_list = []
            for _, row in stock_df.iterrows():
                stock_list.append({
                    "code": str(row.get("code", "")),
                    "name": str(row.get("name", "")),
                    "source": "akshare"
                })

            logger.info(f"âœ… AKShareè‚¡ç¥¨åˆ—è¡¨è·å–æˆåŠŸ: {len(stock_list)}åªè‚¡ç¥¨")
            return stock_list

        except Exception as e:
            logger.error(f"âŒ AKShareè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def get_stock_basic_info(self, code: str) -> Optional[Dict[str, Any]]:
        """
        è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            æ ‡å‡†åŒ–çš„è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        """
        if not self.connected:
            return None
        
        try:
            logger.debug(f"ğŸ“Š è·å–{code}åŸºç¡€ä¿¡æ¯...")
            
            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_info = await self._get_stock_info_detail(code)
            
            if not stock_info:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{code}çš„åŸºç¡€ä¿¡æ¯")
                return None
            
            # è½¬æ¢ä¸ºæ ‡å‡†åŒ–å­—å…¸
            basic_info = {
                "code": code,
                "name": stock_info.get("name", f"è‚¡ç¥¨{code}"),
                "area": stock_info.get("area", "æœªçŸ¥"),
                "industry": stock_info.get("industry", "æœªçŸ¥"),
                "market": self._determine_market(code),
                "list_date": stock_info.get("list_date", ""),
                # æ‰©å±•å­—æ®µ
                "full_symbol": self._get_full_symbol(code),
                "market_info": self._get_market_info(code),
                "data_source": "akshare",
                "last_sync": datetime.now(timezone.utc),
                "sync_status": "success"
            }
            
            logger.debug(f"âœ… {code}åŸºç¡€ä¿¡æ¯è·å–æˆåŠŸ")
            return basic_info
            
        except Exception as e:
            logger.error(f"âŒ è·å–{code}åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def _get_stock_list_cached(self):
        """è·å–ç¼“å­˜çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆé¿å…é‡å¤è·å–ï¼‰"""
        from datetime import datetime, timedelta

        # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœªè¿‡æœŸï¼ˆ1å°æ—¶ï¼‰ï¼Œç›´æ¥è¿”å›
        if self._stock_list_cache is not None and self._cache_time is not None:
            if datetime.now() - self._cache_time < timedelta(hours=1):
                return self._stock_list_cache

        # å¦åˆ™é‡æ–°è·å–
        def fetch_stock_list():
            return self.ak.stock_info_a_code_name()

        try:
            stock_list = await asyncio.to_thread(fetch_stock_list)
            if stock_list is not None and not stock_list.empty:
                self._stock_list_cache = stock_list
                self._cache_time = datetime.now()
                logger.info(f"âœ… è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜æ›´æ–°: {len(stock_list)} åªè‚¡ç¥¨")
                return stock_list
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

        return None

    async def _get_stock_info_detail(self, code: str) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯"""
        try:
            # æ–¹æ³•1: å°è¯•è·å–ä¸ªè‚¡è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«è¡Œä¸šã€åœ°åŒºç­‰è¯¦ç»†ä¿¡æ¯ï¼‰
            def fetch_individual_info():
                return self.ak.stock_individual_info_em(symbol=code)

            try:
                stock_info = await asyncio.to_thread(fetch_individual_info)

                if stock_info is not None and not stock_info.empty:
                    # è§£æä¿¡æ¯
                    info = {"code": code}

                    # æå–è‚¡ç¥¨åç§°
                    name_row = stock_info[stock_info['item'] == 'è‚¡ç¥¨ç®€ç§°']
                    if not name_row.empty:
                        info['name'] = str(name_row['value'].iloc[0])

                    # æå–è¡Œä¸šä¿¡æ¯
                    industry_row = stock_info[stock_info['item'] == 'æ‰€å±è¡Œä¸š']
                    if not industry_row.empty:
                        info['industry'] = str(industry_row['value'].iloc[0])

                    # æå–åœ°åŒºä¿¡æ¯
                    area_row = stock_info[stock_info['item'] == 'æ‰€å±åœ°åŒº']
                    if not area_row.empty:
                        info['area'] = str(area_row['value'].iloc[0])

                    # æå–ä¸Šå¸‚æ—¥æœŸ
                    list_date_row = stock_info[stock_info['item'] == 'ä¸Šå¸‚æ—¶é—´']
                    if not list_date_row.empty:
                        info['list_date'] = str(list_date_row['value'].iloc[0])

                    return info
            except Exception as e:
                logger.debug(f"è·å–{code}ä¸ªè‚¡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")

            # æ–¹æ³•2: ä»ç¼“å­˜çš„è‚¡ç¥¨åˆ—è¡¨ä¸­è·å–åŸºæœ¬ä¿¡æ¯ï¼ˆåªæœ‰ä»£ç å’Œåç§°ï¼‰
            try:
                stock_list = await self._get_stock_list_cached()
                if stock_list is not None and not stock_list.empty:
                    stock_row = stock_list[stock_list['code'] == code]
                    if not stock_row.empty:
                        return {
                            "code": code,
                            "name": str(stock_row['name'].iloc[0]),
                            "industry": "æœªçŸ¥",
                            "area": "æœªçŸ¥"
                        }
            except Exception as e:
                logger.debug(f"ä»è‚¡ç¥¨åˆ—è¡¨è·å–{code}ä¿¡æ¯å¤±è´¥: {e}")

            # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {"code": code, "name": f"è‚¡ç¥¨{code}", "industry": "æœªçŸ¥", "area": "æœªçŸ¥"}

        except Exception as e:
            logger.debug(f"è·å–{code}è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return {"code": code, "name": f"è‚¡ç¥¨{code}", "industry": "æœªçŸ¥", "area": "æœªçŸ¥"}
    
    def _determine_market(self, code: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç åˆ¤æ–­å¸‚åœº"""
        if code.startswith(('60', '68')):
            return "ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€"
        elif code.startswith(('00', '30')):
            return "æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€"
        elif code.startswith('8'):
            return "åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€"
        else:
            return "æœªçŸ¥å¸‚åœº"
    
    def _get_full_symbol(self, code: str) -> str:
        """
        è·å–å®Œæ•´è‚¡ç¥¨ä»£ç 

        Args:
            code: 6ä½è‚¡ç¥¨ä»£ç 

        Returns:
            å®Œæ•´æ ‡å‡†åŒ–ä»£ç ï¼Œå¦‚æœæ— æ³•è¯†åˆ«åˆ™è¿”å›åŸå§‹ä»£ç ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
        """
        # ç¡®ä¿ code ä¸ä¸ºç©º
        if not code:
            return ""

        # æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²
        code = str(code).strip()

        # æ ¹æ®ä»£ç å‰ç¼€åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith(('60', '68', '90')):  # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€ï¼ˆå¢åŠ 90å¼€å¤´çš„Bè‚¡ï¼‰
            return f"{code}.SS"
        elif code.startswith(('00', '30', '20')):  # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€ï¼ˆå¢åŠ 20å¼€å¤´çš„Bè‚¡ï¼‰
            return f"{code}.SZ"
        elif code.startswith(('8', '4')):  # åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€ï¼ˆå¢åŠ 4å¼€å¤´çš„æ–°ä¸‰æ¿ï¼‰
            return f"{code}.BJ"
        else:
            # æ— æ³•è¯†åˆ«çš„ä»£ç ï¼Œè¿”å›åŸå§‹ä»£ç ï¼ˆç¡®ä¿ä¸ä¸ºç©ºï¼‰
            return code if code else ""
    
    def _get_market_info(self, code: str) -> Dict[str, Any]:
        """è·å–å¸‚åœºä¿¡æ¯"""
        if code.startswith(('60', '68')):
            return {
                "market_type": "CN",
                "exchange": "SSE",
                "exchange_name": "ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€",
                "currency": "CNY",
                "timezone": "Asia/Shanghai"
            }
        elif code.startswith(('00', '30')):
            return {
                "market_type": "CN",
                "exchange": "SZSE", 
                "exchange_name": "æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€",
                "currency": "CNY",
                "timezone": "Asia/Shanghai"
            }
        elif code.startswith('8'):
            return {
                "market_type": "CN",
                "exchange": "BSE",
                "exchange_name": "åŒ—äº¬è¯åˆ¸äº¤æ˜“æ‰€", 
                "currency": "CNY",
                "timezone": "Asia/Shanghai"
            }
        else:
            return {
                "market_type": "CN",
                "exchange": "UNKNOWN",
                "exchange_name": "æœªçŸ¥äº¤æ˜“æ‰€",
                "currency": "CNY",
                "timezone": "Asia/Shanghai"
            }
    
    async def get_batch_stock_quotes(self, codes: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸€æ¬¡è·å–å…¨å¸‚åœºå¿«ç…§ï¼‰

        ä¼˜å…ˆä½¿ç”¨æ–°æµªè´¢ç»æ¥å£ï¼ˆæ›´ç¨³å®šï¼‰ï¼Œå¤±è´¥æ—¶å›é€€åˆ°ä¸œæ–¹è´¢å¯Œæ¥å£

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            è‚¡ç¥¨ä»£ç åˆ°è¡Œæƒ…æ•°æ®çš„æ˜ å°„å­—å…¸
        """
        if not self.connected:
            return {}

        # é‡è¯•é€»è¾‘
        max_retries = 2
        retry_delay = 1  # ç§’

        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ“Š æ‰¹é‡è·å– {len(codes)} åªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…... (å°è¯• {attempt + 1}/{max_retries})")

                # ä¼˜å…ˆä½¿ç”¨æ–°æµªè´¢ç»æ¥å£ï¼ˆæ›´ç¨³å®šï¼Œä¸å®¹æ˜“è¢«å°ï¼‰
                def fetch_spot_data_sina():
                    import time
                    time.sleep(0.3)  # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                    return self.ak.stock_zh_a_spot()

                try:
                    spot_df = await asyncio.to_thread(fetch_spot_data_sina)
                    data_source = "sina"
                    logger.debug("âœ… ä½¿ç”¨æ–°æµªè´¢ç»æ¥å£è·å–æ•°æ®")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–°æµªè´¢ç»æ¥å£å¤±è´¥: {e}ï¼Œå°è¯•ä¸œæ–¹è´¢å¯Œæ¥å£...")
                    # å›é€€åˆ°ä¸œæ–¹è´¢å¯Œæ¥å£
                    def fetch_spot_data_em():
                        import time
                        time.sleep(0.5)
                        return self.ak.stock_zh_a_spot_em()
                    spot_df = await asyncio.to_thread(fetch_spot_data_em)
                    data_source = "eastmoney"
                    logger.debug("âœ… ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ¥å£è·å–æ•°æ®")

                if spot_df is None or spot_df.empty:
                    logger.warning("âš ï¸ å…¨å¸‚åœºå¿«ç…§ä¸ºç©º")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    return {}

                # æ„å»ºä»£ç åˆ°è¡Œæƒ…çš„æ˜ å°„
                quotes_map = {}
                codes_set = set(codes)

                # æ„å»ºä»£ç æ˜ å°„è¡¨ï¼ˆæ”¯æŒå¸¦å‰ç¼€çš„ä»£ç åŒ¹é…ï¼‰
                # ä¾‹å¦‚ï¼šsh600000 -> 600000, sz000001 -> 000001
                code_mapping = {}
                for code in codes:
                    code_mapping[code] = code  # åŸå§‹ä»£ç 
                    # æ·»åŠ å¯èƒ½çš„å‰ç¼€å˜ä½“
                    for prefix in ['sh', 'sz', 'bj']:
                        code_mapping[f"{prefix}{code}"] = code

                for _, row in spot_df.iterrows():
                    raw_code = str(row.get("ä»£ç ", ""))

                    # å°è¯•åŒ¹é…ä»£ç ï¼ˆæ”¯æŒå¸¦å‰ç¼€å’Œä¸å¸¦å‰ç¼€ï¼‰
                    matched_code = None
                    if raw_code in code_mapping:
                        matched_code = code_mapping[raw_code]
                    elif raw_code in codes_set:
                        matched_code = raw_code

                    if matched_code:
                        quotes_data = {
                            "name": str(row.get("åç§°", f"è‚¡ç¥¨{matched_code}")),
                            "price": self._safe_float(row.get("æœ€æ–°ä»·", 0)),
                            "change": self._safe_float(row.get("æ¶¨è·Œé¢", 0)),
                            "change_percent": self._safe_float(row.get("æ¶¨è·Œå¹…", 0)),
                            "volume": self._safe_int(row.get("æˆäº¤é‡", 0)),
                            "amount": self._safe_float(row.get("æˆäº¤é¢", 0)),
                            "open": self._safe_float(row.get("ä»Šå¼€", 0)),
                            "high": self._safe_float(row.get("æœ€é«˜", 0)),
                            "low": self._safe_float(row.get("æœ€ä½", 0)),
                            "pre_close": self._safe_float(row.get("æ˜¨æ”¶", 0)),
                            # ğŸ”¥ æ–°å¢ï¼šè´¢åŠ¡æŒ‡æ ‡å­—æ®µ
                            "turnover_rate": self._safe_float(row.get("æ¢æ‰‹ç‡", None)),  # æ¢æ‰‹ç‡ï¼ˆ%ï¼‰
                            "volume_ratio": self._safe_float(row.get("é‡æ¯”", None)),  # é‡æ¯”
                            "pe": self._safe_float(row.get("å¸‚ç›ˆç‡-åŠ¨æ€", None)),  # åŠ¨æ€å¸‚ç›ˆç‡
                            "pb": self._safe_float(row.get("å¸‚å‡€ç‡", None)),  # å¸‚å‡€ç‡
                            "total_mv": self._safe_float(row.get("æ€»å¸‚å€¼", None)),  # æ€»å¸‚å€¼ï¼ˆå…ƒï¼‰
                            "circ_mv": self._safe_float(row.get("æµé€šå¸‚å€¼", None)),  # æµé€šå¸‚å€¼ï¼ˆå…ƒï¼‰
                        }

                        # è½¬æ¢ä¸ºæ ‡å‡†åŒ–å­—å…¸ï¼ˆä½¿ç”¨åŒ¹é…åçš„ä»£ç ï¼‰
                        quotes_map[matched_code] = {
                            "code": matched_code,
                            "symbol": matched_code,
                            "name": quotes_data.get("name", f"è‚¡ç¥¨{matched_code}"),
                            "price": float(quotes_data.get("price", 0)),
                            "change": float(quotes_data.get("change", 0)),
                            "change_percent": float(quotes_data.get("change_percent", 0)),
                            "volume": int(quotes_data.get("volume", 0)),
                            "amount": float(quotes_data.get("amount", 0)),
                            "open_price": float(quotes_data.get("open", 0)),
                            "high_price": float(quotes_data.get("high", 0)),
                            "low_price": float(quotes_data.get("low", 0)),
                            "pre_close": float(quotes_data.get("pre_close", 0)),
                            # ğŸ”¥ æ–°å¢ï¼šè´¢åŠ¡æŒ‡æ ‡å­—æ®µ
                            "turnover_rate": quotes_data.get("turnover_rate"),  # æ¢æ‰‹ç‡ï¼ˆ%ï¼‰
                            "volume_ratio": quotes_data.get("volume_ratio"),  # é‡æ¯”
                            "pe": quotes_data.get("pe"),  # åŠ¨æ€å¸‚ç›ˆç‡
                            "pe_ttm": quotes_data.get("pe"),  # TTMå¸‚ç›ˆç‡ï¼ˆä¸åŠ¨æ€å¸‚ç›ˆç‡ç›¸åŒï¼‰
                            "pb": quotes_data.get("pb"),  # å¸‚å‡€ç‡
                            "total_mv": quotes_data.get("total_mv") / 1e8 if quotes_data.get("total_mv") else None,  # æ€»å¸‚å€¼ï¼ˆè½¬æ¢ä¸ºäº¿å…ƒï¼‰
                            "circ_mv": quotes_data.get("circ_mv") / 1e8 if quotes_data.get("circ_mv") else None,  # æµé€šå¸‚å€¼ï¼ˆè½¬æ¢ä¸ºäº¿å…ƒï¼‰
                            # æ‰©å±•å­—æ®µ
                            "full_symbol": self._get_full_symbol(matched_code),
                            "market_info": self._get_market_info(matched_code),
                            "data_source": "akshare",
                            "last_sync": datetime.now(timezone.utc),
                            "sync_status": "success"
                        }

                found_count = len(quotes_map)
                missing_count = len(codes) - found_count
                logger.debug(f"âœ… æ‰¹é‡è·å–å®Œæˆ: æ‰¾åˆ° {found_count} åª, æœªæ‰¾åˆ° {missing_count} åª")

                # è®°å½•æœªæ‰¾åˆ°çš„è‚¡ç¥¨
                if missing_count > 0:
                    missing_codes = codes_set - set(quotes_map.keys())
                    if missing_count <= 10:
                        logger.debug(f"âš ï¸ æœªæ‰¾åˆ°è¡Œæƒ…çš„è‚¡ç¥¨: {list(missing_codes)}")
                    else:
                        logger.debug(f"âš ï¸ æœªæ‰¾åˆ°è¡Œæƒ…çš„è‚¡ç¥¨: {list(missing_codes)[:10]}... (å…±{missing_count}åª)")

                return quotes_map

            except Exception as e:
                logger.warning(f"âš ï¸ æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                else:
                    logger.error(f"âŒ æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    return {}

    async def get_stock_quotes(self, code: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å•ä¸ªè‚¡ç¥¨å®æ—¶è¡Œæƒ…

        ğŸ”¥ ç­–ç•¥ï¼šä½¿ç”¨ stock_bid_ask_em æ¥å£è·å–å•ä¸ªè‚¡ç¥¨çš„å®æ—¶è¡Œæƒ…æŠ¥ä»·
        - ä¼˜ç‚¹ï¼šåªè·å–å•ä¸ªè‚¡ç¥¨æ•°æ®ï¼Œé€Ÿåº¦å¿«ï¼Œä¸æµªè´¹èµ„æº
        - é€‚ç”¨åœºæ™¯ï¼šæ‰‹åŠ¨åŒæ­¥å•ä¸ªè‚¡ç¥¨

        Args:
            code: è‚¡ç¥¨ä»£ç 

        Returns:
            æ ‡å‡†åŒ–çš„è¡Œæƒ…æ•°æ®
        """
        if not self.connected:
            return None

        try:
            logger.info(f"ğŸ“ˆ ä½¿ç”¨ stock_bid_ask_em æ¥å£è·å– {code} å®æ—¶è¡Œæƒ…...")

            # ğŸ”¥ ä½¿ç”¨ stock_bid_ask_em æ¥å£è·å–å•ä¸ªè‚¡ç¥¨å®æ—¶è¡Œæƒ…
            def fetch_bid_ask():
                return self.ak.stock_bid_ask_em(symbol=code)

            bid_ask_df = await asyncio.to_thread(fetch_bid_ask)

            # ğŸ”¥ æ‰“å°åŸå§‹è¿”å›æ•°æ®
            logger.info(f"ğŸ“Š stock_bid_ask_em è¿”å›æ•°æ®ç±»å‹: {type(bid_ask_df)}")
            if bid_ask_df is not None:
                logger.info(f"ğŸ“Š DataFrame shape: {bid_ask_df.shape}")
                logger.info(f"ğŸ“Š DataFrame columns: {list(bid_ask_df.columns)}")
                logger.info(f"ğŸ“Š DataFrame å®Œæ•´æ•°æ®:\n{bid_ask_df.to_string()}")

            if bid_ask_df is None or bid_ask_df.empty:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{code}çš„è¡Œæƒ…æ•°æ®")
                return None

            # å°† DataFrame è½¬æ¢ä¸ºå­—å…¸
            data_dict = dict(zip(bid_ask_df['item'], bid_ask_df['value']))
            logger.info(f"ğŸ“Š è½¬æ¢åçš„å­—å…¸: {data_dict}")

            # è½¬æ¢ä¸ºæ ‡å‡†åŒ–å­—å…¸
            # ğŸ”¥ æ³¨æ„ï¼šå­—æ®µåå¿…é¡»ä¸ app/routers/stocks.py ä¸­çš„æŸ¥è¯¢å­—æ®µä¸€è‡´
            # å‰ç«¯æŸ¥è¯¢ä½¿ç”¨çš„æ˜¯ high/low/openï¼Œä¸æ˜¯ high_price/low_price/open_price

            # ğŸ”¥ è·å–å½“å‰æ—¥æœŸï¼ˆUTC+8ï¼‰
            from datetime import datetime, timezone, timedelta
            cn_tz = timezone(timedelta(hours=8))
            now_cn = datetime.now(cn_tz)
            trade_date = now_cn.strftime("%Y-%m-%d")  # æ ¼å¼ï¼š2025-11-05

            # ğŸ”¥ æˆäº¤é‡å•ä½è½¬æ¢ï¼šæ‰‹ â†’ è‚¡ï¼ˆ1æ‰‹ = 100è‚¡ï¼‰
            volume_in_lots = int(data_dict.get("æ€»æ‰‹", 0))  # å•ä½ï¼šæ‰‹
            volume_in_shares = volume_in_lots * 100  # å•ä½ï¼šè‚¡

            quotes = {
                "code": code,
                "symbol": code,
                "name": f"è‚¡ç¥¨{code}",  # stock_bid_ask_em ä¸è¿”å›è‚¡ç¥¨åç§°
                "price": float(data_dict.get("æœ€æ–°", 0)),
                "close": float(data_dict.get("æœ€æ–°", 0)),  # ğŸ”¥ close å­—æ®µï¼ˆä¸ price ç›¸åŒï¼‰
                "current_price": float(data_dict.get("æœ€æ–°", 0)),  # ğŸ”¥ current_price å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                "change": float(data_dict.get("æ¶¨è·Œ", 0)),
                "change_percent": float(data_dict.get("æ¶¨å¹…", 0)),
                "pct_chg": float(data_dict.get("æ¶¨å¹…", 0)),  # ğŸ”¥ pct_chg å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                "volume": volume_in_shares,  # ğŸ”¥ å•ä½ï¼šè‚¡ï¼ˆå·²è½¬æ¢ï¼‰
                "amount": float(data_dict.get("é‡‘é¢", 0)),  # å•ä½ï¼šå…ƒ
                "open": float(data_dict.get("ä»Šå¼€", 0)),  # ğŸ”¥ ä½¿ç”¨ open è€Œä¸æ˜¯ open_price
                "high": float(data_dict.get("æœ€é«˜", 0)),  # ğŸ”¥ ä½¿ç”¨ high è€Œä¸æ˜¯ high_price
                "low": float(data_dict.get("æœ€ä½", 0)),  # ğŸ”¥ ä½¿ç”¨ low è€Œä¸æ˜¯ low_price
                "pre_close": float(data_dict.get("æ˜¨æ”¶", 0)),
                # ğŸ”¥ æ–°å¢ï¼šè´¢åŠ¡æŒ‡æ ‡å­—æ®µ
                "turnover_rate": float(data_dict.get("æ¢æ‰‹", 0)),  # æ¢æ‰‹ç‡ï¼ˆ%ï¼‰
                "volume_ratio": float(data_dict.get("é‡æ¯”", 0)),  # é‡æ¯”
                "pe": None,  # stock_bid_ask_em ä¸è¿”å›å¸‚ç›ˆç‡
                "pe_ttm": None,
                "pb": None,  # stock_bid_ask_em ä¸è¿”å›å¸‚å‡€ç‡
                "total_mv": None,  # stock_bid_ask_em ä¸è¿”å›æ€»å¸‚å€¼
                "circ_mv": None,  # stock_bid_ask_em ä¸è¿”å›æµé€šå¸‚å€¼
                # ğŸ”¥ æ–°å¢ï¼šäº¤æ˜“æ—¥æœŸå’Œæ›´æ–°æ—¶é—´
                "trade_date": trade_date,  # äº¤æ˜“æ—¥æœŸï¼ˆæ ¼å¼ï¼š2025-11-05ï¼‰
                "updated_at": now_cn.isoformat(),  # æ›´æ–°æ—¶é—´ï¼ˆISOæ ¼å¼ï¼Œå¸¦æ—¶åŒºï¼‰
                # æ‰©å±•å­—æ®µ
                "full_symbol": self._get_full_symbol(code),
                "market_info": self._get_market_info(code),
                "data_source": "akshare",
                "last_sync": datetime.now(timezone.utc),
                "sync_status": "success"
            }

            logger.info(f"âœ… {code} å®æ—¶è¡Œæƒ…è·å–æˆåŠŸ: æœ€æ–°ä»·={quotes['price']}, æ¶¨è·Œå¹…={quotes['change_percent']}%, æˆäº¤é‡={quotes['volume']}, æˆäº¤é¢={quotes['amount']}")
            return quotes

        except Exception as e:
            logger.error(f"âŒ è·å–{code}å®æ—¶è¡Œæƒ…å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def _get_realtime_quotes_data(self, code: str) -> Dict[str, Any]:
        """è·å–å®æ—¶è¡Œæƒ…æ•°æ®"""
        try:
            # æ–¹æ³•1: è·å–Aè‚¡å®æ—¶è¡Œæƒ…
            def fetch_spot_data():
                return self.ak.stock_zh_a_spot_em()

            try:
                spot_df = await asyncio.to_thread(fetch_spot_data)

                if spot_df is not None and not spot_df.empty:
                    # æŸ¥æ‰¾å¯¹åº”è‚¡ç¥¨
                    stock_data = spot_df[spot_df['ä»£ç '] == code]

                    if not stock_data.empty:
                        row = stock_data.iloc[0]

                        # è§£æè¡Œæƒ…æ•°æ®
                        return {
                            "name": str(row.get("åç§°", f"è‚¡ç¥¨{code}")),
                            "price": self._safe_float(row.get("æœ€æ–°ä»·", 0)),
                            "change": self._safe_float(row.get("æ¶¨è·Œé¢", 0)),
                            "change_percent": self._safe_float(row.get("æ¶¨è·Œå¹…", 0)),
                            "volume": self._safe_int(row.get("æˆäº¤é‡", 0)),
                            "amount": self._safe_float(row.get("æˆäº¤é¢", 0)),
                            "open": self._safe_float(row.get("ä»Šå¼€", 0)),
                            "high": self._safe_float(row.get("æœ€é«˜", 0)),
                            "low": self._safe_float(row.get("æœ€ä½", 0)),
                            "pre_close": self._safe_float(row.get("æ˜¨æ”¶", 0)),
                            # ğŸ”¥ æ–°å¢ï¼šè´¢åŠ¡æŒ‡æ ‡å­—æ®µ
                            "turnover_rate": self._safe_float(row.get("æ¢æ‰‹ç‡", None)),  # æ¢æ‰‹ç‡ï¼ˆ%ï¼‰
                            "volume_ratio": self._safe_float(row.get("é‡æ¯”", None)),  # é‡æ¯”
                            "pe": self._safe_float(row.get("å¸‚ç›ˆç‡-åŠ¨æ€", None)),  # åŠ¨æ€å¸‚ç›ˆç‡
                            "pb": self._safe_float(row.get("å¸‚å‡€ç‡", None)),  # å¸‚å‡€ç‡
                            "total_mv": self._safe_float(row.get("æ€»å¸‚å€¼", None)),  # æ€»å¸‚å€¼ï¼ˆå…ƒï¼‰
                            "circ_mv": self._safe_float(row.get("æµé€šå¸‚å€¼", None)),  # æµé€šå¸‚å€¼ï¼ˆå…ƒï¼‰
                        }
            except Exception as e:
                logger.debug(f"è·å–{code}Aè‚¡å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")

            # æ–¹æ³•2: å°è¯•è·å–å•åªè‚¡ç¥¨å®æ—¶æ•°æ®
            def fetch_individual_spot():
                return self.ak.stock_zh_a_hist(symbol=code, period="daily", adjust="")

            try:
                hist_df = await asyncio.to_thread(fetch_individual_spot)
                if hist_df is not None and not hist_df.empty:
                    # å–æœ€æ–°ä¸€å¤©çš„æ•°æ®ä½œä¸ºå½“å‰è¡Œæƒ…
                    latest_row = hist_df.iloc[-1]
                    return {
                        "name": f"è‚¡ç¥¨{code}",
                        "price": self._safe_float(latest_row.get("æ”¶ç›˜", 0)),
                        "change": 0,  # å†å²æ•°æ®æ— æ³•è®¡ç®—æ¶¨è·Œé¢
                        "change_percent": self._safe_float(latest_row.get("æ¶¨è·Œå¹…", 0)),
                        "volume": self._safe_int(latest_row.get("æˆäº¤é‡", 0)),
                        "amount": self._safe_float(latest_row.get("æˆäº¤é¢", 0)),
                        "open": self._safe_float(latest_row.get("å¼€ç›˜", 0)),
                        "high": self._safe_float(latest_row.get("æœ€é«˜", 0)),
                        "low": self._safe_float(latest_row.get("æœ€ä½", 0)),
                        "pre_close": self._safe_float(latest_row.get("æ”¶ç›˜", 0))
                    }
            except Exception as e:
                logger.debug(f"è·å–{code}å†å²æ•°æ®ä½œä¸ºè¡Œæƒ…å¤±è´¥: {e}")

            return {}

        except Exception as e:
            logger.debug(f"è·å–{code}å®æ—¶è¡Œæƒ…æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _safe_float(self, value: Any) -> float:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        try:
            if pd.isna(value) or value is None:
                return 0.0
            return float(value)
        except (ValueError, TypeError):
            return 0.0
    
    def _safe_int(self, value: Any) -> int:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        try:
            if pd.isna(value) or value is None:
                return 0
            return int(float(value))
        except (ValueError, TypeError):
            return 0
    
    def _safe_str(self, value: Any) -> str:
        """å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
        try:
            if pd.isna(value) or value is None:
                return ""
            return str(value)
        except:
            return ""

    async def get_historical_data(
        self,
        code: str,
        start_date: str,
        end_date: str,
        period: str = "daily"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å†å²è¡Œæƒ…æ•°æ®

        Args:
            code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            period: å‘¨æœŸ (daily, weekly, monthly)

        Returns:
            å†å²è¡Œæƒ…æ•°æ®DataFrame
        """
        if not self.connected:
            return None

        try:
            logger.debug(f"ğŸ“Š è·å–{code}å†å²æ•°æ®: {start_date} åˆ° {end_date}")

            # è½¬æ¢å‘¨æœŸæ ¼å¼
            period_map = {
                "daily": "daily",
                "weekly": "weekly",
                "monthly": "monthly"
            }
            ak_period = period_map.get(period, "daily")

            # æ ¼å¼åŒ–æ—¥æœŸ
            start_date_formatted = start_date.replace('-', '')
            end_date_formatted = end_date.replace('-', '')

            # è·å–å†å²æ•°æ®
            def fetch_historical_data():
                return self.ak.stock_zh_a_hist(
                    symbol=code,
                    period=ak_period,
                    start_date=start_date_formatted,
                    end_date=end_date_formatted,
                    adjust="qfq"  # å‰å¤æƒ
                )

            hist_df = await asyncio.to_thread(fetch_historical_data)

            if hist_df is None or hist_df.empty:
                logger.warning(f"âš ï¸ {code}å†å²æ•°æ®ä¸ºç©º")
                return None

            # æ ‡å‡†åŒ–åˆ—å
            hist_df = self._standardize_historical_columns(hist_df, code)

            logger.debug(f"âœ… {code}å†å²æ•°æ®è·å–æˆåŠŸ: {len(hist_df)}æ¡è®°å½•")
            return hist_df

        except Exception as e:
            logger.error(f"âŒ è·å–{code}å†å²æ•°æ®å¤±è´¥: {e}")
            return None

    def _standardize_historical_columns(self, df: pd.DataFrame, code: str) -> pd.DataFrame:
        """æ ‡å‡†åŒ–å†å²æ•°æ®åˆ—å"""
        try:
            # æ ‡å‡†åŒ–åˆ—åæ˜ å°„
            column_mapping = {
                'æ—¥æœŸ': 'date',
                'å¼€ç›˜': 'open',
                'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount',
                'æŒ¯å¹…': 'amplitude',
                'æ¶¨è·Œå¹…': 'change_percent',
                'æ¶¨è·Œé¢': 'change',
                'æ¢æ‰‹ç‡': 'turnover'
            }

            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)

            # æ·»åŠ æ ‡å‡†å­—æ®µ
            df['code'] = code
            df['full_symbol'] = self._get_full_symbol(code)

            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])

            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_columns = ['open', 'close', 'high', 'low', 'volume', 'amount']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

            return df

        except Exception as e:
            logger.error(f"æ ‡å‡†åŒ–{code}å†å²æ•°æ®åˆ—åå¤±è´¥: {e}")
            return df

    async def get_financial_data(self, code: str) -> Dict[str, Any]:
        """
        è·å–è´¢åŠ¡æ•°æ®

        Args:
            code: è‚¡ç¥¨ä»£ç 

        Returns:
            è´¢åŠ¡æ•°æ®å­—å…¸
        """
        if not self.connected:
            return {}

        try:
            logger.debug(f"ğŸ’° è·å–{code}è´¢åŠ¡æ•°æ®...")

            financial_data = {}

            # 1. è·å–ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
            try:
                def fetch_financial_abstract():
                    return self.ak.stock_financial_abstract(symbol=code)

                main_indicators = await asyncio.to_thread(fetch_financial_abstract)
                if main_indicators is not None and not main_indicators.empty:
                    financial_data['main_indicators'] = main_indicators.to_dict('records')
                    logger.debug(f"âœ… {code}ä¸»è¦è´¢åŠ¡æŒ‡æ ‡è·å–æˆåŠŸ")
            except Exception as e:
                logger.debug(f"è·å–{code}ä¸»è¦è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")

            # 2. è·å–èµ„äº§è´Ÿå€ºè¡¨
            try:
                def fetch_balance_sheet():
                    return self.ak.stock_balance_sheet_by_report_em(symbol=code)

                balance_sheet = await asyncio.to_thread(fetch_balance_sheet)
                if balance_sheet is not None and not balance_sheet.empty:
                    financial_data['balance_sheet'] = balance_sheet.to_dict('records')
                    logger.debug(f"âœ… {code}èµ„äº§è´Ÿå€ºè¡¨è·å–æˆåŠŸ")
            except Exception as e:
                logger.debug(f"è·å–{code}èµ„äº§è´Ÿå€ºè¡¨å¤±è´¥: {e}")

            # 3. è·å–åˆ©æ¶¦è¡¨
            try:
                def fetch_income_statement():
                    return self.ak.stock_profit_sheet_by_report_em(symbol=code)

                income_statement = await asyncio.to_thread(fetch_income_statement)
                if income_statement is not None and not income_statement.empty:
                    financial_data['income_statement'] = income_statement.to_dict('records')
                    logger.debug(f"âœ… {code}åˆ©æ¶¦è¡¨è·å–æˆåŠŸ")
            except Exception as e:
                logger.debug(f"è·å–{code}åˆ©æ¶¦è¡¨å¤±è´¥: {e}")

            # 4. è·å–ç°é‡‘æµé‡è¡¨
            try:
                def fetch_cash_flow():
                    return self.ak.stock_cash_flow_sheet_by_report_em(symbol=code)

                cash_flow = await asyncio.to_thread(fetch_cash_flow)
                if cash_flow is not None and not cash_flow.empty:
                    financial_data['cash_flow'] = cash_flow.to_dict('records')
                    logger.debug(f"âœ… {code}ç°é‡‘æµé‡è¡¨è·å–æˆåŠŸ")
            except Exception as e:
                logger.debug(f"è·å–{code}ç°é‡‘æµé‡è¡¨å¤±è´¥: {e}")

            if financial_data:
                logger.debug(f"âœ… {code}è´¢åŠ¡æ•°æ®è·å–å®Œæˆ: {len(financial_data)}ä¸ªæ•°æ®é›†")
            else:
                logger.warning(f"âš ï¸ {code}æœªè·å–åˆ°ä»»ä½•è´¢åŠ¡æ•°æ®")

            return financial_data

        except Exception as e:
            logger.error(f"âŒ è·å–{code}è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            return {}

    async def get_market_status(self) -> Dict[str, Any]:
        """
        è·å–å¸‚åœºçŠ¶æ€ä¿¡æ¯

        Returns:
            å¸‚åœºçŠ¶æ€ä¿¡æ¯
        """
        try:
            # AKShareæ²¡æœ‰ç›´æ¥çš„å¸‚åœºçŠ¶æ€APIï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            now = datetime.now()

            # ç®€å•çš„äº¤æ˜“æ—¶é—´åˆ¤æ–­
            is_trading_time = (
                now.weekday() < 5 and  # å·¥ä½œæ—¥
                ((9 <= now.hour < 12) or (13 <= now.hour < 15))  # äº¤æ˜“æ—¶é—´
            )

            return {
                "market_status": "open" if is_trading_time else "closed",
                "current_time": now.isoformat(),
                "data_source": "akshare",
                "trading_day": now.weekday() < 5
            }

        except Exception as e:
            logger.error(f"âŒ è·å–å¸‚åœºçŠ¶æ€å¤±è´¥: {e}")
            return {
                "market_status": "unknown",
                "current_time": datetime.now().isoformat(),
                "data_source": "akshare",
                "error": str(e)
            }

    def get_stock_news_sync(self, symbol: str = None, limit: int = 10) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨æ–°é—»ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œè¿”å›åŸå§‹ DataFrameï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œä¸ºNoneæ—¶è·å–å¸‚åœºæ–°é—»
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æ–°é—» DataFrame æˆ– None
        """
        if not self.is_available():
            return None

        try:
            import akshare as ak
            import json
            import time

            if symbol:
                # è·å–ä¸ªè‚¡æ–°é—»
                self.logger.debug(f"ğŸ“° è·å–AKShareä¸ªè‚¡æ–°é—»: {symbol}")

                # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
                symbol_6 = symbol.zfill(6)

                # è·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
                max_retries = 3
                retry_delay = 1  # ç§’
                news_df = None

                for attempt in range(max_retries):
                    try:
                        news_df = ak.stock_news_em(symbol=symbol_6)
                        break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                    except json.JSONDecodeError as e:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"âš ï¸ {symbol} ç¬¬{attempt+1}æ¬¡è·å–æ–°é—»å¤±è´¥(JSONè§£æé”™è¯¯)ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿
                        else:
                            self.logger.error(f"âŒ {symbol} è·å–æ–°é—»å¤±è´¥(JSONè§£æé”™è¯¯): {e}")
                            return None
                    except Exception as e:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"âš ï¸ {symbol} ç¬¬{attempt+1}æ¬¡è·å–æ–°é—»å¤±è´¥: {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            retry_delay *= 2
                        else:
                            raise

                if news_df is not None and not news_df.empty:
                    self.logger.info(f"âœ… {symbol} AKShareæ–°é—»è·å–æˆåŠŸ: {len(news_df)} æ¡")
                    return news_df.head(limit) if limit else news_df
                else:
                    self.logger.warning(f"âš ï¸ {symbol} æœªè·å–åˆ°AKShareæ–°é—»æ•°æ®")
                    return None
            else:
                # è·å–å¸‚åœºæ–°é—»
                self.logger.debug("ğŸ“° è·å–AKShareå¸‚åœºæ–°é—»")
                news_df = ak.news_cctv()

                if news_df is not None and not news_df.empty:
                    self.logger.info(f"âœ… AKShareå¸‚åœºæ–°é—»è·å–æˆåŠŸ: {len(news_df)} æ¡")
                    return news_df.head(limit) if limit else news_df
                else:
                    self.logger.warning("âš ï¸ æœªè·å–åˆ°AKShareå¸‚åœºæ–°é—»æ•°æ®")
                    return None

        except Exception as e:
            self.logger.error(f"âŒ AKShareæ–°é—»è·å–å¤±è´¥: {e}")
            return None

    async def get_stock_news(self, symbol: str = None, limit: int = 10) -> Optional[List[Dict[str, Any]]]:
        """
        è·å–è‚¡ç¥¨æ–°é—»ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œè¿”å›ç»“æ„åŒ–åˆ—è¡¨ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œä¸ºNoneæ—¶è·å–å¸‚åœºæ–°é—»
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if not self.is_available():
            return None

        try:
            import akshare as ak
            import json
            import os

            if symbol:
                # è·å–ä¸ªè‚¡æ–°é—»
                self.logger.debug(f"ğŸ“° è·å–AKShareä¸ªè‚¡æ–°é—»: {symbol}")

                # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
                symbol_6 = symbol.zfill(6)

                # æ£€æµ‹æ˜¯å¦åœ¨ Docker ç¯å¢ƒä¸­
                is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER') == 'true'

                # è·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
                max_retries = 3
                retry_delay = 1  # ç§’
                news_df = None

                # å¦‚æœåœ¨ Docker ç¯å¢ƒä¸­ï¼Œå°è¯•ä½¿ç”¨ curl_cffi ç›´æ¥è°ƒç”¨ API
                if is_docker:
                    try:
                        from curl_cffi import requests as curl_requests
                        self.logger.debug(f"ğŸ³ æ£€æµ‹åˆ° Docker ç¯å¢ƒï¼Œä½¿ç”¨ curl_cffi ç›´æ¥è°ƒç”¨ API")
                        news_df = await asyncio.to_thread(
                            self._get_stock_news_direct,
                            symbol=symbol_6,
                            limit=limit
                        )
                        if news_df is not None and not news_df.empty:
                            self.logger.info(f"âœ… {symbol} Docker ç¯å¢ƒç›´æ¥è°ƒç”¨ API æˆåŠŸ")
                        else:
                            self.logger.warning(f"âš ï¸ {symbol} Docker ç¯å¢ƒç›´æ¥è°ƒç”¨ API å¤±è´¥ï¼Œå›é€€åˆ° AKShare")
                            news_df = None  # å›é€€åˆ° AKShare
                    except ImportError:
                        self.logger.warning(f"âš ï¸ curl_cffi æœªå®‰è£…ï¼Œå›é€€åˆ° AKShare")
                        news_df = None
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {symbol} Docker ç¯å¢ƒç›´æ¥è°ƒç”¨ API å¼‚å¸¸: {e}ï¼Œå›é€€åˆ° AKShare")
                        news_df = None

                # å¦‚æœç›´æ¥è°ƒç”¨å¤±è´¥æˆ–ä¸åœ¨ Docker ç¯å¢ƒï¼Œä½¿ç”¨ AKShare
                if news_df is None:
                    for attempt in range(max_retries):
                        try:
                            news_df = await asyncio.to_thread(
                                ak.stock_news_em,
                                symbol=symbol_6
                            )
                            break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                        except json.JSONDecodeError as e:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"âš ï¸ {symbol} ç¬¬{attempt+1}æ¬¡è·å–æ–°é—»å¤±è´¥(JSONè§£æé”™è¯¯)ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                                await asyncio.sleep(retry_delay)
                                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                            else:
                                self.logger.error(f"âŒ {symbol} è·å–æ–°é—»å¤±è´¥(JSONè§£æé”™è¯¯): {e}")
                                return []
                        except KeyError as e:
                            # ä¸œæ–¹è´¢å¯Œç½‘æ¥å£å˜æ›´æˆ–åçˆ¬è™«æ‹¦æˆªï¼Œè¿”å›çš„å­—æ®µç»“æ„æ”¹å˜
                            if str(e) == "'cmsArticleWebOld'":
                                self.logger.error(f"âŒ {symbol} AKShareæ–°é—»æ¥å£è¿”å›æ•°æ®ç»“æ„å¼‚å¸¸: ç¼ºå°‘ 'cmsArticleWebOld' å­—æ®µ")
                                self.logger.error(f"   è¿™é€šå¸¸æ˜¯å› ä¸ºï¼š1) åçˆ¬è™«æ‹¦æˆª 2) æ¥å£å˜æ›´ 3) ç½‘ç»œé—®é¢˜")
                                self.logger.error(f"   å»ºè®®ï¼šæ£€æŸ¥ AKShare ç‰ˆæœ¬æ˜¯å¦ä¸ºæœ€æ–° (å½“å‰è¦æ±‚ >=1.17.86)")
                                # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…ç¨‹åºå´©æºƒ
                                return []
                            else:
                                if attempt < max_retries - 1:
                                    self.logger.warning(f"âš ï¸ {symbol} ç¬¬{attempt+1}æ¬¡è·å–æ–°é—»å¤±è´¥(å­—æ®µé”™è¯¯): {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                                    await asyncio.sleep(retry_delay)
                                    retry_delay *= 2
                                else:
                                    self.logger.error(f"âŒ {symbol} è·å–æ–°é—»å¤±è´¥(å­—æ®µé”™è¯¯): {e}")
                                    return []
                        except Exception as e:
                            if attempt < max_retries - 1:
                                self.logger.warning(f"âš ï¸ {symbol} ç¬¬{attempt+1}æ¬¡è·å–æ–°é—»å¤±è´¥: {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                                await asyncio.sleep(retry_delay)
                                retry_delay *= 2
                            else:
                                raise

                if news_df is not None and not news_df.empty:
                    news_list = []

                    for _, row in news_df.head(limit).iterrows():
                        title = str(row.get('æ–°é—»æ ‡é¢˜', '') or row.get('æ ‡é¢˜', ''))
                        content = str(row.get('æ–°é—»å†…å®¹', '') or row.get('å†…å®¹', ''))
                        summary = str(row.get('æ–°é—»æ‘˜è¦', '') or row.get('æ‘˜è¦', ''))

                        news_item = {
                            "symbol": symbol,
                            "title": title,
                            "content": content,
                            "summary": summary,
                            "url": str(row.get('æ–°é—»é“¾æ¥', '') or row.get('é“¾æ¥', '')),
                            "source": str(row.get('æ–‡ç« æ¥æº', '') or row.get('æ¥æº', '') or 'ä¸œæ–¹è´¢å¯Œ'),
                            "author": str(row.get('ä½œè€…', '') or ''),
                            "publish_time": self._parse_news_time(row.get('å‘å¸ƒæ—¶é—´', '') or row.get('æ—¶é—´', '')),
                            "category": self._classify_news(content, title),
                            "sentiment": self._analyze_news_sentiment(content, title),
                            "sentiment_score": self._calculate_sentiment_score(content, title),
                            "keywords": self._extract_keywords(content, title),
                            "importance": self._assess_news_importance(content, title),
                            "data_source": "akshare"
                        }

                        # è¿‡æ»¤ç©ºæ ‡é¢˜çš„æ–°é—»
                        if news_item["title"]:
                            news_list.append(news_item)

                    self.logger.info(f"âœ… {symbol} AKShareæ–°é—»è·å–æˆåŠŸ: {len(news_list)} æ¡")
                    return news_list
                else:
                    self.logger.warning(f"âš ï¸ {symbol} æœªè·å–åˆ°AKShareæ–°é—»æ•°æ®")
                    return []
            else:
                # è·å–å¸‚åœºæ–°é—»
                self.logger.debug("ğŸ“° è·å–AKShareå¸‚åœºæ–°é—»")

                try:
                    # è·å–è´¢ç»æ–°é—»
                    news_df = await asyncio.to_thread(
                        ak.news_cctv,
                        limit=limit
                    )

                    if news_df is not None and not news_df.empty:
                        news_list = []

                        for _, row in news_df.iterrows():
                            title = str(row.get('title', '') or row.get('æ ‡é¢˜', ''))
                            content = str(row.get('content', '') or row.get('å†…å®¹', ''))
                            summary = str(row.get('brief', '') or row.get('æ‘˜è¦', ''))

                            news_item = {
                                "title": title,
                                "content": content,
                                "summary": summary,
                                "url": str(row.get('url', '') or row.get('é“¾æ¥', '')),
                                "source": str(row.get('source', '') or row.get('æ¥æº', '') or 'CCTVè´¢ç»'),
                                "author": str(row.get('author', '') or ''),
                                "publish_time": self._parse_news_time(row.get('time', '') or row.get('æ—¶é—´', '')),
                                "category": self._classify_news(content, title),
                                "sentiment": self._analyze_news_sentiment(content, title),
                                "sentiment_score": self._calculate_sentiment_score(content, title),
                                "keywords": self._extract_keywords(content, title),
                                "importance": self._assess_news_importance(content, title),
                                "data_source": "akshare"
                            }

                            if news_item["title"]:
                                news_list.append(news_item)

                        self.logger.info(f"âœ… AKShareå¸‚åœºæ–°é—»è·å–æˆåŠŸ: {len(news_list)} æ¡")
                        return news_list

                except Exception as e:
                    self.logger.debug(f"CCTVæ–°é—»è·å–å¤±è´¥: {e}")

                return []

        except Exception as e:
            self.logger.error(f"âŒ è·å–AKShareæ–°é—»å¤±è´¥ symbol={symbol}: {e}")
            return None

    def _parse_news_time(self, time_str: str) -> Optional[datetime]:
        """è§£ææ–°é—»æ—¶é—´"""
        if not time_str:
            return datetime.utcnow()

        try:
            # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M",
                "%Y-%m-%d",
                "%Y/%m/%d %H:%M:%S",
                "%Y/%m/%d %H:%M",
                "%Y/%m/%d",
                "%m-%d %H:%M",
                "%m/%d %H:%M"
            ]

            for fmt in formats:
                try:
                    parsed_time = datetime.strptime(str(time_str), fmt)

                    # å¦‚æœåªæœ‰æœˆæ—¥ï¼Œè¡¥å……å¹´ä»½
                    if fmt in ["%m-%d %H:%M", "%m/%d %H:%M"]:
                        current_year = datetime.now().year
                        parsed_time = parsed_time.replace(year=current_year)

                    return parsed_time
                except ValueError:
                    continue

            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›å½“å‰æ—¶é—´
            self.logger.debug(f"âš ï¸ æ— æ³•è§£ææ–°é—»æ—¶é—´: {time_str}")
            return datetime.utcnow()

        except Exception as e:
            self.logger.debug(f"è§£ææ–°é—»æ—¶é—´å¼‚å¸¸: {e}")
            return datetime.utcnow()

    def _analyze_news_sentiment(self, content: str, title: str) -> str:
        """
        åˆ†ææ–°é—»æƒ…ç»ª

        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜

        Returns:
            æƒ…ç»ªç±»å‹: positive/negative/neutral
        """
        text = f"{title} {content}".lower()

        # ç§¯æå…³é”®è¯
        positive_keywords = [
            'åˆ©å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'ç›ˆåˆ©', 'çªç ´', 'åˆ›æ–°é«˜', 'ä¹°å…¥', 'æ¨è',
            'çœ‹å¥½', 'ä¹è§‚', 'å¼ºåŠ¿', 'å¤§æ¶¨', 'é£™å‡', 'æš´æ¶¨', 'æ¶¨åœ', 'æ¶¨å¹…',
            'ä¸šç»©å¢é•¿', 'è¥æ”¶å¢é•¿', 'å‡€åˆ©æ¶¦å¢é•¿', 'æ‰­äºä¸ºç›ˆ', 'è¶…é¢„æœŸ',
            'è·æ‰¹', 'ä¸­æ ‡', 'ç­¾çº¦', 'åˆä½œ', 'å¹¶è´­', 'é‡ç»„', 'åˆ†çº¢', 'å›è´­'
        ]

        # æ¶ˆæå…³é”®è¯
        negative_keywords = [
            'åˆ©ç©º', 'ä¸‹è·Œ', 'äºæŸ', 'é£é™©', 'æš´è·Œ', 'å–å‡º', 'è­¦å‘Š', 'ä¸‹è°ƒ',
            'çœ‹ç©º', 'æ‚²è§‚', 'å¼±åŠ¿', 'å¤§è·Œ', 'è·³æ°´', 'æš´è·Œ', 'è·Œåœ', 'è·Œå¹…',
            'ä¸šç»©ä¸‹æ»‘', 'è¥æ”¶ä¸‹é™', 'å‡€åˆ©æ¶¦ä¸‹é™', 'äºæŸ', 'ä½äºé¢„æœŸ',
            'è¢«æŸ¥', 'è¿è§„', 'å¤„ç½š', 'è¯‰è®¼', 'é€€å¸‚', 'åœç‰Œ', 'å•†èª‰å‡å€¼'
        ]

        positive_count = sum(1 for keyword in positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text)

        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'

    def _calculate_sentiment_score(self, content: str, title: str) -> float:
        """
        è®¡ç®—æƒ…ç»ªåˆ†æ•°

        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜

        Returns:
            æƒ…ç»ªåˆ†æ•°: -1.0 åˆ° 1.0
        """
        text = f"{title} {content}".lower()

        # ç§¯æå…³é”®è¯æƒé‡
        positive_keywords = {
            'æ¶¨åœ': 1.0, 'æš´æ¶¨': 0.9, 'å¤§æ¶¨': 0.8, 'é£™å‡': 0.8,
            'åˆ›æ–°é«˜': 0.7, 'çªç ´': 0.6, 'ä¸Šæ¶¨': 0.5, 'å¢é•¿': 0.4,
            'åˆ©å¥½': 0.6, 'çœ‹å¥½': 0.5, 'æ¨è': 0.5, 'ä¹°å…¥': 0.6
        }

        # æ¶ˆæå…³é”®è¯æƒé‡
        negative_keywords = {
            'è·Œåœ': -1.0, 'æš´è·Œ': -0.9, 'å¤§è·Œ': -0.8, 'è·³æ°´': -0.8,
            'åˆ›æ–°ä½': -0.7, 'ç ´ä½': -0.6, 'ä¸‹è·Œ': -0.5, 'ä¸‹æ»‘': -0.4,
            'åˆ©ç©º': -0.6, 'çœ‹ç©º': -0.5, 'å–å‡º': -0.6, 'è­¦å‘Š': -0.5
        }

        score = 0.0

        # è®¡ç®—ç§¯æåˆ†æ•°
        for keyword, weight in positive_keywords.items():
            if keyword in text:
                score += weight

        # è®¡ç®—æ¶ˆæåˆ†æ•°
        for keyword, weight in negative_keywords.items():
            if keyword in text:
                score += weight

        # å½’ä¸€åŒ–åˆ° [-1.0, 1.0]
        return max(-1.0, min(1.0, score / 3.0))

    def _extract_keywords(self, content: str, title: str) -> List[str]:
        """
        æå–å…³é”®è¯

        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        text = f"{title} {content}"

        # å¸¸è§è´¢ç»å…³é”®è¯
        common_keywords = [
            'è‚¡ç¥¨', 'å…¬å¸', 'å¸‚åœº', 'æŠ•èµ„', 'ä¸šç»©', 'è´¢æŠ¥', 'æ”¿ç­–', 'è¡Œä¸š',
            'åˆ†æ', 'é¢„æµ‹', 'æ¶¨åœ', 'è·Œåœ', 'ä¸Šæ¶¨', 'ä¸‹è·Œ', 'ç›ˆåˆ©', 'äºæŸ',
            'å¹¶è´­', 'é‡ç»„', 'åˆ†çº¢', 'å›è´­', 'å¢æŒ', 'å‡æŒ', 'èèµ„', 'IPO',
            'ç›‘ç®¡', 'å¤®è¡Œ', 'åˆ©ç‡', 'æ±‡ç‡', 'GDP', 'é€šèƒ€', 'ç»æµ', 'è´¸æ˜“',
            'ç§‘æŠ€', 'äº’è”ç½‘', 'æ–°èƒ½æº', 'åŒ»è¯', 'æˆ¿åœ°äº§', 'é‡‘è', 'åˆ¶é€ ä¸š'
        ]

        keywords = []
        for keyword in common_keywords:
            if keyword in text:
                keywords.append(keyword)

        return keywords[:10]  # æœ€å¤šè¿”å›10ä¸ªå…³é”®è¯

    def _assess_news_importance(self, content: str, title: str) -> str:
        """
        è¯„ä¼°æ–°é—»é‡è¦æ€§

        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜

        Returns:
            é‡è¦æ€§çº§åˆ«: high/medium/low
        """
        text = f"{title} {content}".lower()

        # é«˜é‡è¦æ€§å…³é”®è¯
        high_importance_keywords = [
            'ä¸šç»©', 'è´¢æŠ¥', 'å¹´æŠ¥', 'å­£æŠ¥', 'é‡å¤§', 'å…¬å‘Š', 'ç›‘ç®¡', 'æ”¿ç­–',
            'å¹¶è´­', 'é‡ç»„', 'é€€å¸‚', 'åœç‰Œ', 'æ¶¨åœ', 'è·Œåœ', 'æš´æ¶¨', 'æš´è·Œ',
            'å¤®è¡Œ', 'è¯ç›‘ä¼š', 'äº¤æ˜“æ‰€', 'è¿è§„', 'å¤„ç½š', 'ç«‹æ¡ˆ', 'è°ƒæŸ¥'
        ]

        # ä¸­ç­‰é‡è¦æ€§å…³é”®è¯
        medium_importance_keywords = [
            'åˆ†æ', 'é¢„æµ‹', 'è§‚ç‚¹', 'å»ºè®®', 'è¡Œä¸š', 'å¸‚åœº', 'è¶‹åŠ¿', 'æœºä¼š',
            'ç ”æŠ¥', 'è¯„çº§', 'ç›®æ ‡ä»·', 'å¢æŒ', 'å‡æŒ', 'ä¹°å…¥', 'å–å‡º',
            'åˆä½œ', 'ç­¾çº¦', 'ä¸­æ ‡', 'è·æ‰¹', 'åˆ†çº¢', 'å›è´­'
        ]

        # æ£€æŸ¥é«˜é‡è¦æ€§
        if any(keyword in text for keyword in high_importance_keywords):
            return 'high'

        # æ£€æŸ¥ä¸­ç­‰é‡è¦æ€§
        if any(keyword in text for keyword in medium_importance_keywords):
            return 'medium'

        return 'low'

    def _classify_news(self, content: str, title: str) -> str:
        """
        åˆ†ç±»æ–°é—»

        Args:
            content: æ–°é—»å†…å®¹
            title: æ–°é—»æ ‡é¢˜

        Returns:
            æ–°é—»ç±»åˆ«
        """
        text = f"{title} {content}".lower()

        # å…¬å¸å…¬å‘Š
        if any(keyword in text for keyword in ['å…¬å‘Š', 'ä¸šç»©', 'è´¢æŠ¥', 'å¹´æŠ¥', 'å­£æŠ¥']):
            return 'company_announcement'

        # æ”¿ç­–æ–°é—»
        if any(keyword in text for keyword in ['æ”¿ç­–', 'ç›‘ç®¡', 'å¤®è¡Œ', 'è¯ç›‘ä¼š', 'å›½åŠ¡é™¢']):
            return 'policy_news'

        # è¡Œä¸šæ–°é—»
        if any(keyword in text for keyword in ['è¡Œä¸š', 'æ¿å—', 'äº§ä¸š', 'é¢†åŸŸ']):
            return 'industry_news'

        # å¸‚åœºæ–°é—»
        if any(keyword in text for keyword in ['å¸‚åœº', 'æŒ‡æ•°', 'å¤§ç›˜', 'æ²ªæŒ‡', 'æ·±æˆæŒ‡']):
            return 'market_news'

        # ç ”ç©¶æŠ¥å‘Š
        if any(keyword in text for keyword in ['ç ”æŠ¥', 'åˆ†æ', 'è¯„çº§', 'ç›®æ ‡ä»·', 'æœºæ„']):
            return 'research_report'

        return 'general'


# å…¨å±€æä¾›å™¨å®ä¾‹
_akshare_provider = None


def get_akshare_provider() -> AKShareProvider:
    """è·å–å…¨å±€AKShareæä¾›å™¨å®ä¾‹"""
    global _akshare_provider
    if _akshare_provider is None:
        _akshare_provider = AKShareProvider()
    return _akshare_provider
