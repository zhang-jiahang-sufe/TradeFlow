#!/usr/bin/env python3
"""
ç¤¾åª’æ¶ˆæ¯çˆ¬è™«ç¤ºä¾‹ç¨‹åº
æ¼”ç¤ºå¦‚ä½•çˆ¬å–ç¤¾äº¤åª’ä½“æ•°æ®å¹¶å…¥åº“åˆ°æ¶ˆæ¯æ•°æ®ç³»ç»Ÿ
"""
import asyncio
import logging
import sys
import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import aiohttp
import time
import random
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.core.database import init_db
from app.services.social_media_service import get_social_media_service

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SocialMediaCrawler:
    """ç¤¾åª’æ¶ˆæ¯çˆ¬è™«åŸºç±»"""
    
    def __init__(self, platform: str):
        self.platform = platform
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.logger = logging.getLogger(f"{self.__class__.__name__}.{platform}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def clean_content(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff#@.,!?()ï¼ˆï¼‰ã€‚ï¼Œï¼ï¼Ÿ]', '', text)
        
        return text
    
    def extract_hashtags(self, text: str) -> List[str]:
        """æå–è¯é¢˜æ ‡ç­¾"""
        hashtags = re.findall(r'#([^#\s]+)#?', text)
        return list(set(hashtags))[:10]  # æœ€å¤š10ä¸ªæ ‡ç­¾
    
    def extract_mentions(self, text: str) -> List[str]:
        """æå–@ç”¨æˆ·"""
        mentions = re.findall(r'@([^\s@]+)', text)
        return list(set(mentions))[:5]  # æœ€å¤š5ä¸ªæåŠ
    
    def analyze_sentiment(self, text: str) -> tuple:
        """ç®€å•æƒ…ç»ªåˆ†æ"""
        positive_keywords = ['åˆ©å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'ç›ˆåˆ©', 'çªç ´', 'åˆ›æ–°é«˜', 'ä¹°å…¥', 'æ¨è', 'çœ‹å¥½', 'ç‰›å¸‚']
        negative_keywords = ['åˆ©ç©º', 'ä¸‹è·Œ', 'äºæŸ', 'é£é™©', 'æš´è·Œ', 'å–å‡º', 'è­¦å‘Š', 'ä¸‹è°ƒ', 'çœ‹ç©º', 'ç†Šå¸‚']
        
        text_lower = text.lower()
        positive_count = sum(1 for keyword in positive_keywords if keyword in text_lower)
        negative_count = sum(1 for keyword in negative_keywords if keyword in text_lower)
        
        if positive_count > negative_count:
            sentiment = 'positive'
            score = min(0.9, 0.5 + (positive_count - negative_count) * 0.1)
        elif negative_count > positive_count:
            sentiment = 'negative'
            score = max(-0.9, -0.5 - (negative_count - positive_count) * 0.1)
        else:
            sentiment = 'neutral'
            score = 0.0
        
        return sentiment, score
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨jiebaç­‰å·¥å…·ï¼‰
        common_keywords = [
            'è‚¡ç¥¨', 'è‚¡ä»·', 'æ¶¨åœ', 'è·Œåœ', 'ä¹°å…¥', 'å–å‡º', 'æŒæœ‰',
            'ä¸šç»©', 'è´¢æŠ¥', 'åˆ†çº¢', 'é‡ç»„', 'å¹¶è´­', 'IPO',
            'ç‰›å¸‚', 'ç†Šå¸‚', 'åå¼¹', 'è°ƒæ•´', 'çªç ´', 'æ”¯æ’‘', 'å‹åŠ›',
            'åŸºæœ¬é¢', 'æŠ€æœ¯é¢', 'æ¶ˆæ¯é¢', 'æ”¿ç­–', 'ç›‘ç®¡'
        ]
        
        keywords = []
        for keyword in common_keywords:
            if keyword in text:
                keywords.append(keyword)
        
        return keywords[:8]  # æœ€å¤š8ä¸ªå…³é”®è¯
    
    def assess_importance(self, engagement: Dict[str, Any], author_influence: float) -> str:
        """è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§"""
        engagement_rate = engagement.get('engagement_rate', 0)
        views = engagement.get('views', 0)
        
        # ç»¼åˆè¯„åˆ†
        score = (engagement_rate * 0.4 + author_influence * 0.4 + min(views / 10000, 1) * 0.2)
        
        if score >= 0.7:
            return 'high'
        elif score >= 0.4:
            return 'medium'
        else:
            return 'low'
    
    def assess_credibility(self, author: Dict[str, Any], content: str) -> str:
        """è¯„ä¼°æ¶ˆæ¯å¯ä¿¡åº¦"""
        verified = author.get('verified', False)
        follower_count = author.get('follower_count', 0)
        
        # åŸºç¡€å¯ä¿¡åº¦
        if verified and follower_count > 100000:
            base_credibility = 'high'
        elif verified or follower_count > 10000:
            base_credibility = 'medium'
        else:
            base_credibility = 'low'
        
        # å†…å®¹è´¨é‡è°ƒæ•´
        if len(content) > 100 and not re.search(r'[!]{3,}|[?]{3,}', content):
            return base_credibility
        else:
            # é™ä½ä¸€çº§
            if base_credibility == 'high':
                return 'medium'
            elif base_credibility == 'medium':
                return 'low'
            else:
                return 'low'


class WeiboCrawler(SocialMediaCrawler):
    """å¾®åšçˆ¬è™«"""
    
    def __init__(self):
        super().__init__('weibo')
        self.base_url = "https://m.weibo.cn/api"
    
    async def crawl_stock_messages(self, symbol: str, limit: int = 50) -> List[Dict[str, Any]]:
        """çˆ¬å–è‚¡ç¥¨ç›¸å…³å¾®åšæ¶ˆæ¯"""
        self.logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–å¾®åšæ¶ˆæ¯: {symbol}")
        
        try:
            # æ¨¡æ‹ŸAPIè°ƒç”¨ï¼ˆå®é™…éœ€è¦æ ¹æ®å¾®åšAPIæ–‡æ¡£å®ç°ï¼‰
            messages = await self._simulate_weibo_api(symbol, limit)
            
            # æ•°æ®æ ‡å‡†åŒ–
            standardized_messages = []
            for msg in messages:
                standardized_msg = await self._standardize_weibo_message(msg, symbol)
                if standardized_msg:
                    standardized_messages.append(standardized_msg)
            
            self.logger.info(f"âœ… å¾®åšæ¶ˆæ¯çˆ¬å–å®Œæˆ: {len(standardized_messages)} æ¡")
            return standardized_messages
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšæ¶ˆæ¯çˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def _simulate_weibo_api(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿå¾®åšAPIå“åº”ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨ï¼‰"""
        # æ¨¡æ‹Ÿæ•°æ®
        mock_messages = []
        
        for i in range(min(limit, 20)):  # æ¨¡æ‹Ÿæœ€å¤š20æ¡
            mock_msg = {
                "id": f"weibo_{symbol}_{int(time.time())}_{i}",
                "text": self._generate_mock_weibo_text(symbol, i),
                "created_at": (datetime.now() - timedelta(hours=random.randint(1, 48))).isoformat(),
                "user": {
                    "id": f"user_{random.randint(1000, 9999)}",
                    "screen_name": f"è‚¡æ°‘{random.randint(100, 999)}",
                    "verified": random.choice([True, False]),
                    "followers_count": random.randint(100, 100000),
                    "location": random.choice(["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "å¹¿å·", "æ­å·"])
                },
                "reposts_count": random.randint(0, 100),
                "comments_count": random.randint(0, 200),
                "attitudes_count": random.randint(10, 500)
            }
            mock_messages.append(mock_msg)
            
            # æ¨¡æ‹ŸAPIé™æµ
            await asyncio.sleep(0.1)
        
        return mock_messages
    
    def _generate_mock_weibo_text(self, symbol: str, index: int) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå¾®åšæ–‡æœ¬"""
        templates = [
            f"{symbol}ä»Šå¤©è¡¨ç°ä¸é”™ï¼Œçœ‹å¥½åç»­èµ°åŠ¿ï¼#è‚¡ç¥¨# #æŠ•èµ„#",
            f"å…³æ³¨{symbol}çš„åŸºæœ¬é¢å˜åŒ–ï¼Œä¸šç»©é¢„æœŸè‰¯å¥½ #ä»·å€¼æŠ•èµ„#",
            f"{symbol}æŠ€æœ¯é¢çªç ´ï¼Œæˆäº¤é‡æ”¾å¤§ï¼Œå€¼å¾—å…³æ³¨ #æŠ€æœ¯åˆ†æ#",
            f"æŒæœ‰{symbol}ä¸€æ®µæ—¶é—´äº†ï¼Œåˆ†çº¢ä¸é”™ï¼Œé•¿æœŸçœ‹å¥½ #é•¿çº¿æŠ•èµ„#",
            f"{symbol}æœ€æ–°æ¶ˆæ¯ï¼šå…¬å¸å‘å¸ƒé‡è¦å…¬å‘Šï¼Œåˆ©å¥½æ¶ˆæ¯ #åˆ©å¥½#"
        ]
        
        return random.choice(templates)
    
    async def _standardize_weibo_message(self, raw_msg: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–å¾®åšæ¶ˆæ¯æ•°æ®"""
        try:
            content = self.clean_content(raw_msg.get('text', ''))
            if not content:
                return None
            
            # è§£æç”¨æˆ·ä¿¡æ¯
            user = raw_msg.get('user', {})
            author = {
                "user_id": str(user.get('id', '')),
                "username": user.get('screen_name', ''),
                "display_name": user.get('screen_name', ''),
                "verified": user.get('verified', False),
                "follower_count": user.get('followers_count', 0),
                "influence_score": min(1.0, user.get('followers_count', 0) / 100000)
            }
            
            # è®¡ç®—äº’åŠ¨æ•°æ®
            reposts = raw_msg.get('reposts_count', 0)
            comments = raw_msg.get('comments_count', 0)
            likes = raw_msg.get('attitudes_count', 0)
            views = likes * 10  # ä¼°ç®—æµè§ˆé‡
            
            engagement = {
                "likes": likes,
                "shares": reposts,
                "comments": comments,
                "views": views,
                "engagement_rate": (likes + reposts + comments) / max(views, 1)
            }
            
            # æƒ…ç»ªåˆ†æ
            sentiment, sentiment_score = self.analyze_sentiment(content)
            
            # æå–æ ‡ç­¾å’Œå…³é”®è¯
            hashtags = self.extract_hashtags(content)
            keywords = self.extract_keywords(content)
            
            # è¯„ä¼°é‡è¦æ€§å’Œå¯ä¿¡åº¦
            importance = self.assess_importance(engagement, author['influence_score'])
            credibility = self.assess_credibility(author, content)
            
            # è§£æå‘å¸ƒæ—¶é—´
            publish_time = datetime.fromisoformat(raw_msg.get('created_at', '').replace('Z', '+00:00'))
            
            return {
                "message_id": raw_msg.get('id'),
                "platform": "weibo",
                "message_type": "post",
                "content": content,
                "media_urls": [],
                "hashtags": hashtags,
                "author": author,
                "engagement": engagement,
                "publish_time": publish_time,
                "sentiment": sentiment,
                "sentiment_score": sentiment_score,
                "confidence": 0.8,  # åˆ†æç½®ä¿¡åº¦
                "keywords": keywords,
                "topics": ["è‚¡ç¥¨è®¨è®º", "æŠ•èµ„è§‚ç‚¹"],
                "importance": importance,
                "credibility": credibility,
                "location": {
                    "country": "CN",
                    "province": "",
                    "city": user.get('location', '')
                },
                "data_source": "crawler_weibo",
                "crawler_version": "1.0",
                "symbol": symbol
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å¾®åšæ¶ˆæ¯æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return None


class DouyinCrawler(SocialMediaCrawler):
    """æŠ–éŸ³çˆ¬è™«"""
    
    def __init__(self):
        super().__init__('douyin')
    
    async def crawl_stock_messages(self, symbol: str, limit: int = 30) -> List[Dict[str, Any]]:
        """çˆ¬å–è‚¡ç¥¨ç›¸å…³æŠ–éŸ³æ¶ˆæ¯"""
        self.logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–æŠ–éŸ³æ¶ˆæ¯: {symbol}")
        
        try:
            # æ¨¡æ‹ŸæŠ–éŸ³æ•°æ®
            messages = await self._simulate_douyin_api(symbol, limit)
            
            # æ•°æ®æ ‡å‡†åŒ–
            standardized_messages = []
            for msg in messages:
                standardized_msg = await self._standardize_douyin_message(msg, symbol)
                if standardized_msg:
                    standardized_messages.append(standardized_msg)
            
            self.logger.info(f"âœ… æŠ–éŸ³æ¶ˆæ¯çˆ¬å–å®Œæˆ: {len(standardized_messages)} æ¡")
            return standardized_messages
            
        except Exception as e:
            self.logger.error(f"âŒ æŠ–éŸ³æ¶ˆæ¯çˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def _simulate_douyin_api(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹ŸæŠ–éŸ³APIå“åº”"""
        mock_messages = []
        
        for i in range(min(limit, 15)):
            mock_msg = {
                "aweme_id": f"douyin_{symbol}_{int(time.time())}_{i}",
                "desc": self._generate_mock_douyin_text(symbol, i),
                "create_time": int((datetime.now() - timedelta(hours=random.randint(1, 72))).timestamp()),
                "author": {
                    "uid": f"dy_user_{random.randint(1000, 9999)}",
                    "nickname": f"è´¢ç»è¾¾äºº{random.randint(100, 999)}",
                    "verification_type": random.choice([0, 1]),
                    "follower_count": random.randint(1000, 500000),
                    "city": random.choice(["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "å¹¿å·"])
                },
                "statistics": {
                    "digg_count": random.randint(50, 2000),
                    "share_count": random.randint(10, 300),
                    "comment_count": random.randint(20, 500),
                    "play_count": random.randint(1000, 50000)
                },
                "video": {
                    "play_addr": {"url_list": [f"https://example.com/video_{i}.mp4"]}
                }
            }
            mock_messages.append(mock_msg)
            await asyncio.sleep(0.1)
        
        return mock_messages
    
    def _generate_mock_douyin_text(self, symbol: str, index: int) -> str:
        """ç”Ÿæˆæ¨¡æ‹ŸæŠ–éŸ³æ–‡æœ¬"""
        templates = [
            f"åˆ†æ{symbol}çš„æŠ•èµ„ä»·å€¼ï¼Œè¿™æ”¯è‚¡ç¥¨å€¼å¾—å…³æ³¨ï¼#è‚¡ç¥¨åˆ†æ #æŠ•èµ„ç†è´¢",
            f"{symbol}æœ€æ–°è´¢æŠ¥è§£è¯»ï¼Œä¸šç»©è¶…é¢„æœŸï¼#è´¢æŠ¥åˆ†æ #ä»·å€¼æŠ•èµ„",
            f"æŠ€æœ¯åˆ†æ{symbol}ï¼Œçªç ´å…³é”®é˜»åŠ›ä½ #æŠ€æœ¯åˆ†æ #è‚¡ç¥¨",
            f"{symbol}è¡Œä¸šå‰æ™¯åˆ†æï¼Œé•¿æœŸçœ‹å¥½è¿™ä¸ªèµ›é“ #è¡Œä¸šåˆ†æ",
            f"ä»Šæ—¥{symbol}æ¶¨åœå¤ç›˜ï¼Œä¸»åŠ›èµ„é‡‘å¤§å¹…æµå…¥ #æ¶¨åœå¤ç›˜"
        ]
        
        return random.choice(templates)
    
    async def _standardize_douyin_message(self, raw_msg: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–æŠ–éŸ³æ¶ˆæ¯æ•°æ®"""
        try:
            content = self.clean_content(raw_msg.get('desc', ''))
            if not content:
                return None
            
            # è§£æç”¨æˆ·ä¿¡æ¯
            author_info = raw_msg.get('author', {})
            author = {
                "user_id": str(author_info.get('uid', '')),
                "username": author_info.get('nickname', ''),
                "display_name": author_info.get('nickname', ''),
                "verified": author_info.get('verification_type', 0) > 0,
                "follower_count": author_info.get('follower_count', 0),
                "influence_score": min(1.0, author_info.get('follower_count', 0) / 500000)
            }
            
            # è§£æäº’åŠ¨æ•°æ®
            stats = raw_msg.get('statistics', {})
            engagement = {
                "likes": stats.get('digg_count', 0),
                "shares": stats.get('share_count', 0),
                "comments": stats.get('comment_count', 0),
                "views": stats.get('play_count', 0),
                "engagement_rate": (stats.get('digg_count', 0) + stats.get('share_count', 0) + stats.get('comment_count', 0)) / max(stats.get('play_count', 1), 1)
            }
            
            # æå–åª’ä½“URL
            video_info = raw_msg.get('video', {})
            media_urls = []
            if video_info and 'play_addr' in video_info:
                url_list = video_info['play_addr'].get('url_list', [])
                if url_list:
                    media_urls = [url_list[0]]
            
            # æƒ…ç»ªåˆ†æå’Œå…¶ä»–å¤„ç†
            sentiment, sentiment_score = self.analyze_sentiment(content)
            hashtags = self.extract_hashtags(content)
            keywords = self.extract_keywords(content)
            importance = self.assess_importance(engagement, author['influence_score'])
            credibility = self.assess_credibility(author, content)
            
            # æ—¶é—´è½¬æ¢
            publish_time = datetime.fromtimestamp(raw_msg.get('create_time', time.time()))
            
            return {
                "message_id": raw_msg.get('aweme_id'),
                "platform": "douyin",
                "message_type": "post",
                "content": content,
                "media_urls": media_urls,
                "hashtags": hashtags,
                "author": author,
                "engagement": engagement,
                "publish_time": publish_time,
                "sentiment": sentiment,
                "sentiment_score": sentiment_score,
                "confidence": 0.75,
                "keywords": keywords,
                "topics": ["è´¢ç»è§†é¢‘", "æŠ•èµ„æ•™è‚²"],
                "importance": importance,
                "credibility": credibility,
                "location": {
                    "country": "CN",
                    "province": "",
                    "city": author_info.get('city', '')
                },
                "data_source": "crawler_douyin",
                "crawler_version": "1.0",
                "symbol": symbol
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æŠ–éŸ³æ¶ˆæ¯æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return None


async def crawl_and_save_social_media(symbols: List[str], platforms: List[str] = None):
    """çˆ¬å–å¹¶ä¿å­˜ç¤¾åª’æ¶ˆæ¯"""
    if platforms is None:
        platforms = ['weibo', 'douyin']
    
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ç¤¾åª’æ¶ˆæ¯: {symbols}, å¹³å°: {platforms}")
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        await init_db()
        
        # è·å–æœåŠ¡
        service = await get_social_media_service()
        
        total_saved = 0
        
        for symbol in symbols:
            logger.info(f"ğŸ“Š å¤„ç†è‚¡ç¥¨: {symbol}")
            
            for platform in platforms:
                try:
                    # åˆ›å»ºå¯¹åº”å¹³å°çš„çˆ¬è™«
                    if platform == 'weibo':
                        async with WeiboCrawler() as crawler:
                            messages = await crawler.crawl_stock_messages(symbol, limit=50)
                    elif platform == 'douyin':
                        async with DouyinCrawler() as crawler:
                            messages = await crawler.crawl_stock_messages(symbol, limit=30)
                    else:
                        logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å¹³å°: {platform}")
                        continue
                    
                    if messages:
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        result = await service.save_social_media_messages(messages)
                        saved_count = result.get('saved', 0)
                        total_saved += saved_count
                        
                        logger.info(f"âœ… {platform} - {symbol}: ä¿å­˜ {saved_count} æ¡æ¶ˆæ¯")
                    else:
                        logger.warning(f"âš ï¸ {platform} - {symbol}: æœªè·å–åˆ°æ¶ˆæ¯")
                    
                    # å¹³å°é—´å»¶è¿Ÿ
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ {platform} - {symbol} å¤„ç†å¤±è´¥: {e}")
                    continue
            
            # è‚¡ç¥¨é—´å»¶è¿Ÿ
            await asyncio.sleep(2)
        
        logger.info(f"ğŸ‰ ç¤¾åª’æ¶ˆæ¯çˆ¬å–å®Œæˆ! æ€»è®¡ä¿å­˜: {total_saved} æ¡")
        return total_saved
        
    except Exception as e:
        logger.error(f"âŒ ç¤¾åª’æ¶ˆæ¯çˆ¬å–è¿‡ç¨‹å¼‚å¸¸: {e}")
        return 0


async def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
    test_symbols = ["000001", "000002", "600000"]
    
    # æµ‹è¯•å¹³å°åˆ—è¡¨
    test_platforms = ["weibo", "douyin"]
    
    logger.info("ğŸ•·ï¸ ç¤¾åª’æ¶ˆæ¯çˆ¬è™«ç¤ºä¾‹ç¨‹åºå¯åŠ¨")
    
    # æ‰§è¡Œçˆ¬å–
    saved_count = await crawl_and_save_social_media(test_symbols, test_platforms)
    
    logger.info(f"ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
    logger.info(f"   - å¤„ç†è‚¡ç¥¨: {len(test_symbols)} åª")
    logger.info(f"   - å¤„ç†å¹³å°: {len(test_platforms)} ä¸ª")
    logger.info(f"   - ä¿å­˜æ¶ˆæ¯: {saved_count} æ¡")
    
    if saved_count > 0:
        logger.info("âœ… ç¤¾åª’æ¶ˆæ¯çˆ¬è™«è¿è¡ŒæˆåŠŸ!")
    else:
        logger.warning("âš ï¸ æœªä¿å­˜ä»»ä½•æ¶ˆæ¯ï¼Œè¯·æ£€æŸ¥é…ç½®")


if __name__ == "__main__":
    asyncio.run(main())
