#!/usr/bin/env python3
"""
å†…éƒ¨æ¶ˆæ¯çˆ¬è™«ç¤ºä¾‹ç¨‹åº
æ¼”ç¤ºå¦‚ä½•çˆ¬å–å†…éƒ¨æ¶ˆæ¯æ•°æ®å¹¶å…¥åº“åˆ°æ¶ˆæ¯æ•°æ®ç³»ç»Ÿ
"""
import asyncio
import logging
import sys
import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import aiohttp
import time
import random
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.core.database import init_db
from app.services.internal_message_service import get_internal_message_service

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InternalMessageCrawler:
    """å†…éƒ¨æ¶ˆæ¯çˆ¬è™«åŸºç±»"""
    
    def __init__(self, source_type: str):
        self.source_type = source_type
        self.session = None
        self.headers = {
            'User-Agent': 'Internal-System-Crawler/1.0',
            'Authorization': 'Bearer internal_token_here'  # å†…éƒ¨ç³»ç»Ÿè®¤è¯
        }
        self.logger = logging.getLogger(f"{self.__class__.__name__}.{source_type}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            timeout=aiohttp.ClientTimeout(total=60)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def clean_content(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        financial_keywords = [
            'ä¸šç»©', 'è´¢æŠ¥', 'è¥æ”¶', 'åˆ©æ¶¦', 'ROE', 'ROA', 'æ¯›åˆ©ç‡',
            'èµ„äº§è´Ÿå€ºç‡', 'ç°é‡‘æµ', 'åˆ†çº¢', 'é‡ç»„', 'å¹¶è´­', 'IPO',
            'ä¼°å€¼', 'PE', 'PB', 'PEG', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡',
            'å¢é•¿', 'ä¸‹æ»‘', 'äºæŸ', 'æ‰­äº', 'é¢„æœŸ', 'é¢„æµ‹',
            'é£é™©', 'æœºä¼š', 'æŒ‘æˆ˜', 'ä¼˜åŠ¿', 'åŠ£åŠ¿', 'ç«äº‰',
            'è¡Œä¸š', 'å¸‚åœº', 'æ”¿ç­–', 'ç›‘ç®¡', 'åˆè§„'
        ]
        
        keywords = []
        text_lower = text.lower()
        for keyword in financial_keywords:
            if keyword in text_lower:
                keywords.append(keyword)
        
        return keywords[:10]  # æœ€å¤š10ä¸ªå…³é”®è¯
    
    def analyze_sentiment(self, text: str) -> tuple:
        """åˆ†ææƒ…ç»ªå€¾å‘"""
        positive_words = ['åˆ©å¥½', 'å¢é•¿', 'ä¸Šæ¶¨', 'ç›ˆåˆ©', 'è¶…é¢„æœŸ', 'çœ‹å¥½', 'æ¨è', 'ä¹°å…¥', 'å¼ºçƒˆæ¨è']
        negative_words = ['åˆ©ç©º', 'ä¸‹æ»‘', 'ä¸‹è·Œ', 'äºæŸ', 'ä½äºé¢„æœŸ', 'çœ‹ç©º', 'å–å‡º', 'é£é™©', 'è­¦å‘Š']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            sentiment = 'positive'
            score = min(0.9, 0.5 + (positive_count - negative_count) * 0.1)
        elif negative_count > positive_count:
            sentiment = 'negative'
            score = max(-0.9, -0.5 - (negative_count - positive_count) * 0.1)
        else:
            sentiment = 'neutral'
            score = 0.0
        
        return sentiment, score
    
    def extract_risk_factors(self, text: str) -> List[str]:
        """æå–é£é™©å› ç´ """
        risk_patterns = [
            r'é£é™©[ï¼š:]([^ã€‚ï¼›\n]+)',
            r'å­˜åœ¨.*?é£é™©',
            r'å¯èƒ½.*?å½±å“',
            r'ä¸ç¡®å®šæ€§.*?å› ç´ '
        ]
        
        risks = []
        for pattern in risk_patterns:
            matches = re.findall(pattern, text)
            risks.extend(matches)
        
        return list(set(risks))[:5]  # æœ€å¤š5ä¸ªé£é™©å› ç´ 
    
    def extract_opportunities(self, text: str) -> List[str]:
        """æå–æœºä¼šå› ç´ """
        opportunity_patterns = [
            r'æœºä¼š[ï¼š:]([^ã€‚ï¼›\n]+)',
            r'æœ‰æœ›.*?å¢é•¿',
            r'é¢„æœŸ.*?æ”¹å–„',
            r'æ½œåœ¨.*?ä»·å€¼'
        ]
        
        opportunities = []
        for pattern in opportunity_patterns:
            matches = re.findall(pattern, text)
            opportunities.extend(matches)
        
        return list(set(opportunities))[:5]  # æœ€å¤š5ä¸ªæœºä¼šå› ç´ 


class ResearchReportCrawler(InternalMessageCrawler):
    """ç ”ç©¶æŠ¥å‘Šçˆ¬è™«"""
    
    def __init__(self):
        super().__init__('research_report')
        self.base_url = "http://internal-research-system/api"
    
    async def crawl_research_reports(self, symbol: str, limit: int = 10) -> List[Dict[str, Any]]:
        """çˆ¬å–ç ”ç©¶æŠ¥å‘Š"""
        self.logger.info(f"ğŸ“Š å¼€å§‹çˆ¬å–ç ”ç©¶æŠ¥å‘Š: {symbol}")
        
        try:
            # æ¨¡æ‹Ÿå†…éƒ¨ç ”ç©¶ç³»ç»ŸAPIè°ƒç”¨
            reports = await self._simulate_research_api(symbol, limit)
            
            # æ•°æ®æ ‡å‡†åŒ–
            standardized_reports = []
            for report in reports:
                standardized_report = await self._standardize_research_report(report, symbol)
                if standardized_report:
                    standardized_reports.append(standardized_report)
            
            self.logger.info(f"âœ… ç ”ç©¶æŠ¥å‘Šçˆ¬å–å®Œæˆ: {len(standardized_reports)} ä»½")
            return standardized_reports
            
        except Exception as e:
            self.logger.error(f"âŒ ç ”ç©¶æŠ¥å‘Šçˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def _simulate_research_api(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿç ”ç©¶ç³»ç»ŸAPIå“åº”"""
        mock_reports = []
        
        report_types = ['quarterly_analysis', 'annual_review', 'industry_analysis', 'valuation_report']
        departments = ['ç ”ç©¶éƒ¨', 'æŠ•èµ„éƒ¨', 'é£æ§éƒ¨', 'ç­–ç•¥éƒ¨']
        analysts = ['å¼ ç ”ç©¶å‘˜', 'æåˆ†æå¸ˆ', 'ç‹ç­–ç•¥å¸ˆ', 'èµµæŠ•èµ„ç»ç†']
        
        for i in range(min(limit, 8)):
            report_type = random.choice(report_types)
            department = random.choice(departments)
            analyst = random.choice(analysts)
            
            mock_report = {
                "report_id": f"RPT_{symbol}_{datetime.now().strftime('%Y%m%d')}_{i:03d}",
                "title": self._generate_report_title(symbol, report_type),
                "content": self._generate_report_content(symbol, report_type),
                "summary": self._generate_report_summary(symbol, report_type),
                "report_type": report_type,
                "department": department,
                "analyst": analyst,
                "analyst_id": f"analyst_{hash(analyst) % 1000:03d}",
                "created_date": (datetime.now() - timedelta(days=random.randint(1, 30))).isoformat(),
                "rating": random.choice(['strong_buy', 'buy', 'hold', 'sell']),
                "target_price": round(random.uniform(10, 50), 2),
                "confidence_level": round(random.uniform(0.6, 0.95), 2),
                "access_level": random.choice(['internal', 'restricted']),
                "tags": self._generate_report_tags(report_type)
            }
            mock_reports.append(mock_report)
            
            await asyncio.sleep(0.1)
        
        return mock_reports
    
    def _generate_report_title(self, symbol: str, report_type: str) -> str:
        """ç”ŸæˆæŠ¥å‘Šæ ‡é¢˜"""
        titles = {
            'quarterly_analysis': f"{symbol} Q{random.randint(1,4)}å­£åº¦ä¸šç»©åˆ†ææŠ¥å‘Š",
            'annual_review': f"{symbol} {datetime.now().year}å¹´åº¦æŠ•èµ„ä»·å€¼åˆ†æ",
            'industry_analysis': f"{symbol} æ‰€å±è¡Œä¸šæ·±åº¦ç ”ç©¶æŠ¥å‘Š",
            'valuation_report': f"{symbol} ä¼°å€¼åˆ†æä¸æŠ•èµ„å»ºè®®"
        }
        return titles.get(report_type, f"{symbol} æŠ•èµ„ç ”ç©¶æŠ¥å‘Š")
    
    def _generate_report_content(self, symbol: str, report_type: str) -> str:
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        base_content = f"""
        ä¸€ã€å…¬å¸æ¦‚å†µ
        {symbol} æ˜¯è¡Œä¸šå†…çš„é‡è¦ä¼ä¸šï¼Œä¸»è¥ä¸šåŠ¡ç¨³å®šï¼Œå¸‚åœºåœ°ä½è¾ƒä¸ºç¨³å›ºã€‚
        
        äºŒã€è´¢åŠ¡åˆ†æ
        æ ¹æ®æœ€æ–°è´¢åŠ¡æ•°æ®ï¼Œå…¬å¸è¥æ”¶å¢é•¿ç¨³å®šï¼Œç›ˆåˆ©èƒ½åŠ›æœ‰æ‰€æå‡ã€‚
        ä¸»è¦è´¢åŠ¡æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œèµ„äº§è´Ÿå€ºç»“æ„åˆç†ã€‚
        
        ä¸‰ã€æŠ•èµ„å»ºè®®
        ç»¼åˆè€ƒè™‘å…¬å¸åŸºæœ¬é¢ã€è¡Œä¸šå‰æ™¯å’Œå¸‚åœºç¯å¢ƒï¼Œç»™å‡ºç›¸åº”æŠ•èµ„å»ºè®®ã€‚
        å»ºè®®å…³æ³¨å…¬å¸åç»­ä¸šç»©è¡¨ç°å’Œè¡Œä¸šæ”¿ç­–å˜åŒ–ã€‚
        
        å››ã€é£é™©æç¤º
        éœ€è¦å…³æ³¨å¸‚åœºæ³¢åŠ¨é£é™©ã€æ”¿ç­–å˜åŒ–é£é™©å’Œè¡Œä¸šç«äº‰åŠ å‰§é£é™©ã€‚
        """
        
        return base_content.strip()
    
    def _generate_report_summary(self, symbol: str, report_type: str) -> str:
        """ç”ŸæˆæŠ¥å‘Šæ‘˜è¦"""
        summaries = {
            'quarterly_analysis': f"{symbol} å­£åº¦ä¸šç»©ç¬¦åˆé¢„æœŸï¼Œç»´æŒä¹°å…¥è¯„çº§",
            'annual_review': f"{symbol} å¹´åº¦è¡¨ç°è‰¯å¥½ï¼Œé•¿æœŸæŠ•èµ„ä»·å€¼æ˜¾è‘—",
            'industry_analysis': f"{symbol} è¡Œä¸šå‰æ™¯å‘å¥½ï¼Œå…¬å¸ç«äº‰ä¼˜åŠ¿æ˜æ˜¾",
            'valuation_report': f"{symbol} å½“å‰ä¼°å€¼åˆç†ï¼Œå…·å¤‡æŠ•èµ„ä»·å€¼"
        }
        return summaries.get(report_type, f"{symbol} æŠ•èµ„ä»·å€¼åˆ†æ")
    
    def _generate_report_tags(self, report_type: str) -> List[str]:
        """ç”ŸæˆæŠ¥å‘Šæ ‡ç­¾"""
        tag_map = {
            'quarterly_analysis': ['å­£åº¦åˆ†æ', 'ä¸šç»©è¯„ä¼°', 'è´¢åŠ¡åˆ†æ'],
            'annual_review': ['å¹´åº¦å›é¡¾', 'ä»·å€¼åˆ†æ', 'é•¿æœŸæŠ•èµ„'],
            'industry_analysis': ['è¡Œä¸šç ”ç©¶', 'ç«äº‰åˆ†æ', 'å¸‚åœºå‰æ™¯'],
            'valuation_report': ['ä¼°å€¼åˆ†æ', 'æŠ•èµ„å»ºè®®', 'ç›®æ ‡ä»·æ ¼']
        }
        return tag_map.get(report_type, ['æŠ•èµ„ç ”ç©¶'])
    
    async def _standardize_research_report(self, raw_report: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–ç ”ç©¶æŠ¥å‘Šæ•°æ®"""
        try:
            content = self.clean_content(raw_report.get('content', ''))
            if not content:
                return None
            
            # æƒ…ç»ªåˆ†æ
            sentiment, sentiment_score = self.analyze_sentiment(content)
            
            # æå–å…³é”®è¯ã€é£é™©å’Œæœºä¼š
            keywords = self.extract_keywords(content)
            risk_factors = self.extract_risk_factors(content)
            opportunities = self.extract_opportunities(content)
            
            # è§£ææ—¶é—´
            created_time = datetime.fromisoformat(raw_report.get('created_date', '').replace('Z', '+00:00'))
            
            return {
                "message_id": raw_report.get('report_id'),
                "message_type": "research_report",
                "title": raw_report.get('title'),
                "content": content,
                "summary": raw_report.get('summary'),
                "source": {
                    "type": "internal_research",
                    "department": raw_report.get('department'),
                    "author": raw_report.get('analyst'),
                    "author_id": raw_report.get('analyst_id'),
                    "reliability": "high"
                },
                "category": "fundamental_analysis",
                "subcategory": raw_report.get('report_type'),
                "tags": raw_report.get('tags', []),
                "importance": "high",
                "impact_scope": "stock_specific",
                "time_sensitivity": "medium_term",
                "confidence_level": raw_report.get('confidence_level', 0.8),
                "sentiment": sentiment,
                "sentiment_score": sentiment_score,
                "keywords": keywords,
                "risk_factors": risk_factors,
                "opportunities": opportunities,
                "related_data": {
                    "financial_metrics": ["revenue", "profit", "roe", "roa"],
                    "price_targets": [raw_report.get('target_price')],
                    "rating": raw_report.get('rating')
                },
                "access_level": raw_report.get('access_level', 'internal'),
                "permissions": ["research_team", "investment_team"],
                "created_time": created_time,
                "effective_time": created_time,
                "expiry_time": created_time + timedelta(days=90),
                "data_source": "internal_research_system",
                "symbol": symbol
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç ”ç©¶æŠ¥å‘Šæ ‡å‡†åŒ–å¤±è´¥: {e}")
            return None


class AnalystNoteCrawler(InternalMessageCrawler):
    """åˆ†æå¸ˆç¬”è®°çˆ¬è™«"""
    
    def __init__(self):
        super().__init__('analyst_note')
        self.base_url = "http://internal-analyst-system/api"
    
    async def crawl_analyst_notes(self, symbol: str, limit: int = 20) -> List[Dict[str, Any]]:
        """çˆ¬å–åˆ†æå¸ˆç¬”è®°"""
        self.logger.info(f"ğŸ“ å¼€å§‹çˆ¬å–åˆ†æå¸ˆç¬”è®°: {symbol}")
        
        try:
            # æ¨¡æ‹Ÿåˆ†æå¸ˆç³»ç»ŸAPIè°ƒç”¨
            notes = await self._simulate_analyst_api(symbol, limit)
            
            # æ•°æ®æ ‡å‡†åŒ–
            standardized_notes = []
            for note in notes:
                standardized_note = await self._standardize_analyst_note(note, symbol)
                if standardized_note:
                    standardized_notes.append(standardized_note)
            
            self.logger.info(f"âœ… åˆ†æå¸ˆç¬”è®°çˆ¬å–å®Œæˆ: {len(standardized_notes)} æ¡")
            return standardized_notes
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†æå¸ˆç¬”è®°çˆ¬å–å¤±è´¥: {e}")
            return []
    
    async def _simulate_analyst_api(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿåˆ†æå¸ˆç³»ç»ŸAPIå“åº”"""
        mock_notes = []
        
        note_types = ['market_observation', 'technical_analysis', 'news_comment', 'strategy_update']
        analysts = ['èµ„æ·±åˆ†æå¸ˆA', 'æŠ€æœ¯åˆ†æå¸ˆB', 'ç­–ç•¥åˆ†æå¸ˆC', 'è¡Œä¸šåˆ†æå¸ˆD']
        
        for i in range(min(limit, 15)):
            note_type = random.choice(note_types)
            analyst = random.choice(analysts)
            
            mock_note = {
                "note_id": f"NOTE_{symbol}_{int(time.time())}_{i}",
                "title": self._generate_note_title(symbol, note_type),
                "content": self._generate_note_content(symbol, note_type),
                "note_type": note_type,
                "analyst": analyst,
                "analyst_id": f"analyst_{hash(analyst) % 1000:03d}",
                "department": "æŠ•èµ„éƒ¨",
                "created_time": (datetime.now() - timedelta(hours=random.randint(1, 168))).isoformat(),
                "priority": random.choice(['high', 'medium', 'low']),
                "confidence": round(random.uniform(0.5, 0.9), 2),
                "tags": self._generate_note_tags(note_type)
            }
            mock_notes.append(mock_note)
            
            await asyncio.sleep(0.05)
        
        return mock_notes
    
    def _generate_note_title(self, symbol: str, note_type: str) -> str:
        """ç”Ÿæˆç¬”è®°æ ‡é¢˜"""
        titles = {
            'market_observation': f"{symbol} å¸‚åœºè¡¨ç°è§‚å¯Ÿ",
            'technical_analysis': f"{symbol} æŠ€æœ¯é¢åˆ†æç¬”è®°",
            'news_comment': f"{symbol} æœ€æ–°æ¶ˆæ¯ç‚¹è¯„",
            'strategy_update': f"{symbol} æŠ•èµ„ç­–ç•¥æ›´æ–°"
        }
        return titles.get(note_type, f"{symbol} åˆ†æç¬”è®°")
    
    def _generate_note_content(self, symbol: str, note_type: str) -> str:
        """ç”Ÿæˆç¬”è®°å†…å®¹"""
        contents = {
            'market_observation': f"{symbol} ä»Šæ—¥è¡¨ç°ç›¸å¯¹ç¨³å®šï¼Œæˆäº¤é‡è¾ƒæ˜¨æ—¥æœ‰æ‰€æ”¾å¤§ã€‚ä¸»åŠ›èµ„é‡‘æµå‘éœ€è¦æŒç»­å…³æ³¨ã€‚",
            'technical_analysis': f"{symbol} æŠ€æœ¯é¢æ˜¾ç¤ºçªç ´20æ—¥å‡çº¿ï¼ŒMACDæŒ‡æ ‡è½¬æ­£ï¼ŒçŸ­æœŸè¶‹åŠ¿å‘å¥½ã€‚å»ºè®®å…³æ³¨é‡ä»·é…åˆæƒ…å†µã€‚",
            'news_comment': f"{symbol} å‘å¸ƒé‡è¦å…¬å‘Šï¼Œå¯¹å…¬å¸åŸºæœ¬é¢äº§ç”Ÿç§¯æå½±å“ã€‚å»ºè®®å¯†åˆ‡å…³æ³¨åç»­è¿›å±•ã€‚",
            'strategy_update': f"åŸºäºæœ€æ–°å¸‚åœºç¯å¢ƒå’Œå…¬å¸åŸºæœ¬é¢å˜åŒ–ï¼Œè°ƒæ•´{symbol}æŠ•èµ„ç­–ç•¥ï¼Œç»´æŒè°¨æ…ä¹è§‚æ€åº¦ã€‚"
        }
        return contents.get(note_type, f"{symbol} ç›¸å…³åˆ†æè§‚ç‚¹")
    
    def _generate_note_tags(self, note_type: str) -> List[str]:
        """ç”Ÿæˆç¬”è®°æ ‡ç­¾"""
        tag_map = {
            'market_observation': ['å¸‚åœºè§‚å¯Ÿ', 'èµ„é‡‘æµå‘', 'æˆäº¤é‡åˆ†æ'],
            'technical_analysis': ['æŠ€æœ¯åˆ†æ', 'å‡çº¿ç³»ç»Ÿ', 'æŒ‡æ ‡åˆ†æ'],
            'news_comment': ['æ¶ˆæ¯é¢', 'å…¬å‘Šè§£è¯»', 'äº‹ä»¶å½±å“'],
            'strategy_update': ['ç­–ç•¥è°ƒæ•´', 'æŠ•èµ„å»ºè®®', 'é£é™©æ§åˆ¶']
        }
        return tag_map.get(note_type, ['åˆ†æç¬”è®°'])
    
    async def _standardize_analyst_note(self, raw_note: Dict[str, Any], symbol: str) -> Optional[Dict[str, Any]]:
        """æ ‡å‡†åŒ–åˆ†æå¸ˆç¬”è®°æ•°æ®"""
        try:
            content = self.clean_content(raw_note.get('content', ''))
            if not content:
                return None
            
            # æƒ…ç»ªåˆ†æ
            sentiment, sentiment_score = self.analyze_sentiment(content)
            
            # æå–å…³é”®è¯
            keywords = self.extract_keywords(content)
            
            # è§£ææ—¶é—´
            created_time = datetime.fromisoformat(raw_note.get('created_time', '').replace('Z', '+00:00'))
            
            # é‡è¦æ€§æ˜ å°„
            priority_map = {'high': 'high', 'medium': 'medium', 'low': 'low'}
            importance = priority_map.get(raw_note.get('priority'), 'medium')
            
            return {
                "message_id": raw_note.get('note_id'),
                "message_type": "analyst_note",
                "title": raw_note.get('title'),
                "content": content,
                "summary": content[:100] + "..." if len(content) > 100 else content,
                "source": {
                    "type": "analyst",
                    "department": raw_note.get('department'),
                    "author": raw_note.get('analyst'),
                    "author_id": raw_note.get('analyst_id'),
                    "reliability": "medium"
                },
                "category": self._map_category(raw_note.get('note_type')),
                "subcategory": raw_note.get('note_type'),
                "tags": raw_note.get('tags', []),
                "importance": importance,
                "impact_scope": "stock_specific",
                "time_sensitivity": "short_term",
                "confidence_level": raw_note.get('confidence', 0.7),
                "sentiment": sentiment,
                "sentiment_score": sentiment_score,
                "keywords": keywords,
                "risk_factors": [],
                "opportunities": [],
                "related_data": {
                    "technical_indicators": ["ma20", "macd", "volume"],
                    "price_targets": [],
                    "rating": "hold"
                },
                "access_level": "internal",
                "permissions": ["investment_team", "research_team"],
                "created_time": created_time,
                "effective_time": created_time,
                "expiry_time": created_time + timedelta(days=7),
                "data_source": "internal_analyst_system",
                "symbol": symbol
            }
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†æå¸ˆç¬”è®°æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return None
    
    def _map_category(self, note_type: str) -> str:
        """æ˜ å°„ç¬”è®°ç±»å‹åˆ°åˆ†æç±»åˆ«"""
        category_map = {
            'market_observation': 'market_sentiment',
            'technical_analysis': 'technical_analysis',
            'news_comment': 'fundamental_analysis',
            'strategy_update': 'risk_assessment'
        }
        return category_map.get(note_type, 'fundamental_analysis')


async def crawl_and_save_internal_messages(symbols: List[str], message_types: List[str] = None):
    """çˆ¬å–å¹¶ä¿å­˜å†…éƒ¨æ¶ˆæ¯"""
    if message_types is None:
        message_types = ['research_report', 'analyst_note']
    
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–å†…éƒ¨æ¶ˆæ¯: {symbols}, ç±»å‹: {message_types}")
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        await init_db()
        
        # è·å–æœåŠ¡
        service = await get_internal_message_service()
        
        total_saved = 0
        
        for symbol in symbols:
            logger.info(f"ğŸ“Š å¤„ç†è‚¡ç¥¨: {symbol}")
            
            for msg_type in message_types:
                try:
                    # åˆ›å»ºå¯¹åº”ç±»å‹çš„çˆ¬è™«
                    if msg_type == 'research_report':
                        async with ResearchReportCrawler() as crawler:
                            messages = await crawler.crawl_research_reports(symbol, limit=10)
                    elif msg_type == 'analyst_note':
                        async with AnalystNoteCrawler() as crawler:
                            messages = await crawler.crawl_analyst_notes(symbol, limit=20)
                    else:
                        logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: {msg_type}")
                        continue
                    
                    if messages:
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        result = await service.save_internal_messages(messages)
                        saved_count = result.get('saved', 0)
                        total_saved += saved_count
                        
                        logger.info(f"âœ… {msg_type} - {symbol}: ä¿å­˜ {saved_count} æ¡æ¶ˆæ¯")
                    else:
                        logger.warning(f"âš ï¸ {msg_type} - {symbol}: æœªè·å–åˆ°æ¶ˆæ¯")
                    
                    # ç±»å‹é—´å»¶è¿Ÿ
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"âŒ {msg_type} - {symbol} å¤„ç†å¤±è´¥: {e}")
                    continue
            
            # è‚¡ç¥¨é—´å»¶è¿Ÿ
            await asyncio.sleep(2)
        
        logger.info(f"ğŸ‰ å†…éƒ¨æ¶ˆæ¯çˆ¬å–å®Œæˆ! æ€»è®¡ä¿å­˜: {total_saved} æ¡")
        return total_saved
        
    except Exception as e:
        logger.error(f"âŒ å†…éƒ¨æ¶ˆæ¯çˆ¬å–è¿‡ç¨‹å¼‚å¸¸: {e}")
        return 0


async def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
    test_symbols = ["000001", "000002", "600000"]
    
    # æµ‹è¯•æ¶ˆæ¯ç±»å‹
    test_types = ["research_report", "analyst_note"]
    
    logger.info("ğŸ“Š å†…éƒ¨æ¶ˆæ¯çˆ¬è™«ç¤ºä¾‹ç¨‹åºå¯åŠ¨")
    
    # æ‰§è¡Œçˆ¬å–
    saved_count = await crawl_and_save_internal_messages(test_symbols, test_types)
    
    logger.info(f"ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
    logger.info(f"   - å¤„ç†è‚¡ç¥¨: {len(test_symbols)} åª")
    logger.info(f"   - å¤„ç†ç±»å‹: {len(test_types)} ç§")
    logger.info(f"   - ä¿å­˜æ¶ˆæ¯: {saved_count} æ¡")
    
    if saved_count > 0:
        logger.info("âœ… å†…éƒ¨æ¶ˆæ¯çˆ¬è™«è¿è¡ŒæˆåŠŸ!")
    else:
        logger.warning("âš ï¸ æœªä¿å­˜ä»»ä½•æ¶ˆæ¯ï¼Œè¯·æ£€æŸ¥é…ç½®")


if __name__ == "__main__":
    asyncio.run(main())
